

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Regression &#8212; ECON 5129</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'Lab2/regression';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
    
    
      
    
    
    <img src="../_static/ASBS_small_.PNG" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="../_static/ASBS_small_.PNG" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Course Overview
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">PRELIMINARY</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../getting_started/getting_started.html">Getting Started</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../getting_started/about_py.html">About Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../getting_started/setting_up_your_python_environment.html">Setting up Your Python Environment</a></li>
<li class="toctree-l2"><a class="reference internal" href="../getting_started/python_by_example.html">An Introductory Example</a></li>
<li class="toctree-l2"><a class="reference internal" href="../getting_started/learn_more.html">Learn More</a></li>
<li class="toctree-l2"><a class="reference internal" href="../getting_started/exercises.html">Exercises</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LAB MATERIALS</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../Lab1/lab1_overview.html">Introduction to Regression &amp; MLE</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../Lab1/regression_sol.html">Introduction to Regression</a></li>



<li class="toctree-l2"><a class="reference internal" href="../Lab1/mle.html">Maximum Likelihood Estimation</a></li>


</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="lab2_overview.html">Penalized Regression</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="regression_2.html">Penalized Regression</a></li>
</ul>
</li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/Lab2/regression.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Regression</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction-to-regression">Introduction to Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dataset">Dataset</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-regression">Linear Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise">Exercise</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Exercise</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multivariate-linear-regression">Multivariate Linear Regression</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Exercise</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#nonlinear-relationships-in-linear-regression">Nonlinear Relationships in Linear Regression</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Exercise</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretability">Interpretability</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lasso-regression">Lasso Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#overfitting-and-regularization">Overfitting and Regularization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cross-validation-of-regularization-parameter">Cross-validation of Regularization Parameter</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#holdout">Holdout</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">Exercise</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lasso-in-econometrics">Lasso in Econometrics</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#random-forests">Random Forests</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#regression-trees">Regression Trees</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">Exercise</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">Exercise</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">Random Forests</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">Exercise</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#neural-networks">Neural Networks</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mathematical-background">Mathematical Background</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">Exercise</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#application">Application</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#input-scaling">Input Scaling</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id10">Exercise</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tradeoffs">Tradeoffs</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id11">Exercise</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercises">Exercises</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-1">Exercise 1</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-2">Exercise 2</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-3">Exercise 3</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-4">Exercise 4</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-5">Exercise 5</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-6">Exercise 6</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-7">Exercise 7</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-8">Exercise 8</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-9">Exercise 9</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-10">Exercise 10</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-11">Exercise 11</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="regression">
<h1>Regression<a class="headerlink" href="#regression" title="Permalink to this heading">#</a></h1>
<p><strong>Co-author</strong></p>
<blockquote>
<div><ul class="simple">
<li><p><a class="reference external" href="https://economics.ubc.ca/faculty-and-staff/paul-schrimpf/">Paul Schrimpf <em>UBC</em></a></p></li>
</ul>
</div></blockquote>
<p><strong>Prerequisites</strong></p>
<ul class="simple">
<li><p><a class="reference external" href="https://datascience.quantecon.org/../scientific/applied_linalg.html">Applied Linear Algebra</a></p></li>
<li><p><a class="reference external" href="https://datascience.quantecon.org/../scientific/optimization.html">Optimization</a></p></li>
</ul>
<p><strong>Outcomes</strong></p>
<ul class="simple">
<li><p>Recall linear regression from linear algebra</p></li>
<li><p>Know what feature engineering is and how feature engineering can be automated by neural networks</p></li>
<li><p>Understand the related concepts of overfitting and regularization</p></li>
<li><p>Understand lasso regression and its relation to linear regression</p></li>
<li><p>Understand regression forests as well as its relation to linear regression</p></li>
<li><p>Understand the basics of the multi-layer perceptron</p></li>
<li><p>Use scikit-learn to fit linear regression, lasso, regression forests, and multi-layer perceptron to data on housing prices near Seattle, WA</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Uncomment following line to install on colab</span>
<span class="c1">#! pip install fiona geopandas xgboost gensim folium pyLDAvis descartes</span>
</pre></div>
</div>
</div>
</div>
<section id="introduction-to-regression">
<h2>Introduction to Regression<a class="headerlink" href="#introduction-to-regression" title="Permalink to this heading">#</a></h2>
<p>The goal of regression analysis is to provide accurate mapping from one or
more input variables (called features in machine learning or exogenous
variables in econometrics) to a continuous output variable (called the label or
target in machine learning and the endogenous variable in
econometrics).</p>
<p>In this lecture, we will study some of the most fundamental and widely-used
regression algorithms.</p>
<p>We will follow this same general pattern when we learn each algorithm:</p>
<ul class="simple">
<li><p>Describe the mathematical foundation for the algorithm</p></li>
<li><p>Use the <a class="reference external" href="https://scikit-learn.org/stable/">scikit-learn</a> python package to
apply the algorithm to a real world dataset on house prices in Washington state</p></li>
</ul>
<section id="dataset">
<h3>Dataset<a class="headerlink" href="#dataset" title="Permalink to this heading">#</a></h3>
<p>Let’s load the dataset and take a quick look at our task.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>

<span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;#165aa7&#39;</span><span class="p">,</span> <span class="s1">&#39;#cb495c&#39;</span><span class="p">,</span> <span class="s1">&#39;#fec630&#39;</span><span class="p">,</span> <span class="s1">&#39;#bb60d5&#39;</span><span class="p">,</span> <span class="s1">&#39;#f47915&#39;</span><span class="p">,</span> <span class="s1">&#39;#06ab54&#39;</span><span class="p">,</span> <span class="s1">&#39;#002070&#39;</span><span class="p">,</span> <span class="s1">&#39;#b27d12&#39;</span><span class="p">,</span> <span class="s1">&#39;#007030&#39;</span><span class="p">]</span>

<span class="c1"># We will import all these here to ensure that they are loaded, but</span>
<span class="c1"># will usually re-import close to where they are used to make clear</span>
<span class="c1"># where the functions come from</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">linear_model</span><span class="p">,</span> <span class="n">metrics</span><span class="p">,</span> <span class="n">neural_network</span><span class="p">,</span> <span class="n">pipeline</span><span class="p">,</span> <span class="n">model_selection</span>
<span class="p">)</span>

<span class="n">url</span> <span class="o">=</span> <span class="s2">&quot;https://datascience.quantecon.org/assets/data/kc_house_data.csv&quot;</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">info</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
RangeIndex: 21613 entries, 0 to 21612
Data columns (total 21 columns):
 #   Column         Non-Null Count  Dtype  
---  ------         --------------  -----  
 0   id             21613 non-null  int64  
 1   date           21613 non-null  object 
 2   price          21613 non-null  float64
 3   bedrooms       21613 non-null  int64  
 4   bathrooms      21613 non-null  float64
 5   sqft_living    21613 non-null  int64  
 6   sqft_lot       21613 non-null  int64  
 7   floors         21613 non-null  float64
 8   waterfront     21613 non-null  int64  
 9   view           21613 non-null  int64  
 10  condition      21613 non-null  int64  
 11  grade          21613 non-null  int64  
 12  sqft_above     21613 non-null  int64  
 13  sqft_basement  21613 non-null  int64  
 14  yr_built       21613 non-null  int64  
 15  yr_renovated   21613 non-null  int64  
 16  zipcode        21613 non-null  int64  
 17  lat            21613 non-null  float64
 18  long           21613 non-null  float64
 19  sqft_living15  21613 non-null  int64  
 20  sqft_lot15     21613 non-null  int64  
dtypes: float64(5), int64(15), object(1)
memory usage: 3.5+ MB
</pre></div>
</div>
</div>
</div>
<p>This dataset contains sales prices on houses in King County (which
includes Seattle),
Washington, from May 2014 to May 2015.</p>
<p>The data comes from
<a class="reference external" href="https://www.kaggle.com/harlfoxem/housesalesprediction">Kaggle</a> . Variable definitions and additional documentation are available at that link.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s2">&quot;price&quot;</span><span class="p">,</span> <span class="s2">&quot;date&quot;</span><span class="p">,</span> <span class="s2">&quot;id&quot;</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="c1"># convert everything to be a float for later on</span>
<span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="n">X</span><span class="p">[</span><span class="n">col</span><span class="p">]</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">col</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span>
<span class="n">X</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>bedrooms</th>
      <th>bathrooms</th>
      <th>sqft_living</th>
      <th>sqft_lot</th>
      <th>floors</th>
      <th>waterfront</th>
      <th>view</th>
      <th>condition</th>
      <th>grade</th>
      <th>sqft_above</th>
      <th>sqft_basement</th>
      <th>yr_built</th>
      <th>yr_renovated</th>
      <th>zipcode</th>
      <th>lat</th>
      <th>long</th>
      <th>sqft_living15</th>
      <th>sqft_lot15</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>3.0</td>
      <td>1.00</td>
      <td>1180.0</td>
      <td>5650.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>3.0</td>
      <td>7.0</td>
      <td>1180.0</td>
      <td>0.0</td>
      <td>1955.0</td>
      <td>0.0</td>
      <td>98178.0</td>
      <td>47.5112</td>
      <td>-122.257</td>
      <td>1340.0</td>
      <td>5650.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>3.0</td>
      <td>2.25</td>
      <td>2570.0</td>
      <td>7242.0</td>
      <td>2.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>3.0</td>
      <td>7.0</td>
      <td>2170.0</td>
      <td>400.0</td>
      <td>1951.0</td>
      <td>1991.0</td>
      <td>98125.0</td>
      <td>47.7210</td>
      <td>-122.319</td>
      <td>1690.0</td>
      <td>7639.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2.0</td>
      <td>1.00</td>
      <td>770.0</td>
      <td>10000.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>3.0</td>
      <td>6.0</td>
      <td>770.0</td>
      <td>0.0</td>
      <td>1933.0</td>
      <td>0.0</td>
      <td>98028.0</td>
      <td>47.7379</td>
      <td>-122.233</td>
      <td>2720.0</td>
      <td>8062.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4.0</td>
      <td>3.00</td>
      <td>1960.0</td>
      <td>5000.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>5.0</td>
      <td>7.0</td>
      <td>1050.0</td>
      <td>910.0</td>
      <td>1965.0</td>
      <td>0.0</td>
      <td>98136.0</td>
      <td>47.5208</td>
      <td>-122.393</td>
      <td>1360.0</td>
      <td>5000.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>3.0</td>
      <td>2.00</td>
      <td>1680.0</td>
      <td>8080.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>3.0</td>
      <td>8.0</td>
      <td>1680.0</td>
      <td>0.0</td>
      <td>1987.0</td>
      <td>0.0</td>
      <td>98074.0</td>
      <td>47.6168</td>
      <td>-122.045</td>
      <td>1800.0</td>
      <td>7503.0</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># notice the log here!</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s2">&quot;price&quot;</span><span class="p">])</span>
<span class="n">df</span><span class="p">[</span><span class="s2">&quot;log_price&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">y</span>
<span class="n">y</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0    12.309982
1    13.195614
2    12.100712
3    13.311329
4    13.142166
Name: price, dtype: float64
</pre></div>
</div>
</div>
</div>
<p>While we will be using all variables in <code class="docutils literal notranslate"><span class="pre">X</span></code> in our regression models,
we will explain some algorithms that use only the <code class="docutils literal notranslate"><span class="pre">sqft_living</span></code> variable.</p>
<p>Here’s what the log house price looks like against <code class="docutils literal notranslate"><span class="pre">sqft_living</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">var_scatter</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">var</span><span class="o">=</span><span class="s2">&quot;sqft_living&quot;</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">ax</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
    <span class="n">df</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">var</span> <span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">&quot;log_price&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.35</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">ax</span>

<span class="n">var_scatter</span><span class="p">(</span><span class="n">df</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/f2befe73cddf2886a6177a0714612362e930d853f7cf930f0dd6d439c2795e4b.png" src="../_images/f2befe73cddf2886a6177a0714612362e930d853f7cf930f0dd6d439c2795e4b.png" />
</div>
</div>
</section>
</section>
<section id="linear-regression">
<h2>Linear Regression<a class="headerlink" href="#linear-regression" title="Permalink to this heading">#</a></h2>
<p>Let’s dive in by studying the <a class="reference external" href="https://en.wikipedia.org/wiki/%22Hello,_World!%22_program">“Hello World”</a> of regression
algorithms: linear regression.</p>
<p>Suppose we would like to predict the log of the sale price of a home, given
only the livable square footage of the home.</p>
<p>The linear regression model for this situation is</p>
<div class="math notranslate nohighlight">
\[
\log(\text{price}) = \beta_0 + \beta_1 \text{sqft_living} + \epsilon
\]</div>
<p><span class="math notranslate nohighlight">\( \beta_0 \)</span> and <span class="math notranslate nohighlight">\( \beta_1 \)</span> are called parameters (also coefficients or
weights). The machine learning algorithm is tasked with finding the parameter values
that best fit the data.</p>
<p><span class="math notranslate nohighlight">\( \epsilon \)</span> is the error term. It would be unusual for the observed
<span class="math notranslate nohighlight">\( \log(\text{price}) \)</span> to be an exact linear function of
<span class="math notranslate nohighlight">\( \text{sqft_living} \)</span>. The error term captures the deviation of
<span class="math notranslate nohighlight">\( \log(\text{price}) \)</span> from a linear function of <span class="math notranslate nohighlight">\( \text{sqft_living} \)</span>.</p>
<p>The linear regression algorithm will choose the parameters that minimize the
<em>mean squared error</em> (MSE) function, which for our example is written.</p>
<div class="math notranslate nohighlight">
\[
\frac{1}{N} \sum_{i=1}^N \left(\log(\text{price}_i) - (\beta_0 + \beta_1 \text{sqft_living}_i) \right)^2
\]</div>
<p>The output of this algorithm is the straight line (hence linear) that passes as
close to the points on our scatter chart as possible.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">sns.lmplot</span></code> function below will plot our scatter chart and draw the
optimal linear regression line through the data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sns</span><span class="o">.</span><span class="n">lmplot</span><span class="p">(</span>
    <span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s2">&quot;sqft_living&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">&quot;log_price&quot;</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span>
    <span class="n">scatter_kws</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">s</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.35</span><span class="p">)</span>
<span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/9ab68a1a2e7d463ed13d553815029b89d67211057aef68fe83f0468f9f2353a3.png" src="../_images/9ab68a1a2e7d463ed13d553815029b89d67211057aef68fe83f0468f9f2353a3.png" />
</div>
</div>
<p>Let’s use <code class="docutils literal notranslate"><span class="pre">sklearn</span></code> to replicate the figure ourselves.</p>
<p>First, we fit the model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># import</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">linear_model</span>

<span class="c1"># construct the model instance</span>
<span class="n">sqft_lr_model</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">LinearRegression</span><span class="p">()</span>

<span class="c1"># fit the model</span>
<span class="n">sqft_lr_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">[[</span><span class="s2">&quot;sqft_living&quot;</span><span class="p">]],</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># print the coefficients</span>
<span class="n">beta_0</span> <span class="o">=</span> <span class="n">sqft_lr_model</span><span class="o">.</span><span class="n">intercept_</span>
<span class="n">beta_1</span> <span class="o">=</span> <span class="n">sqft_lr_model</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Fit model: log(price) = </span><span class="si">{</span><span class="n">beta_0</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> + </span><span class="si">{</span><span class="n">beta_1</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> sqft_living&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Fit model: log(price) = 12.2185 + 0.0004 sqft_living
</pre></div>
</div>
</div>
</div>
<p>Then, we construct the plot.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ax</span> <span class="o">=</span> <span class="n">var_scatter</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>

<span class="c1"># points for the line</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="n">df</span><span class="p">[</span><span class="s2">&quot;sqft_living&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">beta_0</span> <span class="o">+</span> <span class="n">beta_1</span><span class="o">*</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&lt;matplotlib.lines.Line2D at 0x20498f8be10&gt;]
</pre></div>
</div>
<img alt="../_images/5271acf904e88a8651fe99260e0607b78c27334b4cfb72deb1967ebfcd2f444b.png" src="../_images/5271acf904e88a8651fe99260e0607b78c27334b4cfb72deb1967ebfcd2f444b.png" />
</div>
</div>
<p>We can call the <code class="docutils literal notranslate"><span class="pre">predict</span></code> method on our model to evaluate the model at
arbitrary points.</p>
<p>For example, we can ask the model to predict the sale price of a 5,000-square-foot home.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Note, the argument needs to be two-dimensional. You&#39;ll see why shortly.</span>
<span class="n">logp_5000</span> <span class="o">=</span> <span class="n">sqft_lr_model</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mi">5000</span><span class="p">]])[</span><span class="mi">0</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The model predicts a 5,000 sq. foot home would cost </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">logp_5000</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> dollars&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The model predicts a 5,000 sq. foot home would cost 1486889.32 dollars
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>C:\Users\Miguel\anaconda3\Lib\site-packages\sklearn\base.py:464: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names
  warnings.warn(
</pre></div>
</div>
</div>
</div>
</section>
<section id="exercise">
<h2>Exercise<a class="headerlink" href="#exercise" title="Permalink to this heading">#</a></h2>
<p>See exercise 1 in the <span class="xref myst">exercise list</span>.</p>
</section>
<section id="id1">
<h2>Exercise<a class="headerlink" href="#id1" title="Permalink to this heading">#</a></h2>
<p>See exercise 2 in the <span class="xref myst">exercise list</span>.</p>
<section id="multivariate-linear-regression">
<h3>Multivariate Linear Regression<a class="headerlink" href="#multivariate-linear-regression" title="Permalink to this heading">#</a></h3>
<p>Our example regression above is called a univariate linear
regression since it uses a single feature.</p>
<p>In practice, more features would be used.</p>
<p>Suppose that in addition to <code class="docutils literal notranslate"><span class="pre">sqft_living</span></code>, we also wanted to use the <code class="docutils literal notranslate"><span class="pre">bathrooms</span></code> variable.</p>
<p>In this case, the linear regression model is</p>
<div class="math notranslate nohighlight">
\[
\log(\text{price}) = \beta_0 + \beta_1 \text{sqft_living} +
\beta_2 \text{bathrooms} + \epsilon
\]</div>
<p>We could keep adding one variable at a time, along with a new <span class="math notranslate nohighlight">\( \beta_{j} \)</span> coefficient for the :math:<code class="docutils literal notranslate"><span class="pre">j</span></code>th variable, but there’s an easier way.</p>
<p>Let’s write this equation in vector/matrix form as</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\underbrace{\begin{bmatrix} \log(\text{price}_1) \\ \log(\text{price}_2) \\ \vdots \\ \log(\text{price}_N)\end{bmatrix}}_Y = \underbrace{\begin{bmatrix} 1 &amp; \text{sqft_living}_1 &amp; \text{bathrooms}_1 \\ 1 &amp; \text{sqft_living}_2 &amp; \text{bathrooms}_2 \\ \vdots &amp; \vdots &amp; \vdots \\ 1 &amp; \text{sqft_living}_N &amp; \text{bathrooms}_N \end{bmatrix}}_{X} \underbrace{\begin{bmatrix} \beta_0 \\ \beta_1 \\ \beta_2 \end{bmatrix}}_{\beta} + \epsilon
\end{split}\]</div>
<p>Notice that we can add as many columns to <span class="math notranslate nohighlight">\( X \)</span> as we’d like and the linear
regression model will still be written <span class="math notranslate nohighlight">\( Y = X \beta + \epsilon \)</span>.</p>
<p>The mean squared error loss function for the general model is</p>
<div class="math notranslate nohighlight">
\[
\frac{1}{N} \sum_{i=1}^N (y_i - X_i \beta)^2 = \frac{1}{N} \| y - X \beta\|_2^2
\]</div>
<p>where <span class="math notranslate nohighlight">\( || \cdot ||_2 \)</span> is the <a class="reference external" href="http://mathworld.wolfram.com/L2-Norm.html">l2-norm</a>.</p>
<p>Let’s fit the linear regression model using all columns in <code class="docutils literal notranslate"><span class="pre">X</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lr_model</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">lr_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-1" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-1" type="checkbox" checked><label for="sk-estimator-id-1" class="sk-toggleable__label sk-toggleable__label-arrow">LinearRegression</label><div class="sk-toggleable__content"><pre>LinearRegression()</pre></div></div></div></div></div></div></div>
</div>
<p>We just fit a model with 18 variables – just as quickly and easily as
fitting the model with 1 variable!</p>
<p>Visualizing a 18-dimensional model is rather difficult, but just so we can see how the
extra features changed our model, let’s make the log price vs <code class="docutils literal notranslate"><span class="pre">sqft_living</span></code>
one more time – this time including the prediction from both of our linear models.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ax</span> <span class="o">=</span> <span class="n">var_scatter</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">scatter_model</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">x</span><span class="o">=</span><span class="s2">&quot;sqft_living&quot;</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">ax</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>

    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">x</span><span class="p">],</span> <span class="n">mod</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">c</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ax</span>

<span class="n">scatter_model</span><span class="p">(</span><span class="n">lr_model</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">scatter_model</span><span class="p">(</span><span class="n">sqft_lr_model</span><span class="p">,</span> <span class="n">X</span><span class="p">[[</span><span class="s2">&quot;sqft_living&quot;</span><span class="p">]],</span> <span class="n">ax</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s2">&quot;data&quot;</span><span class="p">,</span> <span class="s2">&quot;full model&quot;</span><span class="p">,</span> <span class="s2">&quot;sqft model&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.legend.Legend at 0x2049951fcd0&gt;
</pre></div>
</div>
<img alt="../_images/140acd254df296eb4d86529bb141be5b160cbce8679c8bb38331214d7141a7d3.png" src="../_images/140acd254df296eb4d86529bb141be5b160cbce8679c8bb38331214d7141a7d3.png" />
</div>
</div>
</section>
<section id="id2">
<h3>Exercise<a class="headerlink" href="#id2" title="Permalink to this heading">#</a></h3>
<p>See exercise 3 in the <span class="xref myst">exercise list</span>.</p>
</section>
<section id="nonlinear-relationships-in-linear-regression">
<h3>Nonlinear Relationships in Linear Regression<a class="headerlink" href="#nonlinear-relationships-in-linear-regression" title="Permalink to this heading">#</a></h3>
<p>While it sounds like an oxymoron, a linear regression model can actually include non-linear features.</p>
<p>The distinguishing feature of the linear regression model is that each
prediction is generated by taking the dot product (a linear operator) between a
feature vector (one row of <span class="math notranslate nohighlight">\( X \)</span>) and a coefficient vector (<span class="math notranslate nohighlight">\( \beta \)</span>).</p>
<p>There is, however, no restriction on what element we include in our feature
vector.</p>
<p>Let’s consider an example…</p>
<p>Starting from the <code class="docutils literal notranslate"><span class="pre">sqft_living</span></code>-only model, suppose we have a hunch that we
should also include the <em>fraction of square feet above ground</em>.</p>
<p>This last variable can be computed as <code class="docutils literal notranslate"><span class="pre">sqft_above</span> <span class="pre">/</span> <span class="pre">sqft_living</span></code>.</p>
<p>This second feature is nonlinear, but could easily be included as a column in
<code class="docutils literal notranslate"><span class="pre">X</span></code>.</p>
<p>Let’s see this in action.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X2</span> <span class="o">=</span> <span class="n">X</span><span class="p">[[</span><span class="s2">&quot;sqft_living&quot;</span><span class="p">]]</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">X2</span><span class="p">[</span><span class="s2">&quot;pct_sqft_above&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="s2">&quot;sqft_above&quot;</span><span class="p">]</span> <span class="o">/</span> <span class="n">X</span><span class="p">[</span><span class="s2">&quot;sqft_living&quot;</span><span class="p">]</span>

<span class="n">sqft_above_lr_model</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">sqft_above_lr_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X2</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="n">new_mse</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">sqft_above_lr_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X2</span><span class="p">))</span>
<span class="n">old_mse</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">sqft_lr_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X2</span><span class="p">[[</span><span class="s2">&quot;sqft_living&quot;</span><span class="p">]]))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The mse changed from </span><span class="si">{</span><span class="n">old_mse</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> to </span><span class="si">{</span><span class="n">new_mse</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> by including our new feature&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The mse changed from 0.1433 to 0.1430 by including our new feature
</pre></div>
</div>
</div>
</div>
</section>
<section id="id3">
<h3>Exercise<a class="headerlink" href="#id3" title="Permalink to this heading">#</a></h3>
<p>See exercise 4 in the <span class="xref myst">exercise list</span>.</p>
<p>Determining which columns belong in <span class="math notranslate nohighlight">\( X \)</span> is called <em>feature
engineering</em> and is a large part of a machine learning practitioner’s job.</p>
<p>You may recall from (or will see in) your econometrics course(s) that
choosing which control variables to include in a regression model
is an important part of applied research.</p>
</section>
<section id="interpretability">
<h3>Interpretability<a class="headerlink" href="#interpretability" title="Permalink to this heading">#</a></h3>
<p>Before moving to our next regression model, we want to broach the idea of
the <strong>interpretability</strong> of models.</p>
<p>An interpretable model is one for which we can analyze the
coefficients to explain why it makes its predictions.</p>
<p>Recall <span class="math notranslate nohighlight">\( \beta_0 \)</span> and <span class="math notranslate nohighlight">\( \beta_1 \)</span> from the univariate model.</p>
<p>The interpretation of the model is that <span class="math notranslate nohighlight">\( \beta_0 \)</span> captures the notion of
the average or starting house price and <span class="math notranslate nohighlight">\( \beta_1 \)</span> is the additional value
per square foot.</p>
<p>Concretely, we have</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">beta_0</span><span class="p">,</span> <span class="n">beta_1</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(12.218464096380853, 0.0003987465387451508)
</pre></div>
</div>
</div>
</div>
<p>which means that our model predicts the log price of a house to be 12.22, plus
an additional 0.0004 for every square foot.</p>
<p>Using more exotic machine learning methods might potentially be more accurate but
less interpretable.</p>
<p>The accuracy vs interpretability tradeoff is a hot discussion topic, especially relating
to concepts like machine learning ethics. This is something you
should be aware of as you continue to learn about these techniques.</p>
</section>
</section>
<section id="lasso-regression">
<h2>Lasso Regression<a class="headerlink" href="#lasso-regression" title="Permalink to this heading">#</a></h2>
<p>Lasso regression is very closely related to linear regression.</p>
<p>The lasso model also generates predictions using <span class="math notranslate nohighlight">\( y = X \beta \)</span> but
optimizes over a slightly different loss function.</p>
<p>The optimization problem solved by lasso regression can be written as</p>
<div class="math notranslate nohighlight">
\[
\min_{\beta} {|| X \beta - y||_2}^2 + \underbrace{\alpha {|| \beta ||_1}}_{\text{new part}}
\]</div>
<p>where <span class="math notranslate nohighlight">\( || a ||_1 = \sum_{i=1}^N | a_i| \)</span> is the <a class="reference external" href="http://mathworld.wolfram.com/L1-Norm.html">l1-norm</a> and <span class="math notranslate nohighlight">\( \alpha \)</span> is called the regularization parameter.</p>
<p>The additional term penalizes large coefficients and in practice, effectively sets coefficients to zero
for features that are not informative about the
target.</p>
<p>Let’s see an example of what this looks like using the full feature set in
<code class="docutils literal notranslate"><span class="pre">X</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lasso_model</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">Lasso</span><span class="p">()</span>
<span class="n">lasso_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="n">lasso_coefs</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">lasso_model</span><span class="o">.</span><span class="n">coef_</span><span class="p">)))</span>
<span class="n">lr_coefs</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">lr_model</span><span class="o">.</span><span class="n">coef_</span><span class="p">)))</span>
<span class="n">coefs</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="nb">dict</span><span class="p">(</span><span class="n">lasso</span><span class="o">=</span><span class="n">lasso_coefs</span><span class="p">,</span> <span class="n">linreg</span><span class="o">=</span><span class="n">lr_coefs</span><span class="p">))</span>
<span class="n">coefs</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>lasso</th>
      <th>linreg</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>bedrooms</th>
      <td>-0.000000e+00</td>
      <td>-1.220820e-02</td>
    </tr>
    <tr>
      <th>bathrooms</th>
      <td>0.000000e+00</td>
      <td>6.912370e-02</td>
    </tr>
    <tr>
      <th>sqft_living</th>
      <td>3.007615e-04</td>
      <td>9.573470e-05</td>
    </tr>
    <tr>
      <th>sqft_lot</th>
      <td>2.736772e-07</td>
      <td>4.711823e-07</td>
    </tr>
    <tr>
      <th>floors</th>
      <td>0.000000e+00</td>
      <td>7.515336e-02</td>
    </tr>
    <tr>
      <th>waterfront</th>
      <td>0.000000e+00</td>
      <td>3.711951e-01</td>
    </tr>
    <tr>
      <th>view</th>
      <td>0.000000e+00</td>
      <td>6.040466e-02</td>
    </tr>
    <tr>
      <th>condition</th>
      <td>0.000000e+00</td>
      <td>6.263658e-02</td>
    </tr>
    <tr>
      <th>grade</th>
      <td>0.000000e+00</td>
      <td>1.589338e-01</td>
    </tr>
    <tr>
      <th>sqft_above</th>
      <td>-0.000000e+00</td>
      <td>4.022408e-05</td>
    </tr>
    <tr>
      <th>sqft_basement</th>
      <td>1.614446e-05</td>
      <td>5.551062e-05</td>
    </tr>
    <tr>
      <th>yr_built</th>
      <td>-1.159230e-03</td>
      <td>-3.410520e-03</td>
    </tr>
    <tr>
      <th>yr_renovated</th>
      <td>8.070315e-05</td>
      <td>3.659044e-05</td>
    </tr>
    <tr>
      <th>zipcode</th>
      <td>7.111990e-04</td>
      <td>-6.458836e-04</td>
    </tr>
    <tr>
      <th>lat</th>
      <td>0.000000e+00</td>
      <td>1.399994e+00</td>
    </tr>
    <tr>
      <th>long</th>
      <td>-0.000000e+00</td>
      <td>-1.591565e-01</td>
    </tr>
    <tr>
      <th>sqft_living15</th>
      <td>2.038719e-04</td>
      <td>9.856623e-05</td>
    </tr>
    <tr>
      <th>sqft_lot15</th>
      <td>-1.042168e-06</td>
      <td>-2.610066e-07</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Notice that many coefficients from the lasso regression have been set to
zero.</p>
<p>The intuition here is that the corresponding features hadn’t provided
enough predictive power to be worth considering alongside the other features.</p>
<p>The default value for the <span class="math notranslate nohighlight">\( \alpha \)</span> parameter is 1.0.</p>
<p>Larger <span class="math notranslate nohighlight">\( \alpha \)</span> values cause coefficients to shrink (and maybe
additional ones to be thrown out).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Compute lasso for many alphas (the lasso path)</span>
<span class="kn">from</span> <span class="nn">itertools</span> <span class="kn">import</span> <span class="n">cycle</span>
<span class="n">alphas</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">50</span><span class="p">))</span>
<span class="n">alphas</span><span class="p">,</span> <span class="n">coefs_lasso</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">lasso_path</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">alphas</span><span class="o">=</span><span class="n">alphas</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">10000</span><span class="p">)</span>

<span class="c1"># plotting</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">color_cycle</span> <span class="o">=</span> <span class="n">cycle</span><span class="p">(</span><span class="n">colors</span><span class="p">)</span>
<span class="n">log_alphas</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="n">alphas</span><span class="p">)</span>
<span class="k">for</span> <span class="n">coef_l</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">name</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">coefs_lasso</span><span class="p">,</span> <span class="n">color_cycle</span><span class="p">,</span> <span class="nb">list</span><span class="p">(</span><span class="n">X</span><span class="p">)):</span>
   <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">log_alphas</span><span class="p">,</span> <span class="n">coef_l</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">c</span><span class="p">)</span>
   <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;-Log(alpha)&#39;</span><span class="p">)</span>
   <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;lasso coefficients&#39;</span><span class="p">)</span>
   <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Lasso Path&#39;</span><span class="p">)</span>
   <span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;tight&#39;</span><span class="p">)</span>
   <span class="n">maxabs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">coef_l</span><span class="p">))</span>
   <span class="n">i</span> <span class="o">=</span> <span class="p">[</span><span class="n">idx</span> <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">coef_l</span><span class="p">))</span> <span class="k">if</span> <span class="nb">abs</span><span class="p">(</span><span class="n">coef_l</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span> <span class="o">&gt;=</span> <span class="p">(</span><span class="mf">0.9</span><span class="o">*</span><span class="n">maxabs</span><span class="p">)][</span><span class="mi">0</span><span class="p">]</span>
   <span class="n">xnote</span> <span class="o">=</span> <span class="n">log_alphas</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
   <span class="n">ynote</span> <span class="o">=</span> <span class="n">coef_l</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
   <span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="p">(</span><span class="n">xnote</span><span class="p">,</span> <span class="n">ynote</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="n">c</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/e68bf6b28fbd8f53bb177a70b59613bf144f27661af912b5c5c4b11a63127ffe.png" src="../_images/e68bf6b28fbd8f53bb177a70b59613bf144f27661af912b5c5c4b11a63127ffe.png" />
</div>
</div>
<section id="overfitting-and-regularization">
<h3>Overfitting and Regularization<a class="headerlink" href="#overfitting-and-regularization" title="Permalink to this heading">#</a></h3>
<p>You might be wondering, “Why would we ever want to throw variables out,
can’t that only hurt our model?”</p>
<p>The primary answer: it helps us avoid a common issue called <strong>overfitting</strong>.</p>
<p>Overfitting occurs when a model that specializes its coefficients too much on the
data it was trained on, and only to perform poorly when predicting on data
outside the training set.</p>
<p>The extreme example of overfitting is a model that can perfectly memorize the
training data, but can do no better than just randomly guess when predicting
on a new observation.</p>
<p>The techniques applied to reduce overfitting are known as <strong>regularization</strong>.</p>
<p>Regularization is an attempt to limit a model’s ability to specialize too narrowly
on training data (e.g. limit overfitting) by penalizing extreme values of the
model’s parameters.</p>
<p>The additional term in the lasso regression loss function (<span class="math notranslate nohighlight">\( \alpha ||\beta||_1 \)</span>)
is a form of regularization.</p>
<p>Let’s demonstrate the overfitting and regularization phenomenon on our housing
price data as follows:</p>
<ol class="arabic simple">
<li><p>Split the data set into training and testing subsets. We will use the first 50 observations for training and the rest for testing.</p></li>
<li><p>Fit the linear regression model and report MSE on training and testing datasets.</p></li>
<li><p>Fit the lasso model and report the same statistics.</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">fit_and_report_mses</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span><span class="p">):</span>
    <span class="n">mod</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="k">return</span> <span class="nb">dict</span><span class="p">(</span>
        <span class="n">mse_train</span><span class="o">=</span><span class="n">metrics</span><span class="o">.</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">mod</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)),</span>
        <span class="n">mse_test</span><span class="o">=</span><span class="n">metrics</span><span class="o">.</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">mod</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">))</span>
    <span class="p">)</span>

<span class="n">n_test</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:</span><span class="n">n_test</span><span class="p">,</span> <span class="p">:]</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">n_test</span><span class="p">:,</span> <span class="p">:]</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:</span><span class="n">n_test</span><span class="p">]</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">n_test</span><span class="p">:]</span>

<span class="n">fit_and_report_mses</span><span class="p">(</span><span class="n">linear_model</span><span class="o">.</span><span class="n">LinearRegression</span><span class="p">(),</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;mse_train&#39;: 0.05545627171577257, &#39;mse_test&#39;: 0.6705205330633802}
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fit_and_report_mses</span><span class="p">(</span><span class="n">linear_model</span><span class="o">.</span><span class="n">Lasso</span><span class="p">(),</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;mse_train&#39;: 0.1062919502412495, &#39;mse_test&#39;: 0.2910434823708496}
</pre></div>
</div>
</div>
</div>
<p>Notice how the MSE on the training dataset was smaller for the linear model
without the regularization, but the MSE on the test dataset was much
higher.</p>
<p>This strongly suggests that the linear regression model was
overfitting.</p>
<p>The regularization parameter has a large impact on overfitting.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">alphas</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="n">mse</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">([</span><span class="n">fit_and_report_mses</span><span class="p">(</span><span class="n">linear_model</span><span class="o">.</span><span class="n">Lasso</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">50000</span><span class="p">),</span>
                           <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
                    <span class="k">for</span> <span class="n">alpha</span> <span class="ow">in</span> <span class="n">alphas</span><span class="p">])</span>
<span class="n">mse</span><span class="p">[</span><span class="s2">&quot;log_alpha&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="n">alphas</span><span class="p">)</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">mse</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">&quot;log_alpha&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">&quot;mse_test&quot;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">mse</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">&quot;log_alpha&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">&quot;mse_train&quot;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$-\log(\alpha)$&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;MSE&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">get_legend</span><span class="p">()</span><span class="o">.</span><span class="n">remove</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s2">&quot;test&quot;</span><span class="p">,(</span><span class="n">mse</span><span class="o">.</span><span class="n">log_alpha</span><span class="p">[</span><span class="mi">15</span><span class="p">],</span> <span class="n">mse</span><span class="o">.</span><span class="n">mse_test</span><span class="p">[</span><span class="mi">15</span><span class="p">]),</span><span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s2">&quot;train&quot;</span><span class="p">,(</span><span class="n">mse</span><span class="o">.</span><span class="n">log_alpha</span><span class="p">[</span><span class="mi">30</span><span class="p">],</span> <span class="n">mse</span><span class="o">.</span><span class="n">mse_train</span><span class="p">[</span><span class="mi">30</span><span class="p">]),</span><span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(-2.3688789921995554, 0.2249868810554537, &#39;train&#39;)
</pre></div>
</div>
<img alt="../_images/db979654e61213efcd30702966d5a151a9f27b06f40ee18e0ae9ca5f41bb5731.png" src="../_images/db979654e61213efcd30702966d5a151a9f27b06f40ee18e0ae9ca5f41bb5731.png" />
</div>
</div>
</section>
<section id="cross-validation-of-regularization-parameter">
<h3>Cross-validation of Regularization Parameter<a class="headerlink" href="#cross-validation-of-regularization-parameter" title="Permalink to this heading">#</a></h3>
<p>As you can see in the figure above, the regularization parameter has a
large impact on MSE in the test data.</p>
<p>Moreover, the relationship between the test data MSE and <span class="math notranslate nohighlight">\( \alpha \)</span> is
complicated and non-monotonic.</p>
<p>One popular method for choosing the regularization parameter is cross-validation.</p>
<p>Roughly speaking, cross-validation splits the dataset into many training/testing
subsets, then chooses the regularization parameter value that minimizes the
average MSE.</p>
<p>More precisely, k-fold cross-validation does the following:</p>
<ol class="arabic simple">
<li><p>Partition the dataset randomly into k subsets/”folds”.</p></li>
<li><p>Compute <span class="math notranslate nohighlight">\( MSE_j(\alpha)= \)</span> mean squared error in j-th subset
when using the j-th subset as test data, and other k-1 as training
data.</p></li>
<li><p>Minimize average (across folds) MSE <span class="math notranslate nohighlight">\( \min_\alpha \frac{1}{k}
\sum_{j=1}^k MSE_j(\alpha) \)</span>.</p></li>
</ol>
<p>The following code plots 5-fold, cross-validated MSE as a function of
<span class="math notranslate nohighlight">\( \alpha \)</span>, using the same training data as above.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span>
<span class="n">mse</span><span class="p">[</span><span class="s2">&quot;cv&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">cross_val_score</span><span class="p">(</span><span class="n">linear_model</span><span class="o">.</span><span class="n">Lasso</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">50000</span><span class="p">),</span>
                                  <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;neg_mean_squared_error&#39;</span><span class="p">))</span>
          <span class="k">for</span> <span class="n">alpha</span> <span class="ow">in</span> <span class="n">alphas</span><span class="p">]</span>
<span class="n">mse</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">&quot;log_alpha&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">&quot;cv&quot;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s2">&quot;5 fold cross-validation&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">mse</span><span class="o">.</span><span class="n">log_alpha</span><span class="p">[</span><span class="mi">40</span><span class="p">],</span> <span class="n">mse</span><span class="o">.</span><span class="n">cv</span><span class="p">[</span><span class="mi">40</span><span class="p">]),</span> <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">get_legend</span><span class="p">()</span><span class="o">.</span><span class="n">remove</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$-\log(\alpha)$&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;MSE&quot;</span><span class="p">)</span>
<span class="n">fig</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/2f2afe576d0d4ca97ebd6ed30cc7e5706791faa692a01ebf15fa456bfe1fed56.png" src="../_images/2f2afe576d0d4ca97ebd6ed30cc7e5706791faa692a01ebf15fa456bfe1fed56.png" />
</div>
</div>
<p>scikit learn also includes methods to automate the above and select
<span class="math notranslate nohighlight">\( \alpha \)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># LassoCV exploits special structure of lasso problem to minimize CV more efficiently</span>
<span class="n">lasso</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">LassoCV</span><span class="p">(</span><span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
<span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="n">lasso</span><span class="o">.</span><span class="n">alpha_</span><span class="p">)</span> <span class="c1"># should roughly = minimizer on graph, not exactly equal due to random splitting</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>-1.9732104006516822
</pre></div>
</div>
</div>
</div>
</section>
<section id="holdout">
<h3>Holdout<a class="headerlink" href="#holdout" title="Permalink to this heading">#</a></h3>
<p>Practitioners often use another technique to avoid overfitting, called
<em>holdout</em>.</p>
<p>We demonstrated an extreme example of applying holdout above when we used only
the first 50 observations to train our models.</p>
<p>In general, good practice is to split the entire dataset into a training subset
and testing or validation subset.</p>
<p>The splitting should be done randomly. It should leave enough data in the
training dataset to produce a good model, but also enough in the validation
subset to determine the degree of overfitting.</p>
<p>There aren’t hard and fast rules for how much data to put in each subset, but a
reasonable default uses 75% of the data for training and the
rest for testing.</p>
<p>As in the example above, the training data is often further split
while selecting regularization parameters with cross-validation.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">sklearn</span></code> function <code class="docutils literal notranslate"><span class="pre">model_selection.train_test_split</span></code> will do this for you:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># note test_size=0.25 is the default value, but is shown here so you</span>
<span class="c1"># can see how to change it</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">model_selection</span><span class="o">.</span><span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="id4">
<h3>Exercise<a class="headerlink" href="#id4" title="Permalink to this heading">#</a></h3>
<p>See exercise 5 in the <span class="xref myst">exercise list</span>.</p>
</section>
<section id="lasso-in-econometrics">
<h3>Lasso in Econometrics<a class="headerlink" href="#lasso-in-econometrics" title="Permalink to this heading">#</a></h3>
<p>Lasso is becoming increasingly popular in economics, partially due to
the work of Victor Chernozhukov and his coauthors.</p>
<p>In econometrics, the goal is typically to estimate some coefficient of interest or causal
effect, rather than obtaining the most precise prediction.</p>
<p>Among other things, this different goal affects how the regularization parameter
must be chosen.</p>
<p>[<span class="xref myst">regBC11</span>] and [<span class="xref myst">regCHS16</span>] are somewhat approachable introductions
to this area.</p>
<p>The latter of these two references includes an R package.</p>
<p>[<span class="xref myst">regCCD+18</span>] reflects close to the state-of-art in this rapidly
advancing area.</p>
</section>
</section>
<section id="random-forests">
<h2>Random Forests<a class="headerlink" href="#random-forests" title="Permalink to this heading">#</a></h2>
<p>Random forests are also becoming increasingly popular in economics, thanks to the work of Susan Athey and her coauthors.</p>
<p>[<span class="xref myst">regAI17</span>] gives a very brief overview of this work,
and [<span class="xref myst">regAI18</span>] are video lectures and associated code aimed
at a broader audience.</p>
<section id="regression-trees">
<h3>Regression Trees<a class="headerlink" href="#regression-trees" title="Permalink to this heading">#</a></h3>
<p>To understand a forest, we must first understand trees.</p>
<p>We will begin by fitting a tree to this simulated data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="c1"># Simulate some data and plot it</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">Xsim</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">n</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">Ey_x</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span><span class="o">/</span><span class="mi">3</span><span class="o">*</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">5</span><span class="o">*</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">-</span><span class="mf">0.5</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>

<span class="n">ysim</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">apply_along_axis</span><span class="p">(</span><span class="n">Ey_x</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Xsim</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n</span><span class="p">)</span><span class="o">*</span><span class="mf">0.1</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">plotly.graph_objects</span> <span class="k">as</span> <span class="nn">go</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">surface_scatter_plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">f</span><span class="p">,</span> <span class="n">xlo</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">xhi</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">ngrid</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
                         <span class="n">width</span><span class="o">=</span><span class="mi">860</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">700</span><span class="p">,</span> <span class="n">f0</span><span class="o">=</span><span class="n">Ey_x</span><span class="p">,</span> <span class="n">show_f0</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="n">scatter</span> <span class="o">=</span> <span class="n">go</span><span class="o">.</span><span class="n">Scatter3d</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span><span class="n">y</span><span class="o">=</span><span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span><span class="n">z</span><span class="o">=</span><span class="n">y</span><span class="p">,</span>
                           <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;markers&#39;</span><span class="p">,</span>
                           <span class="n">marker</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">opacity</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="n">xgrid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">xlo</span><span class="p">,</span><span class="n">xhi</span><span class="p">,</span><span class="n">ngrid</span><span class="p">)</span>
    <span class="n">ey</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">xgrid</span><span class="p">),</span><span class="nb">len</span><span class="p">(</span><span class="n">xgrid</span><span class="p">)))</span>
    <span class="n">ey0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">xgrid</span><span class="p">),</span><span class="nb">len</span><span class="p">(</span><span class="n">xgrid</span><span class="p">)))</span>
    <span class="n">colorscale</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="n">colors</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">colors</span><span class="p">[</span><span class="mi">2</span><span class="p">]]]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">xgrid</span><span class="p">)):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">xgrid</span><span class="p">)):</span>
            <span class="n">ey</span><span class="p">[</span><span class="n">j</span><span class="p">,</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">f</span><span class="p">([</span><span class="n">xgrid</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="n">xgrid</span><span class="p">[</span><span class="n">j</span><span class="p">]])</span>
            <span class="n">ey0</span><span class="p">[</span><span class="n">j</span><span class="p">,</span><span class="n">i</span><span class="p">]</span><span class="o">=</span> <span class="n">f0</span><span class="p">([</span><span class="n">xgrid</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="n">xgrid</span><span class="p">[</span><span class="n">j</span><span class="p">]])</span>
    <span class="n">surface</span> <span class="o">=</span> <span class="n">go</span><span class="o">.</span><span class="n">Surface</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">xgrid</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">xgrid</span><span class="p">,</span> <span class="n">z</span><span class="o">=</span><span class="n">ey</span><span class="p">,</span> <span class="n">colorscale</span><span class="o">=</span><span class="n">colorscale</span><span class="p">,</span> <span class="n">opacity</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">show_f0</span><span class="p">):</span>
        <span class="n">surface0</span> <span class="o">=</span> <span class="n">go</span><span class="o">.</span><span class="n">Surface</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">xgrid</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">xgrid</span><span class="p">,</span> <span class="n">z</span><span class="o">=</span><span class="n">ey0</span><span class="p">,</span> <span class="n">opacity</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">colorscale</span><span class="o">=</span><span class="n">colorscale</span><span class="p">)</span>
        <span class="n">layers</span> <span class="o">=</span> <span class="p">[</span><span class="n">scatter</span><span class="p">,</span> <span class="n">surface</span><span class="p">,</span> <span class="n">surface0</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">layers</span> <span class="o">=</span> <span class="p">[</span><span class="n">scatter</span><span class="p">,</span> <span class="n">surface</span><span class="p">]</span>
    <span class="n">fig</span> <span class="o">=</span> <span class="n">go</span><span class="o">.</span><span class="n">FigureWidget</span><span class="p">(</span>
        <span class="n">data</span><span class="o">=</span><span class="n">layers</span><span class="p">,</span>
        <span class="n">layout</span> <span class="o">=</span> <span class="n">go</span><span class="o">.</span><span class="n">Layout</span><span class="p">(</span>
            <span class="n">autosize</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">scene</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span>
                <span class="n">xaxis_title</span><span class="o">=</span><span class="s1">&#39;X1&#39;</span><span class="p">,</span>
                <span class="n">yaxis_title</span><span class="o">=</span><span class="s1">&#39;X2&#39;</span><span class="p">,</span>
                <span class="n">zaxis_title</span><span class="o">=</span><span class="s1">&#39;Y&#39;</span>
            <span class="p">),</span>
            <span class="n">width</span><span class="o">=</span><span class="n">width</span><span class="p">,</span>
            <span class="n">height</span><span class="o">=</span><span class="n">height</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">fig</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">surface_scatter_plot</span><span class="p">(</span><span class="n">Xsim</span><span class="p">,</span> <span class="n">ysim</span><span class="p">,</span> <span class="n">Ey_x</span><span class="p">)</span>
<span class="n">fig</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">AttributeError</span><span class="g g-Whitespace">                            </span>Traceback (most recent call last)
<span class="nn">File ~\anaconda3\Lib\site-packages\IPython\core\formatters.py:922,</span> in <span class="ni">IPythonDisplayFormatter.__call__</span><span class="nt">(self, obj)</span>
<span class="g g-Whitespace">    </span><span class="mi">920</span> <span class="n">method</span> <span class="o">=</span> <span class="n">get_real_method</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">print_method</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">921</span> <span class="k">if</span> <span class="n">method</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<span class="ne">--&gt; </span><span class="mi">922</span>     <span class="n">method</span><span class="p">()</span>
<span class="g g-Whitespace">    </span><span class="mi">923</span>     <span class="k">return</span> <span class="kc">True</span>

<span class="nn">File ~\anaconda3\Lib\site-packages\plotly\basewidget.py:741,</span> in <span class="ni">BaseFigureWidget._ipython_display_</span><span class="nt">(self)</span>
<span class="g g-Whitespace">    </span><span class="mi">737</span><span class="w"> </span><span class="sd">&quot;&quot;&quot;</span>
<span class="g g-Whitespace">    </span><span class="mi">738</span><span class="sd"> Handle rich display of figures in ipython contexts</span>
<span class="g g-Whitespace">    </span><span class="mi">739</span><span class="sd"> &quot;&quot;&quot;</span>
<span class="g g-Whitespace">    </span><span class="mi">740</span> <span class="c1"># Override BaseFigure&#39;s display to make sure we display the widget version</span>
<span class="ne">--&gt; </span><span class="mi">741</span> <span class="n">widgets</span><span class="o">.</span><span class="n">DOMWidget</span><span class="o">.</span><span class="n">_ipython_display_</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

<span class="ne">AttributeError</span>: type object &#39;DOMWidget&#39; has no attribute &#39;_ipython_display_&#39;
</pre></div>
</div>
<div class="output text_html"><div>                            <div id="b8cd95cb-bcfb-40ff-924d-f8aa93fd952a" class="plotly-graph-div" style="height:700px; width:860px;"></div>            <script type="text/javascript">                require(["plotly"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("b8cd95cb-bcfb-40ff-924d-f8aa93fd952a")) {                    Plotly.newPlot(                        "b8cd95cb-bcfb-40ff-924d-f8aa93fd952a",                        [{"marker":{"opacity":0.3,"size":2},"mode":"markers","x":[0.4844045748705237,0.9586977481615169,0.2055554875208363,0.10760929569823763,0.09770208348775411,0.2028631865333661,0.7054367808241943,0.4445273699705864,0.41868072504199805,0.769147920934262,0.9181619004615663,0.4897510409622713,0.8379819925754267,0.3747727171884657,0.7465357657969414,0.6864193150269382,0.9313814677470342,0.8877163072076063,0.2061542792709511,0.3943258226392107,0.4708072453628467,0.284382171966029,0.07176138139184329,0.6561022964469442,0.38760992746317313,0.8417163798205305,0.13643314267320095,0.4900877100777269,0.17205969611823624,0.9090422467104429,0.09383023672091262,0.9838726832468947,0.9175851088120485,0.17955905162813535,0.11406244687399325,0.6852131045676564,0.632600365936691,0.6697954141460651,0.5736884747315776,0.2969665210033675,0.5632058347200561,0.48902671761870997,0.005835143963222822,0.2539063652979733,0.9024956924947598,0.9586688228452025,0.309677591937659,0.6715051303853414,0.42519576765991507,0.18446568458193524,0.28561608852833587,0.7877804280361895,0.7495789411386414,0.23198699910987042,0.05220814099428861,0.8841818155675086,0.9570338375936531,0.5015278296026258,0.22851717571492058,0.3888430231562414,0.05090540461011084,0.39399510270434046,0.573525759793514,0.5363637182713147,0.07339946794498553,0.8836313082046855,0.3449866319190833,0.13850058295707657,0.769856600465761,0.5693294848265762,0.954839995947369,0.21233459535725252,0.8825645311282156,0.4489664265970652,0.43501111004195303,0.8362171242351882,0.12510825108093437,0.4858142140589631,0.6420714924753205,0.19759334354442715,0.1025007084409435,0.655681069383328,0.2643564433915796,0.0638817148978843,0.8727612269203202,0.23866710204030073,0.013040596925155468,0.5593957517069296,0.8741556364840274,0.07269372626937243,0.5451505372960254,0.1723776179140578,0.33503204889916094,0.9488333430251332,0.6530151627210725,0.30799169948944893,0.35026376172346807,0.35959726861246843,0.6649437273759881,0.9041084996684016,0.18338486639512575,0.5099061539855396,0.051243238777537226,0.47866650335033467,0.8819205572950931,0.12020475233013983,0.9299468478136297,0.5396309902398457,0.27960216656892767,0.332335746606661,0.8907670324425042,0.8885552917155373,0.045887619802298274,0.3775961423457346,0.8507081862269633,0.12222907620616841,0.38230014117465516,0.5173029661689659,0.6625286605152041,0.8082379360728806,0.09261178295280525,0.6081880105939012,0.49708259130302923,0.3747071050047117,0.04238554735685862,0.03489953543658331,0.3493513444340415,0.8227803081809258,0.5334258103016877,0.8161229949038386,0.6803703980384185,0.7054951311359544,0.8040312737940131,0.7945663593618552,0.30559665152803805,0.5368760659325199,0.9582685739615082,0.8212492764071748,0.5927255120433825,0.7929700730836142,0.31319295918553147,0.7390148605123282,0.9332280578311595,0.6324223093786644,0.6427809864230523,0.3908102690280656,0.259136455990853,0.5942194213187497,0.9601617390954059,0.9293815749596901,0.3176267157912146,0.7575373791439968,0.4203620464025358,0.0038321666756871986,0.6362643283008802,0.12358474303454337,0.690679055089639,0.656074008299083,0.46471873677873854,0.6456587388465224,0.9882361942847873,0.7392638421024736,0.8467468251110911,0.7951514194365569,0.685387364494464,0.3295093725582585,0.3678288288214937,0.6133554337964884,0.46383293375844237,0.07035898847232036,0.2451214023010112,0.7633682325078416,0.9880519522818489,0.06981438062937728,0.06477797252964712,0.6588277949587507,0.8005946356467343,0.5078898185605967,0.7530089949908633,0.2648524486591123,0.33534806891660185,0.8722756778329084,0.6370438207611248,0.40249559258911893,0.2068731093245484,0.008291901376815347,0.9008167281710352,0.07250550832345193,0.09149147867206164,0.9858302945486279,0.6097056392067616,0.2661663243465028,0.17840963254399433,0.31205278774581113,0.03450755859015486,0.3692299003004734,0.4377914214810279,0.44757466515367583,0.46435795124541124,0.5062039401042614,0.15078902384487036,0.31966661647128347,0.32080065667807445,0.12230780109774975,0.16782286112198674,0.3966062567983192,0.5803021024523578,0.7703894135406257,0.42298155527381964,0.24345245150901518,0.1732474753317278,0.4321410487005448,0.5163578070549961,0.8412348425297119,0.5819449257113318,0.9448281122465088,0.7968995847176008,0.6040864840790531,0.5219846938117574,0.4338724176029235,0.14967702516137704,0.9458511416515691,0.7749821849675984,0.32020259200021683,0.4603637025177245,0.3500780400406943,0.9264323466272127,0.2882766504466445,0.22413091191562606,0.7647554880728069,0.6851578276378527,0.2793538691283538,0.9139469478713074,0.6841030598046993,0.831602128092962,0.7267482571206834,0.15823108722486767,0.5935484794184336,0.19305122386234774,0.48507251025204356,0.7589017512827573,0.8815247536078129,0.7793842581559175,0.27664185556756304,0.14533871246448948,0.19212419614578724,0.4839898434330794,0.3439377122832449,0.5249896365091116,0.6090188371166414,0.6256095622507819,0.5061851368564624,0.8937914237988464,0.6972144535193744,0.38992936842177417,0.4936679114905853,0.9475341840586817,0.3566034104398881,0.1479928476422223,0.2017644905138708,0.5668659597473766,0.11158228968895001,0.9335598195506315,0.4020581041250766,0.6423226699992243,0.12418831378276829,0.2918434651865064,0.20969668299150512,0.7158345193386415,0.5253184515847004,0.6849254891638962,0.43393461857727766,0.3865979082777692,0.5376702105285024,0.06063040824028021,0.2854367376861979,0.5216274877411128,0.9165967387368656,0.6615347270636177,0.7229808942202652,0.7215417610067512,0.14518814180058504,0.27437975204408127,0.8601561086214489,0.5833716931355948,0.9767131030830919,0.03270723219272831,0.3356511064755223,0.46453794799951864,0.4568646379955388,0.3580943488538604,0.1320556654931604,0.6173798899537382,0.4469943078202775,0.21538418321621833,0.5870415013409483,0.5401632892744763,0.20822194923416837,0.3387263006826223,0.9653453150788242,0.48045577210570367,0.8535699764031917,0.34079741207785197,0.2767431844492315,0.6683977905405332,0.6235356922438546,0.12722057381434726,0.3033512872102826,0.2597271566425441,0.8773296488057978,0.9182247131915735,0.8208890986136065,0.9323317345162774,0.45302882885214724,0.0475485092854222,0.11261584939787339,0.5389809186632206,0.7605680408902005,0.8827579915719641,0.5921553031022593,0.5744117433619831,0.02572485055902607,0.039393487843362274,0.23460803437880628,0.3921668575796413,0.773221313498433,0.7298102600750161,0.8772646543013878,0.14043056412481203,0.5029595134720793,0.29235005425171123,0.011152188611060043,0.48796748982375804,0.1888676063779663,0.6522703988333798,0.9844847028799173,0.506520336757554,0.8388638066972706,0.5090609923343784,0.16425488231794994,0.9233244270787259,0.2794994540184935,0.644770064551952,0.9630013187850822,0.7134200997782123,0.48712227346792136,0.7848804359285,0.4556873251317981,0.2820255946873583,0.23944164109304977,0.6903492750712781,0.2966746985737464,0.05987856588668583,0.4669583269796852,0.34795688475252207,0.0696775376830494,0.40472624445214733,0.03414575711964818,0.798670819950402,0.6745559444955644,0.17053443872428753,0.4562356588455887,0.889665230550749,0.5445035880206595,0.1958989768425441,0.3337096670761909,0.3277734195782146,0.2572972691161861,0.5372294308548715,0.5561492182110137,0.9445798925309924,0.9763756697446204,0.8374647809062765,0.45180057758143655,0.9466930877440759,0.6044131008577674,0.04182429677111543,0.31745667470870853,0.07020392046629986,0.35108022531944083,0.904883965656573,0.44927101803051916,0.864561038235956,0.6134161436949644,0.7574122576161382,0.20428656335283413,0.6384185628008567,0.09404589718250056,0.7171901986209599,0.30508299361648417,0.3669759052797198,0.020654491956852983,0.7966010680889847,0.8629937855700188,0.8118177061032901,0.10249063222197108,0.049769555299867596,0.04970179197887581,0.20055105566811615,0.281384637662492,0.09074010591461101,0.05468436330261772,0.6862365096961834,0.6269027747339356,0.4967495798497922,0.47699885159140876,0.34936915133148605,0.8942371834413897,0.6289008378278225,0.20621022431885005,0.8981755169230027,0.045257756734975674,0.8971269837093373,0.6324491641009804,0.5547205850993594,0.7911299520649747,0.527692136607595,0.1277613738042016,0.4006453927975314,0.8215246577787494,0.22276917175158484,0.37997851202354826,0.12527084142520029,0.5849587557646077,0.8296306995647156,0.07017261194029611,0.9620210140945895,0.5692635418988375,0.602365965441481,0.11999096059697834,0.20421683049463935,0.347320544747028,0.8689959763384005,0.3322618575065297,0.28768722171046646,0.8760164140421665,0.9708529957855644,0.6697494719603226,0.7573543945073639,0.04366907376275131,0.2645882302593012,0.27206296523202145,0.08283466061849654,0.721472784518763,0.03786336498763898,0.3063712352923167,0.5840299098161325,0.8707806527044445,0.28763430020570235,0.20642845600981574,0.3459571634229023,0.9685702339983192,0.30435509555910345,0.519571197972387,0.6476795980401022,0.4369178201689744,0.8881639606731936,0.32135353174504244,0.1259645770917166,0.9925068165861969,0.3198641889569286,0.8808522584528592,0.10359683732367486,0.8871325106163624,0.7941808922011279,0.678650337091879,0.7134926044596495,0.25776071828245695,0.676276964097282,0.6063679111152904,0.6286211273373701,0.3518916545989007,0.062004986211611324,0.3336623268119755,0.029000135673992844,0.08106101644993824,0.915708153733722,0.7885006827205872,0.9168075588014064,0.11304949437937739,0.13887487589357939,0.7449296070018201,0.8038887040943354,0.3605964399455741,0.1808076539904162,0.4453100278217943,0.7263350831722821,0.20635913451594334,0.6735312631113387,0.6618630665585741,0.49243201447238705,0.13672555094786942,0.4858816628951157,0.6373830071562053,0.05878343751244364,0.8580325512869884,0.24188708207173837,0.9170892484740407,0.5440696783345272,0.6753565193920577,0.3451576490369319,0.27541289017408643,0.8564037558038639,0.11929485571391008,0.08262901471872997,0.6728134953741576,0.44333772418325756,0.7877538436932675,0.14281484663284438,0.26567712374000574,0.8035827714870989,0.7121935758110665,0.43995180855692106,0.6020893554134871,0.5863213726220518,0.16963598047882056,0.8567240493750403,0.8939651335009395,0.11185986975080753,0.2536233625735239,0.9684408521319078,0.8445507591564817,0.09400829983189463,0.48979556397903246,0.9483644164500443,0.7769745856664374,0.9420719448961886,0.941898900874941,0.12798170117073093,0.08038230387979306,0.038552087075738095,0.35526478964754027,0.8701712004017016,0.17553687515794825,0.05251680439599138,0.9371202771727544,0.6054545136020333,0.8774095118057068,0.46295575352464335,0.7059712514496779,0.3451077541393983,0.5017281024364931,0.5275201214478956,0.5401429860153708,0.5616644718849417,0.7503720495678382,0.9494613237942177,0.9264567158569043,0.40076960006505824,0.8059236650967564,0.3928281964501311,0.6555320433292695,0.7223891935801197,0.5477541852857578,0.7833341263991372,0.3881324837302702,0.8195816524969755,0.44957992731567487,0.8688627748056985,0.43321620965135477,0.3346945932425377,0.540454890972907,0.8515712566528655,0.6748553004578335,0.24754473037737168,0.522454072805936,0.6495208194281904,0.2392353864377902,0.49343683973894126,0.4095048212065501,0.5568554277604935,0.3467184939855067,0.8847224570609692,0.3826304904161113,0.19984530110451781,0.40263045731198266,0.8809797489943402,0.11685749018611835,0.3510562603379448,0.14495969036855083,0.7825178212815891,0.21732118504221032,0.8948580082895528,0.19360964964297644,0.31945635972658426,0.5009657709495436,0.8388286907181597,0.17829231352279706,0.1939832943072307,0.4208655721270913,0.5120566002371391,0.07315450809259616,0.3799399564534355,0.06744218524434376,0.6631848411189697,0.30357062750249253,0.7320445718373706,0.9460174460635802,0.3960366302682581,0.3256771815273638,0.8358586700379296,0.6060111475505157,0.369801035984602,0.5846422132529316,0.173278575626846,0.5419068927848736,0.10272346839975222,0.5559903749457785,0.6681351578402671,0.5269176785644276,0.02072533517355768,0.012700991782921833,0.9114659538203155,0.1904426087407849,0.7900998036293029,0.03661430788896036,0.2752507657712332,0.8994532439443835,0.542376594821776,0.3338124982900914,0.9090256006440588,0.38941391168878425,0.17096697271422767,0.8338298100765758,0.6762219259793848,0.48876477391962214,0.2563050327910755,0.31029029428418065,0.5389563040081502,0.18442448290069235,0.25310165081886893,0.29369187762027593,0.014364039636297843,0.08182242641033055,0.6813655261606929,0.8014983069288399,0.5746634174689966,0.8845673912074655,0.8353935042261038,0.6592663827984562,0.5628560406102546,0.69384667078497,0.1607782978820772,0.2189770048314702,0.08982572943212486,0.543491927728627,0.5194676269529953,0.14374641156086787,0.09452043051113812,0.7549899504546875,0.8033861559208882,0.1371413826236001,0.40232769049501116,0.21136508843015278,0.9864509358566289,0.9823901300706706,0.09180347339912642,0.051146788967287304,0.8641588280703245,0.20015411036111808,0.8022743483329782,0.2609279250626495,0.7135928713739746,0.382098073503661,0.40842482882949227,0.057379926111777335,0.4676102944856123,0.6363713687288572,0.017591342257227915,0.14318807227319752,0.892372823112324,0.055723160656975135,0.4142761999160417,0.0693349820367064,0.6755723124253319,0.7609381116115256,0.7986444058258108,0.2781645379466644,0.32676008673389156,0.3633981115689433,0.4550955005734494,0.972525270278762,0.1698672906607157,0.9284374144320334,0.3153432084438965,0.8085128534026155,0.46696320183214646,0.35116819412831624,0.7241602103697014,0.472944346533543,0.5462026592044662,0.2131948690384563,0.7605048557350754,0.8742711316883571,0.6992376457306132,0.783548425223813,0.28256757424196677,0.7492394789670823,0.19780685355623917,0.0826920014791146,0.3116290877940766,0.7896933430178543,0.5590072560256973,0.6718082720289893,0.680385287824961,0.3625408499682714,0.8229412657241849,0.10528438218689484,0.8903997237920319,0.8221194561485357,0.7849653830358846,0.8906193750031622,0.3365068060350167,0.9119431259499212,0.6781776681125643,0.01913904875892125,0.9461785425100547,0.34757211015743694,0.5308466828293874,0.17589388325688593,0.9761799428077125,0.4138464731999133,0.030098443593936763,0.6928209493024219,0.3306805506249577,0.16430981371166498,0.14631209824571467,0.6618735093301527,0.4735417213991254,0.35091360910363867,0.19538174557721133,0.8242018213020817,0.6179047953788511,0.11840387766739668,0.34983606503876785,0.64303272544677,0.11016053279001137,0.5282841481141637,0.07714991477537969,0.6441841449360506,0.7139850643633573,0.6074645165697947,0.9172559017833514,0.5533987469153113,0.15284786472468703,0.3754523300697771,0.639605447152014,0.7072385872963604,0.49240491081124216,0.7146377738759833,0.4805637278557464,0.36567477634375867,0.043229922876628635,0.8284406386106195,0.4662916231377987,0.1568014208332703,0.38155077223615486,0.03981790510521843,0.6149540274447421,0.4899456664461618,0.3105480957045328,0.19580057049339472,0.43747603976787186,0.2872400801607973,0.143483081536513,0.5566891769089648,0.22702322005922138,0.9951205291825842,0.8412543600565313,0.2989625676815114,0.08275121633498617,0.5124768818507969,0.25906108133706696,0.521507696249333,0.18252535616763765,0.34700346011306904,0.8968123078470817,0.1679444368142028,0.5507408646761591,0.11645163480543308,0.23597049047559548,0.32591431292639406,0.9031423883165206,0.24628658298546424,0.6532335475824598,0.27754627092903184,0.37345761994867743,0.38203367302721736,0.12304583560186844,0.3829199492535681,0.031059737757776373,0.464441076142885,0.33191238337283757,0.21258024813468557,0.30788728146705713,0.47838007928304627,0.7465593772490312,0.6011026261246659,0.7540435466968942,0.19707625338166512,0.7291529960288993,0.9593974408526643,0.8294444960348746,0.17297485788467537,0.9418171196142681,0.4597707187651493,0.8813031520955649,0.23089991627208972,0.5988720761287131,0.6968216569622698,0.027651476963143184,0.8552812778125644,0.1683600020158551,0.6985142754263128,0.07939408949309756,0.053918700313969614,0.9724186016583345,0.5272142618187037,0.6897416488385948,0.09293235016006085,0.37241100927778925,0.5272811421087271,0.4199065748234331,0.7737964068901912,0.19349350665274923,0.6351014124715372,0.03682457622174928,0.40169397965918185,0.8849606103976789,0.9228030576160801,0.9516490094693743,0.6007810126004712,0.050375339272657804,0.46275500282269144,0.5577862377339574,0.3584064516025649,0.42359319126784634,0.5298334337062722,0.5989801289806495,0.4093895358051829,0.22291534543571712,0.4792408409376556,0.8437903360619654,0.6492030466643566,0.7111380550755002,0.2557851345669685,0.928655866135429,0.028109747667828033,0.48094590316379104,0.9617706639555195,0.8064657022577484,0.04812605520136437,0.7358004316303475,0.5619108167228065,0.9544355979272046,0.5063273814855508,0.8351884815622893,0.14775326344526718,0.966457684484706,0.4527205292226375,0.2939728188408156,0.9267139642922787,0.017527045381997675,0.16981025039869646,0.5442244566995993,0.7423954967722665,0.8117985617531753,0.2649252660135807,0.926166591068579,0.5825939830622288,0.8905361196079457,0.36159153535311706,0.670077576359443,0.5976941494760178,0.0043733706682478335,0.5382384579850837,0.1776856326171149,0.8232230574493989,0.24519670347798583,0.6899550750359581,0.5920582370887454,0.6480088677361296,0.8051312530189572,0.8662827044075477,0.8166023370665387,0.4848657677730932,0.4261757504747239,0.9598301922456136,0.30887898538515224,0.9446864008470858,0.9398836513919321,0.3252778712334905,0.2082985253667723,0.26587794622361716,0.35352278353108324,0.7186393783031059,0.7949667343083054,0.9965226484847731,0.4424865337501511,0.10815134434037832,0.4233636577213533,0.5529478874437154,0.6475462880276855,0.5915696816794955,0.15796099941508646,0.38108302403758654,0.3945887231224311,0.6076369906845083,0.5017620451594217,0.8019231764994349,0.028677417931429416,0.24379436212115113,0.8060757992982622,0.12629107798755412,0.10994772516902851,0.7384683418164675,0.7014921558121836,0.9808492618146839,0.711316701731178,0.5692668552894032,0.7348097400938671,0.13622283167334626,0.06446496715519112,0.467959646324169,0.7151898529871077,0.13185691444379444,0.9214061627448277,0.496229585911636,0.765419220999571,0.9133072865882882,0.20146860484566764,0.07462975461251697,0.7369167261529006,0.22908076706206515,0.2927471476203918,0.46726318781817267,0.3625408052940895,0.2531979907248225,0.7869677645643929,0.022329085546181626,0.9844372556621794,0.9552442374854703,0.33714492332496515,0.9128620545541428,0.828957347779843,0.043403934171116876,0.0421858956847847,0.6040491025175334,0.4326199199815487,0.26568249519337117,0.3668294191844518,0.09713417976861227,0.1766196678389883,0.7314409195083201,0.06314597233111763,0.924894123236493,0.3790416373407324,0.7234028053355775,0.052114899170672135,0.8015604664217599,0.13263938029522993,0.5867281596621167,0.5166786356998293,0.3516829375195728,0.6457295414374307,0.2556736273109843,0.8818749816465424,0.24368253548994256,0.9439506524336253,0.20514098106843226,0.7644798822544999,0.5891203347862738,0.8860012041905608,0.5617033911079499,0.6180397180171778,0.3816482620967193,0.9233503527344971,0.18101505692142028,0.4777491860469604,0.21710213124609068,0.35335215334364856,0.9825097751244545,0.7891410309160888,0.7615463051537203,0.43592595268497125,0.4923663674685269,0.4412125447113947,0.19148700531194163,0.5481432093636606,0.5052164181839228,0.947843833574237,0.033931744062735136,0.2562127396676964,0.6195412300389918,0.8062116686572761,0.9890291475335391,0.4235089183808133,0.7915279609336199,0.006405912496410537,0.07028425185840292],"y":[0.6939871084070043,0.2271522613535586,0.5233219965660161,0.14812778387637016,0.9999463825872611,0.31333753541966247,0.8020993250573836,0.12641431815807036,0.42551794973226864,0.6448095273185173,0.524947885503153,0.1349773061614984,0.342493824905154,0.385823400509979,0.5756822468755465,0.42018740381477504,0.5944896706606343,0.7861618614255891,0.10361077121335982,0.9215655246080882,0.027384641785446417,0.5752093036946143,0.6087609500833933,0.0802882916859432,0.5860490673259945,0.8905228822750644,0.47149070597707055,0.7523864693949527,0.4363434021235576,0.9483659960051731,0.05201196238514383,0.762184182545624,0.45135345551720696,0.6805777511827544,0.5964860965171513,0.6518773161869286,0.45071949722859794,0.5688824352062263,0.6102424038891904,0.5935361504133866,0.7157154666308788,0.1025484757663292,0.12436359762565441,0.054992780302851374,0.8932791067053948,0.3842741855681169,0.6903090546616547,0.3797987976011984,0.44752422720290375,0.4363294815269949,0.15524459287424353,0.6782801807514466,0.15058051474203005,0.1811042833917006,0.42466981348207766,0.6887479940470127,0.7703237512711255,0.36672834059885673,0.6074036274329695,0.07853508562236056,0.7424805742910434,0.7907302403313882,0.9646447027701633,0.539400966754535,0.9695790623097985,0.47032589370035116,0.29890404508793134,0.8268372025435791,0.7302514915776382,0.7113514273739987,0.05705515076552592,0.3155274224389084,0.5526355383810474,0.01887541142323146,0.09505467137351986,0.6442048254159709,0.8551779984359056,0.125439203268344,0.21068341726216844,0.9696789313139879,0.5084043789371421,0.13931045625546934,0.790575795361949,0.5850483259587705,0.8397110155801961,0.7998554816122949,0.5397330830042527,0.32591767084736145,0.2380574726785588,0.3846910116822736,0.13798115746807527,0.273199845076329,0.5919736729257196,0.5060749640659528,0.18841506374751293,0.3364457214825065,0.5960235422757206,0.7422577050659276,0.4376793239320397,0.06663657681290436,0.36181339632539267,0.3214314876084401,0.11124603245971798,0.194331006912981,0.2659152559070539,0.46408796785754725,0.06755313365182691,0.9868554776382824,0.9835763815781587,0.008311037700919122,0.25109534266642286,0.5461330188418786,0.3967045326587797,0.547079515754605,0.3639562163068477,0.494521343447219,0.44411607124005315,0.1731416543063885,0.30863874322452844,0.016566298751011344,0.4760579905985275,0.6461740397204361,0.071144866618993,0.7712899999875976,0.37461065561782847,0.831788530825519,0.19705831908988847,0.2374701528393811,0.7449515073500977,0.8418593118871404,0.26884542487840846,0.571597169872879,0.0708984166470904,0.7885769962434673,0.598084121809462,0.10676950535143948,0.37394852995572647,0.2742569136607068,0.3987462590099148,0.1859572812290392,0.1166596969639837,0.615628790315536,0.032555036347929,0.6406674246831182,0.25316951286789646,0.3239261576139958,0.501305598879507,0.02924043944175514,0.08104323558731341,0.8196352363834664,0.4767387130338542,0.8241363876128727,0.7750297892974335,0.2580230671927237,0.08203407344518676,0.931552368225893,0.6532242283951674,0.9024038869679272,0.12090452401655505,0.22825374352893157,0.41149351144615465,0.5164362673165933,0.8622817284966545,0.307290338382899,0.3616979540437515,0.19277000294044566,0.3393699987043126,0.34184801535886933,0.32307106374485894,0.34902444980415925,0.8523608041710469,0.30546487661528987,0.48320920978651705,0.6918242030303536,0.9782735024122107,0.8593908379673305,0.9412235214799178,0.16400607530720812,0.6710605703668885,0.5513836033383531,0.9575534575574538,0.36425605237917746,0.19829574845857412,0.5980359822939424,0.42778494633683384,0.05783047120270279,0.5153569781923284,0.4630875357625378,0.17761046052059437,0.555500838299878,0.9637524038810417,0.027599862441618495,0.7936707072929139,0.058063637871596074,0.8602674129248385,0.8858896725410799,0.5310814321981795,0.675744225608767,0.2046501168893372,0.548826533293952,0.13720525811412077,0.1871535624078824,0.20388560437482917,0.9874174617170715,0.31735742103944575,0.01925542322785123,0.8647959727527458,0.7927441160067127,0.06234783778744657,0.0753663306138741,0.25298555387613286,0.7321938007783205,0.8886896423244345,0.28800232196451137,0.869853168871922,0.7700518917677651,0.8283045760201462,0.4824212700947671,0.9423560315136015,0.7031057086783401,0.7693040753769315,0.03475943060588493,0.37858075771652766,0.45572206177449126,0.33448126132562084,0.9277442103781877,0.6411094511123023,0.8713288879882101,0.7071424459453083,0.21068234171777434,0.7597017623317527,0.29354173307129616,0.8614681834309871,0.5819453915539455,0.7074795004510037,0.11328297550357591,0.3511261097331069,0.2590293313437847,0.3144073379989223,0.14915754061628395,0.8165894494318074,0.3061786841140045,0.26529854158692645,0.6601575395837954,0.05089735035575327,0.9684104694264415,0.830175542201278,0.008075913476424446,0.8812465356796482,0.5158038074241449,0.3127707588718096,0.9786518775012413,0.8811623443609117,0.057866303073144,0.029542074451331946,0.9473785149860482,0.14030313105788472,0.37667955233109396,0.5597553182189383,0.7312557589598454,0.9401180339141867,0.7454073347721835,0.2226752091893751,0.20054399888468166,0.41700018467496014,0.24282459966729908,0.873169777660923,0.5744858288303903,0.11707920198955946,0.6967748611142367,0.45716491608099896,0.4635704161412926,0.6586200084709177,0.5605908342096063,0.6371520983980127,0.5267026518735105,0.7983828968579625,0.3757539916261524,0.07214372919586576,0.4965155026754219,0.578809405164485,0.4871160745783292,0.5654414335731723,0.30173520372650664,0.48299229584330805,0.4228597983412218,0.7922978081968135,0.9526207970441777,0.07331697954491356,0.7871434063910857,0.4688951188805047,0.3450525900269651,0.2756610091239967,0.64623623359554,0.008223980636105743,0.4175120438764387,0.5068265964184139,0.6689136761138024,0.4920434677061645,0.8799463287728813,0.5800379238188971,0.5670397090811659,0.6113824762576469,0.9003924354429378,0.1967571484899957,0.998052314366261,0.836008215668953,0.09700696812533782,0.9796898225578985,0.0036687821115585217,0.8057842675440133,0.034867276464950026,0.8113186065700637,0.10430646131383614,0.05617031909653902,0.5947546471666323,0.09857438502273741,0.519957089788786,0.2862483896682062,0.603116706673105,0.6188857254759236,0.5142746142697854,0.03382314277249199,0.4517748112045503,0.5504493047650637,0.9920681808293779,0.9848168326762882,0.9807526083907553,0.8019080423157973,0.08220725753659386,0.005999297222298083,0.0517815190833556,0.9344732147633672,0.4357035064452216,0.7546467183019558,0.483009611367337,0.11821205531927315,0.798112459028997,0.087760362715252,0.5025973420280744,0.33216198154595955,0.7536967605187304,0.5973829537877308,0.2155578092136401,0.37278549650790105,0.14894906986703493,0.8113130179803756,0.44362376508787593,0.027492456244333607,0.04611599340435901,0.29117698421119675,0.9276577008212697,0.8403911801523029,0.011894644525997422,0.948292611485691,0.7409325211415898,0.004590962281041944,0.5066457634424218,0.5485352984484081,0.7953087984304065,0.6195889304951105,0.7455045265506325,0.9077189779188125,0.5878710517432219,0.45179565380547837,0.8488991471983862,0.5900194518963565,0.14180231138007815,0.7339052835167824,0.38205162724371655,0.06662807165431861,0.24363829481184562,0.7757468081042175,0.41524797903274224,0.07974853994749165,0.339908144856535,0.19330144480057243,0.33950690136195727,0.9792639246462835,0.07130054724206847,0.07980761810605752,0.9897752118673355,0.7221551048292623,0.8662333024818587,0.668539378904915,0.05656359469341965,0.7994249342595116,0.8671625698807839,0.4283328494265646,0.7188355685557251,0.903400664103495,0.077830515100491,0.08423400025081784,0.8171100000729425,0.18947962953765585,0.8444081027381694,0.476140076103468,0.3178036540882667,0.736479903654524,0.051108006826030006,0.6814439651484991,0.7209615883721735,0.03790620642840925,0.2380166927020626,0.37884977729624525,0.5614675161507691,0.6495080028468733,0.41382364170103914,0.667276620694801,0.17568799633715182,0.9107141094214021,0.007850782777502396,0.07485927941911452,0.847923916204841,0.7604238955574301,0.6077976859975931,0.6090575144296719,0.9411690500929445,0.4490087130366883,0.2645165234938013,0.20054741030121337,0.023588988431898383,0.1412473024820553,0.25693002392493514,0.607446193020867,0.28575068568600615,0.7644358802436321,0.8117721852469097,0.9504226861546153,0.5857308095738376,0.1269423756220026,0.29989735782593296,0.9068475872733941,0.8236091407011936,0.23001578609848738,0.11904933983047594,0.3279546834782706,0.3007704355267261,0.08578123747861888,0.22358828909549433,0.9407788265621243,0.31352987390892273,0.43283905352652996,0.7260045914871704,0.4341156450395808,0.6908301357513027,0.5854257921820187,0.451795250533218,0.6696425826999063,0.24057143553102323,0.6975495684464544,0.6927389976450429,0.16727588884998668,0.27222957424166316,0.43126341656162803,0.2738391593773628,0.49330083938580593,0.2952341995822708,0.8325196738465116,0.5689224236431761,0.47132563310670883,0.7722067791516755,0.3721992320982993,0.04766062606495114,0.49982205072199914,0.15838920352102737,0.6086496633827504,0.30431584793611866,0.18482404918156004,0.007332677031389623,0.03879866836038515,0.8625141998142577,0.932902038411531,0.6211057486484287,0.8078672955662246,0.21819867573260732,0.32079354481728195,0.006847280121651478,0.2715198174478032,0.855513570279846,0.5587062745070444,0.3130564399130641,0.26085538417738785,0.7339063632194525,0.6875922021498163,0.023150672021863383,0.3496141344169442,0.6695599719966395,0.7804575540894468,0.4590368051972976,0.11219096913827009,0.6527842425503587,0.8111273751653583,0.7795469987271125,0.2117847244953861,0.3604637008297158,0.13320357093257673,0.9194584577044573,0.6683807622788288,0.3862710003736298,0.629704606771983,0.7232428323996398,0.3375886360462048,0.24773665422992186,0.5703550235871364,0.5052727197181858,0.056170234085179405,0.6022350631541371,0.05916306662720516,0.2858271071033741,0.8627136298132587,0.19044624942934962,0.05764319836801224,0.6435038345131568,0.045832403432489954,0.4635048013573927,0.6092648743921207,0.55097115512791,0.021238205398385168,0.8865099146778919,0.28523512816916685,0.8804899512265347,0.31821872853990674,0.21261508879476088,0.6244005348413189,0.20129717115380585,0.7064772818938488,0.9702686450609999,0.7570947398876666,0.8151997554641768,0.7919695294019158,0.23182511551141738,0.309273295662711,0.36983738806936184,0.00608677701212923,0.6009714029043044,0.9490988941709088,0.28681912369695395,0.8837271717116046,0.6981805236358404,0.3329885862002888,0.6555532256084721,0.5331150916438719,0.12354066731446722,0.8441201798743455,0.6442737854389101,0.9254492290265665,0.07579655744117064,0.21998403036037617,0.532698049470447,0.9437459478289608,0.22496521213071663,0.984796896542077,0.3071854476675083,0.6628763145222702,0.8370351130257355,0.35512710646780077,0.21235770967706358,0.3279957420794012,0.46949762581448107,0.056731806006103835,0.2602632752070677,0.7862444742992823,0.6477999588600657,0.6554543399441162,0.8248336838781081,0.6017880237799448,0.8139447700165909,0.4040301740420056,0.5407767591349624,0.8260181225693561,0.32030849838664543,0.43004804054985146,0.41861920902112804,0.10242775408599825,0.08458745966461667,0.6946178227370605,0.6646384915145723,0.655725629676886,0.619759777312031,0.9899312680155343,0.22195597475023576,0.21559237307264045,0.5277495752045133,0.20084376914168944,0.9153402203422498,0.4015791187180078,0.3005958887031067,0.26021893901581705,0.27202053761573297,0.5528142866104536,0.8589343280707331,0.6577930846226309,0.3989661577676048,0.8219020989798701,0.9318782075338317,0.6764125407389914,0.08737745738991898,0.9731595598556888,0.6263807537311658,0.6694153951188537,0.09442400776233129,0.6489455976788674,0.42410562953594855,0.6226585491342261,0.9784213652448319,0.17912115870640655,0.5471045982978952,0.2770268068581271,0.03246661516950866,0.21536372850143126,0.6014500674316204,0.6812341901967907,0.8080557247074055,0.12802901218623963,0.021823048165833248,0.010105145027369034,0.6931521711319277,0.3543300965116417,0.42059428503147356,0.025488923168810684,0.6064494305076034,0.0664674861717427,0.8212678302538655,0.744902924227854,0.5224889721049337,0.6768436207722572,0.18704590693614753,0.5396010971315826,0.2727160288095085,0.8811514391514425,0.6491943792607952,0.9382089419132712,0.22893095483478498,0.6022703328111875,0.031141493353981642,0.6066550629900087,0.353230784704452,0.12321060182705146,0.1956368163183415,0.2387441783984079,0.0006809801555976991,0.8701697952797508,0.9873119336676988,0.5047057394505934,0.11941024349760443,0.5673844839530027,0.4245740890275884,0.45446961421436527,0.09336092145946384,0.06945409642332556,0.07468303665271181,0.7664963034818013,0.40378589794602127,0.5010379161692404,0.38601732486750473,0.7046367717608224,0.2634385837778972,0.35198907249245226,0.8414805463985711,0.6418420391054396,0.13241770331091196,0.8114955201542734,0.1957004222059231,0.6854406093784646,0.5227166584858417,0.6500633454589255,0.40354402460298033,0.5582994325162763,0.1020854043220144,0.24890575129520887,0.4000202884072307,0.4216485626218305,0.2658694664930634,0.7770721848939017,0.9566892279429544,0.25418138871500484,0.27115613944504313,0.9622306906327125,0.2498422138614943,0.994603723112545,0.6547530412519179,0.23007498682458816,0.19911091027965866,0.6072029086458108,0.8666295009054484,0.00854689355560867,0.2072561826287419,0.8693551067549572,0.9358978525205429,0.6267606032457219,0.3056914103548124,0.5079564608022394,0.23181711827763607,0.3925922783788931,0.9529995666626355,0.7579162099442789,0.8998387858605639,0.24579532453174024,0.5720042245310277,0.4802050449882971,0.9129387621519611,0.31746291393024284,0.37311274161112684,0.7586615105293761,0.8785758984860313,0.10034295765120671,0.6811945255259229,0.9429808480220508,0.6806464875727489,0.11011079613066277,0.8345816952455672,0.5223100990083168,0.7124949837658078,0.15401552360937598,0.25257324133747805,0.7778151098441795,0.4267232207239967,0.0583483638363661,0.8268291800220093,0.22672434468470315,0.48865886434522254,0.4184448288866902,0.9486613911960122,0.802900701172625,0.2799400939371621,0.1300979740027135,0.5931528026536643,0.5267302446356751,0.7553806106926467,0.7515924370714745,0.5912534343750245,0.6126266921513915,0.5412222347617491,0.49823894788189593,0.11094061870583205,0.7951348903919472,0.5204771964899276,0.12793258024846343,0.6478227038213713,0.6616625295364585,0.0652249993365488,0.427726893765914,0.26581827070775343,0.13851156334616477,0.9019432485796642,0.4915843092920761,0.8198489753603022,0.8394823692498863,0.4185591353151792,0.5356618291355653,0.3639975489705426,0.3496654314439068,0.6047271472422074,0.37760630702788733,0.25730549963018456,0.6254784322557833,0.9777544812285601,0.21710173928987087,0.1918709098024698,0.1660164554777579,0.9242466459678745,0.25339032298628217,0.6167663166084785,0.28017087289683795,0.81451510499305,0.32766987211378995,0.9807269613848895,0.9614635532070477,0.20686219817475326,0.6996989811325608,0.6628897327750327,0.1352716981293115,0.853142371137291,0.6957519700226229,0.5207421755646241,0.7397238709938847,0.5418609517826874,0.9824216073348266,0.24633969697535563,0.06547837563493308,0.3138632507317718,0.9648815750885072,0.9795360953661663,0.12643289481754982,0.3369097421444074,0.38805997498739486,0.8411323618545794,0.16746771992511333,0.3895415743707853,0.22416166012954442,0.023258494110383165,0.6451408537948016,0.14523361921665567,0.07872853634761257,0.12412547766048598,0.5875470909581881,0.20181788453822602,0.12307289253247011,0.4996263477362497,0.4096273794376236,0.6559618739532622,0.7768138875494531,0.6414540105971732,0.2348265340249419,0.6138443604368271,0.8951435175897303,0.3237243743517372,0.7379866154011586,0.47353904463921515,0.6000654380665464,0.5674636089510716,0.04519061563721105,0.6297816889350842,0.47375471405793634,0.5749139672816105,0.17504115665531073,0.24439527181859844,0.9986316714391124,0.46658024714525625,0.6078804518786459,0.2472989056629945,0.3864716770400669,0.9757861511477887,0.2127296338204887,0.7820611425315216,0.7117669780407277,0.7195977666276436,0.00867835155855734,0.44085823678605895,0.9664383136731405,0.6272308832465189,0.19037146294088236,0.18829724723273011,0.7813168907639595,0.49016694528761073,0.18323104709158966,0.18280928645128847,0.8481752225253948,0.027388132119428144,0.9546081666540706,0.23956616066100023,0.6591021471993892,0.44438456645326063,0.24508539471046342,0.41661896202601634,0.20870094023757169,0.5276668643936638,0.3394791922290794,0.3531558295515522,0.5133037179824216,0.41831430776918876,0.900059751130457,0.6816114300465268,0.4383626469128211,0.783559649816939,0.9728139932159607,0.49919797226592544,0.8103609606674295,0.3969945790869793,0.12054049179086024,0.6101891808233549,0.1281477114853159,0.20428066545240064,0.6122108872190203,0.005092058800514221,0.2184850670543742,0.530530478806757,0.04958080561108558,0.46790597673970424,0.04840216751799986,0.4670473858191544,0.9027480589235869,0.921876404197032,0.7564357973812698,0.4708209382289724,0.9102457172327787,0.14031121494052745,0.6259851622915826,0.7801423609434038,0.9271353789575544,0.6060520599318959,0.3457561483454573,0.4501220818195931,0.3523190031833112,0.7460244601643241,0.05189694318046878,0.05634864359149783,0.5078279021418522,0.7965824145532924,0.5673080255253885,0.511350471281945,0.38445664194051454,0.9106008995805975,0.5222650221571933,0.2541970526755166,0.8047529617788436,0.3511795874480902,0.5789116810159354,0.05908370949084463,0.7384330505540163,0.0463089432786713,0.5584023446474418,0.06262570463046346,0.31554756678494267,0.05249601369960333,0.19492831798497523,0.27818896200949905,0.8555138891541422,0.5639406306036329,0.25418274110697003,0.1077448828935541,0.1918930805760204,0.7620431596726251,0.7349213994039194,0.5453037107231264,0.3885308944470528,0.041589092977295294,0.16077887985714368,0.8346785597412415,0.8459734940351387,0.8048133574726419,0.49104385195160216,0.7135872622121046,0.8004920187544523,0.22110848049881748,0.417012705758508,0.0028391099571177802,0.01808817601476953,0.024421926174491015,0.6531093805750807,0.10425141845878816,0.5651893250298106,0.5967100787694999,0.055330884490394916,0.15972187295989593,0.37431013153145865,0.4950720878825172,0.5633049124591488,0.8082462242455096,0.6543010094563193,0.13193782663920584,0.4487601373414243,0.9699368131327766,0.26798621750199036,0.18768416079024275,0.8735471487226935,0.49868387955298943,0.9958528845330199,0.21905912164705843,0.4628508026841496,0.3311192642740318,0.7971357008985916,0.7645902144706941,0.2875354108217463,0.909417281755697,0.8443712067159211,0.8568925534619668,0.22550732397987594,0.8306150106374339,0.06977409806638157,0.9114586785252025,0.8379197702314123,0.5377866257845059,0.1399834685166257,0.8865725631715798,0.5872927996059086,0.3158315365200517,0.05189455726270131,0.0816414827456774,0.8093606743070036,0.0864612490285721,0.8027317008114625,0.15650671976781205,0.20013066591576922,0.5266070385512986,0.6565174253921201,0.18612104028232257,0.8976132934851399,0.13839773856258208,0.3170215110887076,0.5949309678721502,0.18903659626263736,0.5225097338414009,0.7583445946737076,0.4858292288788778,0.10082829641507485,0.24320501260167204,0.12938238900127308,0.1373772048520906,0.30274098633604907,0.14181377570246434,0.9731884607914257,0.36844308061910713,0.5814472758945074,0.011386576484796684,0.43506570370736974,0.6517514309748034,0.8978247120123758,0.771428263896079],"z":[0.1900140342079966,-0.017559974767502068,0.07401687624389894,0.10880608557845402,-0.21445949928063152,0.08556314031363713,-0.06388830401975742,0.08739710835377007,0.254643803042109,-0.030889528376100178,-0.16123366272049044,0.1491176120037063,0.004833648198968643,0.1420690638316705,-0.08476064869993152,0.18939360729485205,-0.2690667445177383,-0.2883924774877205,0.004736589758373877,0.21515242179496463,0.18668447757556983,0.3916398671356899,-0.050041313024624445,-0.020224394352822243,0.2223873593691364,-0.11594899660183115,-0.04048951438764767,0.24574255094352682,0.14833884763480296,-0.28323578680650946,0.04693209709868769,-0.314804337753456,-0.29652765463578823,0.014727126025508491,0.10232761867779194,-0.05392647402048228,-0.09193001259257964,-0.03217063715289673,0.0012265756208495876,0.254454377098365,0.2066184758030603,-0.13961227235321516,-0.00744860127056937,0.03448355260044804,-0.16056353076345084,-0.24104417748897805,0.35762867399150267,-0.08699696431422677,0.06252817745768727,0.25821097959363687,0.09641553277736489,-0.053775245882418665,-0.11149327982774904,-0.11020335980879088,0.1400846529213357,-0.3550506441726803,-0.18318505674111352,0.10273954006314706,0.14280864405795562,0.19561485896395275,0.027503331030453222,0.21735516584477788,0.11469943552706487,0.002640065091465485,0.19865710198492892,-0.26054244448506725,0.29199439813167377,0.4553038215202182,-0.0631980042303563,-0.06926598098830392,0.13915238237102878,0.20564163088563067,-0.2091465455062242,0.05343866642067048,0.0070404391409659844,-0.2855213829349277,0.11142074171374244,0.14470300375182737,0.03898777709122254,0.15327550681514884,0.1922633989908792,-0.04893616934321085,0.22597618266493202,0.183442871737314,-0.18657698156591745,0.46774459769880167,-0.01902027648444135,0.051004364057010344,-0.12315172902857217,0.11983590175585067,0.0013803030267964256,0.28797002790521786,0.1655324709175992,-0.2715458428286329,-0.056654631611527645,0.17786566672454215,0.16585166489387274,0.22854806478995096,-0.14078704725615124,-0.006635819646412147,0.03395595081506653,0.06901900086642788,0.02358923697888441,0.010338739109916076,-0.21031005687354026,0.1951287520378101,-0.14710555222582872,-0.009578553127551112,0.3068660794168091,0.0832603966121706,-0.31928611831060344,-0.25599115407834777,0.014384277467034232,0.23761359721072448,-0.26585973969683896,0.021264232559885105,0.30037073405258774,0.026789935987475866,0.012331308615984637,-0.02196403272508627,0.187700516038314,0.12300678163458457,0.14028821961721294,0.33174010023765743,0.21632240132167874,0.0154126230022721,0.11741199679874893,-0.12303283789690549,0.286912313788453,-0.07596405061888145,0.08374980275795413,-0.1436247000836583,-0.057838882476546574,0.029245439581868954,0.04607309262460968,0.24054810798660942,-0.1289122400064428,-0.2492906103736142,0.01969030137217247,0.02099079112699946,0.028133271297165294,-0.10210194636144165,-0.1533037223864866,0.04148556993066009,-0.04760393610565739,0.31427502239544386,0.29151111639634797,0.09047656065748205,-0.19894733997577824,-0.2870810238854856,0.25193567346479784,-0.18607350433976333,0.10688148745293183,-0.04162479847717995,0.10915133230328945,0.3641922294301553,-0.06491957755977934,-0.004900750332979216,0.21163261022929078,-0.11715998593363569,-0.4432865323565569,-0.08780664596730192,-0.30449688079827225,-0.13975767233765352,-0.05381726139272203,0.021740658956532793,0.16324645505321528,0.09254889918540038,0.1500296006870414,0.06324305400378706,-0.03660446351461588,-0.17070110676705372,-0.1938762818579025,0.3099908442874937,-0.06883717466406106,0.013044036877785256,0.0591654622555331,0.07326555469203197,-0.11579807151215964,0.20027397750580656,0.30927616218637294,-0.2017794838596949,0.006676050944096473,0.2884645545477023,0.101676973324694,-0.0009321192326673649,-0.21052800502504326,0.01819414377949461,0.2800252551356037,-0.27151471864349264,0.0802793689765913,0.143052585865785,0.2635510217932321,0.16657813662651666,0.1019426335689775,0.20953039542926533,0.17746414836235663,0.43015298204260033,0.14362173710820397,0.11823302535824161,0.10143945421673292,0.21480017517364444,0.04883261633580792,0.06606968601181329,0.11315841310302559,-0.10895911584936002,0.10690858139506862,-0.14779670320457322,-0.0269542389284316,0.18624645602013942,0.1749186299241806,0.23830861245468585,0.009416053825556131,-0.18312068029831313,0.17416389798496756,-0.5133890183664284,-0.28026190408839813,0.0764526895534604,0.16018589828783253,0.07954630556616846,0.26149042042429455,-0.22449536807298895,-0.2108823329090065,0.28881805212116607,-0.004575923887697619,0.19892029943063405,-0.2328019583303006,0.21647163390707008,0.2204296229120928,-0.004762990871068018,-0.0802850577472349,0.08858523080276763,-0.347797345164214,-0.11294924055846108,-0.45928853479809006,0.05333679094705743,0.1667463344213094,-0.01797978401437819,0.14920002473286392,0.03285581245239557,-0.1756059397795803,-0.39654822491484343,0.06634213412965263,0.4374912566926718,0.0054965064077769354,0.23671987917002696,0.17095772178071944,-0.07195707166351023,0.009990656515913507,0.15023105110343865,0.03516290736379912,0.17976976865971644,-0.1915196893958857,0.07475035482143982,0.1140058591061881,0.23489982759518702,-0.07638627204239791,0.0973835919107462,0.28481407768721645,0.4360990166806454,0.1639630137639727,-0.03338723737126964,-0.05621767127720574,0.13562492625995592,-0.14960453564755893,0.09521820172602609,0.18736190231225697,0.1890896599750096,0.03812033475264089,0.09851175576569218,-0.38396163268256805,0.1348949336256647,0.19826064313152253,0.03026200050152758,-0.01011376740277048,0.1408577341045619,0.14229428049661258,-0.09577103965366923,0.05589784111071325,-0.136268790891511,-0.25152164277708106,0.15961005155991922,0.3622275029294004,-0.13328000283365415,0.04936936934195056,-0.21351700378695979,0.08343762594156012,0.33512313314200265,0.046760443531040086,0.21463302873216583,0.2156324005186513,0.06642538935087235,-0.011437257471578508,0.3652866312209476,0.13861373835560004,-0.06219664994664609,-0.012831909195504254,0.15789233623820392,0.21152963264012353,-0.4075215140426156,-0.020829948607111237,-0.09274981733343696,0.12998911499874646,0.25234856998405986,0.07046869722642239,-0.06193107619865995,0.254438576713284,0.0784204430989001,0.14863255106698556,-0.14672882500712425,-0.1527685515756732,-0.21081942369714995,-0.1495658608398989,0.15972848923087854,-0.046812363860347636,0.10360925764129632,-0.2413330531950297,-0.08459104628429148,-0.1360722042053124,0.040070576853440995,0.042925627997824164,-0.014862832742528278,-0.057504123941473965,0.3998452946322666,0.08846297950711987,-0.07743968849308196,-0.22777013343607616,-0.2818245700474781,0.22825010405431612,0.09205346976233275,0.16572425901747453,0.051567077190934014,0.3054528264886879,0.10051642817631015,0.0825015830269755,0.018860948156993268,-0.15382198231535785,-0.24144988388982788,0.14830541417454085,0.030534918124562965,-0.28593824707349635,0.19616577732813645,0.008803703520281297,-0.15792923240413748,-0.22723006897802336,0.08407391249250928,-0.30674134192617153,0.21445992119202817,-0.07822364748007937,0.05347545482611359,-0.15644791860903906,0.3715019742143015,0.12739855907714884,0.05107072499630687,0.17234423385899544,0.015883834309802405,0.0009095477003625441,-0.09569820784350365,-0.2611181675439048,-0.2592624527574774,0.05725289454037244,0.2755643942963517,-0.3235847564418167,0.15107545459396335,0.29331900035024766,0.16381414695544255,0.33589732504198433,-0.06934306360664988,-0.06025328384164205,0.059790734722130985,0.011279942665827117,-0.05665902452307914,-0.23580977266569356,0.3217988717012242,-0.11921071380262824,-0.03738429937079281,0.06994319213540824,0.10815896803792076,0.05490268471397839,-0.03756270502623117,-0.11513970835884937,0.09435515683182388,-0.30186002389182803,-0.04221021328806569,-0.15054879977488594,-0.14172581186275288,0.05543521287185839,-0.10261886846785233,-0.19174930471649718,0.2843851334260964,0.3837244282865897,0.02524531103611083,-0.19213194925195154,-0.25423665245973426,-0.2444898456703477,0.09254134891320925,-0.03815136614153086,0.00569619334189829,0.21155397857268762,0.01937576247464895,0.17166612816069093,0.10850267210861667,0.20719987783456073,-0.02002965052516432,0.3051639810323856,0.18152882368829576,0.13099574665357772,-0.38218905880459714,-0.09611734246926518,0.14094330726125937,-0.1692415944949999,0.11707473913809105,-0.1292787710968823,-0.07707982623684274,0.17388247233835513,-0.24816572804021636,0.13310124522152758,0.17993539258573382,0.2233301633783136,-0.15290924324351066,0.35584174789523937,0.012057468740978405,-0.015867095481452875,-0.07038170048640463,-0.32246253542268855,0.12016816481873993,-0.276100954008953,0.1693804084369698,0.17171273018388142,0.1458969698068539,0.1869333584866491,0.18422985046317922,-0.38231569901518975,0.27362098138172114,0.20857261633817747,-0.20665881140778508,-0.13776690870755195,-0.14638928977789084,0.11573767517808844,0.03381840666759084,0.38295072912800876,0.11471076147423817,0.058161156245054094,-0.10170588530960972,0.16261259428947838,0.2211763255211194,0.20703796115582368,-0.21527468487272106,0.3813663520366856,0.1530499479131129,0.2860583551583379,-0.32295022039688415,0.2732258027318051,0.1322951279935548,0.13949121681028717,0.15498575943645476,-0.26838405800545717,0.15169302184027875,0.07304642386936953,-0.19928261374276351,0.3009546108658108,-0.27126878788094877,0.058669747118972417,0.029934619241496124,-0.1800335856079849,-0.04654239502785197,-0.10534943163632322,0.2826757075100826,0.029283471037211087,0.17871362332370494,-0.07670156590624906,0.39900181581423044,0.14935954060594994,0.33770780004376355,-0.05228510668358973,-0.01257059925435449,-0.06865621784612894,0.004907831796467536,-0.21103689798244668,0.27140677837362503,0.2598770321122288,-0.03938196299436071,-0.01118740410094654,0.3722707941163441,0.16678794597467056,-0.03752713969365458,-0.195810458544927,0.1665137579837182,-0.10828936969365421,0.0046991891422796805,-0.07115503750067528,0.17808196884270622,0.15475213883691338,0.06609093046502652,0.06457769125477834,-0.10165157875721427,0.18115884189354,-0.236472252729314,0.08517474773229212,0.04803137298807964,0.2035295963512847,0.3420230950849055,-0.19017344184206317,0.16015744356236836,0.045745462785331696,-0.08067284767844476,-0.015749262161305656,-0.37845199539996643,0.13634153086753875,0.12253722260453956,-0.2469508285646843,0.0016400497640235087,-0.1021379644823073,0.05668002751878945,0.05676062112834221,0.041199287694120484,-0.19375882881840623,-0.26593974096212386,0.041114395854341526,0.13787947080434587,-0.24306096402582583,-0.13244484009636742,-0.07764927254824344,0.19738501378447082,-0.30237031159790584,-0.18153402855847947,-0.23573710655444188,-0.32720421531112637,0.15689855920754725,-0.07761097861815788,-0.029066127335661905,0.13623278563967628,-0.1442170004503972,0.22751062444594294,0.005034494348363789,-0.3482479638281548,-0.1058984414479835,-0.15074440907875722,0.3622589268096822,-0.24991971214261754,0.03380587066202714,0.040628302991137205,-0.04779690858098637,0.06582476488791772,0.031015847042466925,-0.1366219957880218,-0.32485902699281616,-0.07017432558186644,0.0328144191222586,-0.06565077746907663,0.38688209209215196,0.16402204361992131,-0.0597385611451029,0.2287414923338378,-0.18468274656021283,0.29151793105209783,-0.17945041033500203,0.17333542462340185,-0.3508726105177101,0.1758133459860326,-0.07651540975161555,0.05881269277249792,-0.3097909298425968,0.009476999270205504,0.22200144133380212,0.03146060754195883,-0.10273903448184352,0.2517552953707542,0.19290673052093396,0.2180298230559044,0.1844932343631119,0.28317497831682464,-0.18880226533203018,0.2700852236124192,0.1353026073248304,-0.07018266126833025,-0.0450036251859548,-0.08332161544175667,0.25251864728784224,0.12882477303197845,-0.17509828322026663,0.19300067689018433,-0.09866113700914719,0.29827220591106496,0.2811424742289973,0.16185170777471203,-0.1274012737657303,0.2588160262677072,0.12623032830932432,0.1840335040209087,0.189364145184863,0.021005298865970884,0.3206641939019202,0.001630706846417057,-0.1206032313555682,0.46959816497705603,-0.11670364578640542,-0.05530613962192594,0.1860573829775401,0.3583510908904565,-0.27033789610898834,-0.04035032349659612,0.16111791544858708,-0.03544250408306587,0.4512769924284761,-0.009112630265082217,0.18908905869760334,0.10266770767846521,-0.07052294426555372,-0.010664179973943817,-0.04889054250108634,0.1110547588618036,-0.15850638051193372,0.26274171165296367,-0.07377313048619556,0.01006439691662481,-0.02455316014370191,-0.37562739556981073,0.0518907907345747,0.06283488984360178,-0.03619767493569209,0.1767330069714389,0.06417384326702326,-0.26558732793375595,-0.13426520384981872,0.2688398111483384,0.2224075082630383,0.10580512092929246,0.02094623317077665,0.22169750772750918,0.07320650039635293,0.15247768333763717,-0.04977333296272417,-0.030143635894415176,-0.03548200870114357,-0.15133213416960717,0.05960397562148875,-0.07034532911323584,-0.005915985592557363,0.02921996652622986,-0.02307854360091397,0.04338186798809008,0.24699638957238546,0.14683544976791824,0.09466061261502276,0.023129861893777342,0.24064695894768484,0.16970617296264195,-0.01071027625951633,-0.07053982237900736,-0.15208169541980737,0.18162691745428206,0.11811237227594848,0.2613074389494558,-0.383660864888013,-0.05579877102971073,0.2681987576832695,-0.029347992747643492,-0.04655818085410393,0.2354736672077009,-0.04816100918703137,-0.006932384067234912,-0.26445747177207213,0.21302675342975907,0.32331111600336176,-0.04354419505540193,0.18307690215741684,0.2539383815800309,-0.289754932841048,-0.016386568120281053,-0.20136188510622402,-0.08062692700026484,0.29252600199343537,0.19256677325627602,0.14806252130907083,-0.21479662192900065,-0.10003307672378416,0.019659704030762115,0.3823904895777894,0.2431524634048472,0.2551923611070046,-0.2994446646565775,0.10305891932516681,-0.16160895027873837,0.0389900170796057,-0.22700886325558403,-0.1058228682594621,0.16426055079653462,-0.06380779440752897,0.06972134874364189,0.2440899925974922,0.3250309809354813,-0.14956587449164147,-0.27115805935594134,-0.03418117958515663,-0.19002782255033146,0.23405245227482852,-0.23897841036456452,0.10994919580125397,0.1563156392488927,0.3322959176594697,-0.22937712215268138,0.11466749116260108,0.11471231575429039,-0.05344857964220225,0.34345725447021525,0.07204358737145028,0.1836031423554929,-0.24648913524996477,-0.297710926415344,-0.07202773087488691,-0.28566299269359974,0.35251276429436196,-0.22211704573221375,-0.004660550157818001,0.1161893066531029,-0.3084485653226971,0.304861533127178,-0.034658773171490714,0.35805770092774736,-0.05748205045356809,0.05188421276936833,0.07495890665581587,-0.01480422668002103,0.2624206920411025,0.29360814044720496,0.20937839627060922,-0.10669003697638768,0.06659935990914546,0.2679509747604231,0.27006124998803716,-0.19369452229156764,0.10504279656528405,0.06571922310026126,0.027487720289289336,-0.010087738945123803,0.15554510971335506,0.29133519653953255,0.25324122047038744,-0.06998695858060873,-0.10611857932722828,0.17355142644496502,-0.2534238845194823,-0.12689292790133253,0.007021712452790707,0.12023513840830455,-0.1730038034684182,-0.10338570868128051,0.06289853280932711,-0.21547065850230376,0.04098344581368267,0.19442014644646424,0.08243550047123724,-0.230888344028444,0.2102638280324044,0.09700569752891788,0.22654325400755077,0.009193858734785565,0.004000043308911853,0.20724349384451035,0.036680397000847284,0.34332359172424876,0.2500762519903109,0.18613775456718828,0.08149829216196186,-0.057690740758918835,0.21448281677733902,-0.14123798268678361,-0.26195952931759525,0.03776198340181576,0.13953758174456382,-0.08156417459003029,-0.019855272906383323,0.06718547149722214,0.10464661084495286,0.1572205496919441,-0.24801226470836835,0.2931783169197777,0.08047524454092234,-0.01873524971071952,-0.03206627557987454,0.12273619216425244,-0.18960098468239706,0.4185880228586881,-3.0301110787278596e-05,0.13628149435463416,0.18723263097992526,0.23330551806939845,0.036861622715196385,0.4740047330993725,-0.04289694973365471,-0.1585070107820478,0.05479642448877181,-0.033646454777879475,0.13469642505206664,0.02969942219128003,-0.04484588431132963,0.1484975125404588,-0.028328376522962474,0.10799533803769891,-0.22908971656582786,-0.3354840392973181,-0.3064723332189253,0.17341614860102422,-0.25551325673825587,0.22311448620847565,-0.1164574577101413,0.2627744287419966,0.030147674822415316,-0.00969558981355094,0.05500769737505737,-0.24378471266635643,-0.11647912967568903,-0.03790226901532077,0.046544051555949734,0.13971583075800217,-0.20137562287889818,-0.06004330672139947,-0.13787953645564804,0.13657428248285935,0.3046719888490164,0.20117514296215436,0.32040545970238676,-0.17420793161469317,0.21773510398899082,0.04431005015367549,0.12317334027425134,0.19916035221248887,-0.06974030806384068,-0.2778152939397447,-0.38260312929632745,-0.10022484188036675,-0.05100717164488342,0.07604735346256401,0.09340874656767699,0.3785935261280443,0.0630231334889858,-0.04472282322060467,-0.11791677595538669,0.02200984610563402,0.13907195531711042,0.24200927959599597,-0.32296944562032437,-0.06496702558523001,-0.1810469011795514,0.2757555178152866,-0.2740475354048786,-0.0329480403429058,0.2255958103058021,-0.3800033061555175,-0.23573714440308174,-0.1345893483120863,-0.354189552421025,-0.019229148079205047,-0.15088079518191477,-0.04460768711507085,-0.21254341284102846,0.13698399458530366,-0.32516591212197216,0.1628150550174206,0.275552238182429,-0.14600581357077186,0.27063216379765653,0.06505384049281077,0.1347266283859915,-0.1451463135445761,-0.06377319034379604,0.1315082614232588,0.018375629650856165,-0.016354738589118803,-0.15499836592088248,0.1823850718223964,0.011230219287285209,-0.12999521145774823,0.10333832878567069,-0.04054740490696264,0.07964308167673909,0.04740969809413066,0.23975917001973382,-0.18284935877511488,0.070975953230752,0.03600401536775928,-0.15134487041469508,-0.256832443503967,-0.07775483624050814,0.019299764421273374,-0.12616379402454003,0.009475849194466882,0.4256631450946521,-0.09410109530093638,-0.3392319689004539,0.18782324114847349,0.13122067697280856,0.18291800442734557,0.22700627084093386,-0.10683258857317415,-0.3132820340576138,0.004308675869347661,0.1551355680270834,0.15654427399188078,0.1940213524844934,0.16219239180312273,0.07421134130891859,-0.010919555259732368,0.2025550216318261,-0.044309834958284854,0.012725382620664083,-0.011945196135599426,0.30852339214528957,-0.1367215254099949,-0.030204234661117754,0.23169995891563,-0.07724434331511207,0.26064989805898703,0.20267799517854793,-0.047770648655267395,-0.1469888698504822,-0.0734754555631717,-0.11468942352870243,0.054187212874720544,-0.06153967326843812,0.2536258972489585,0.02852927563697029,0.4041856174969919,-0.08702293619438602,0.04564042108364242,-0.36649459134018814,0.06291069441703546,0.09821566134972629,-0.0744920290234342,0.08463059091302375,0.04307344528811295,-0.09792762175534851,0.3344979532313217,-0.047271758415324286,0.22547044881710993,0.14406609451146649,0.42103277046525023,-0.1221660519686451,0.01977318835454875,-0.36335918862533706,-0.13608692805051203,0.18203641480010432,-0.33225436693250493,0.02705818861561199,0.22617066372476857,0.0030918211725916786,-0.04278132995442498,0.3054488149515743,0.14228362543052403,0.1848047384744902,-0.008589043456912512,0.08970151341719404,-0.09263573708976439,0.06304751324049096,-0.2216547330072052,0.03941491378141879,-0.1986238010421516,0.10633319542756037,-0.27416539374415566,0.01278115580549724,0.1727876822957154,0.1048373950973441,0.18131355984261072,-0.08213063648456878,0.22205508028908338,-0.3581391606328834,0.17907120437646717,-0.07564283014819202,0.07486933689874906,-0.02528529808286703,0.004664517170848113,-0.3690428590262971,0.3276377517128082,0.030514118101915597,0.2721294095881031,-0.0977140607641595,0.14127075528218414,0.10472632558748511,0.23134134705137843,0.26202041723564906,-0.2096222807531752,-0.15437605437538898,0.019497288722414102,0.13719443338701665,0.09844195762520679,0.09237806041729668,0.05309116575479096,0.019817953780194977,-0.03497661821133442,-0.1701771842748302,-0.014632483417327067,0.2731037188862757,-0.089812601102591,-0.4047480210716348,0.05494082821167002,0.2594580935828873,-0.19331418819137852,0.13038509613838192,0.25475463874033755],"type":"scatter3d","uid":"7fd7dde2-ef4f-4d41-8d1d-f635be70f9ea"},{"colorscale":[[0,"#165aa7"],[1,"#fec630"]],"opacity":1.0,"x":[0.0,0.02040816326530612,0.04081632653061224,0.061224489795918366,0.08163265306122448,0.1020408163265306,0.12244897959183673,0.14285714285714285,0.16326530612244897,0.18367346938775508,0.2040816326530612,0.22448979591836732,0.24489795918367346,0.26530612244897955,0.2857142857142857,0.3061224489795918,0.32653061224489793,0.3469387755102041,0.36734693877551017,0.3877551020408163,0.4081632653061224,0.42857142857142855,0.44897959183673464,0.4693877551020408,0.4897959183673469,0.5102040816326531,0.5306122448979591,0.5510204081632653,0.5714285714285714,0.5918367346938775,0.6122448979591836,0.6326530612244897,0.6530612244897959,0.673469387755102,0.6938775510204082,0.7142857142857142,0.7346938775510203,0.7551020408163265,0.7755102040816326,0.7959183673469387,0.8163265306122448,0.836734693877551,0.8571428571428571,0.8775510204081632,0.8979591836734693,0.9183673469387754,0.9387755102040816,0.9591836734693877,0.9795918367346939,1.0],"y":[0.0,0.02040816326530612,0.04081632653061224,0.061224489795918366,0.08163265306122448,0.1020408163265306,0.12244897959183673,0.14285714285714285,0.16326530612244897,0.18367346938775508,0.2040816326530612,0.22448979591836732,0.24489795918367346,0.26530612244897955,0.2857142857142857,0.3061224489795918,0.32653061224489793,0.3469387755102041,0.36734693877551017,0.3877551020408163,0.4081632653061224,0.42857142857142855,0.44897959183673464,0.4693877551020408,0.4897959183673469,0.5102040816326531,0.5306122448979591,0.5510204081632653,0.5714285714285714,0.5918367346938775,0.6122448979591836,0.6326530612244897,0.6530612244897959,0.673469387755102,0.6938775510204082,0.7142857142857142,0.7346938775510203,0.7551020408163265,0.7755102040816326,0.7959183673469387,0.8163265306122448,0.836734693877551,0.8571428571428571,0.8775510204081632,0.8979591836734693,0.9183673469387754,0.9387755102040816,0.9591836734693877,0.9795918367346939,1.0],"z":[[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.0,-0.0,-0.0,-0.0,-0.0,-0.0,-0.0,-0.0,-0.0,-0.0,-0.0,-0.0,-0.0,-0.0,-0.0,-0.0,-0.0,-0.0,-0.0],[0.0,0.003853979053497032,0.007667864019745306,0.0114019779221783,0.015017473666267794,0.01847673817737098,0.021743783700702617,0.02478462219262264,0.027567618908335596,0.030063821507524142,0.032247261254133075,0.03409522317683338,0.03558848237960649,0.036711504044040076,0.03745260504265187,0.03780407548193024,0.037762258910643304,0.037327590358985675,0.03650459181283028,0.035301825170168,0.03373180316914337,0.03181085921432805,0.029558977455468392,0.02699958488644794,0.024159307627321228,0.02106769392488915,0.017756906753522882,0.01426138921420151,0.010617506212716076,0.006863166144768219,0.003037426523685465,-0.0008199123464751883,-0.004668721425295604,-0.008468960410307121,-0.012181094288133614,-0.015766504628962518,-0.019187891345524975,-0.022409660736967544,-0.025398295780679883,-0.02812270481982264,-0.03055454501905593,-0.032668517223462305,-0.034442629153159446,-0.03585842419551121,-0.0369011734147433,-0.03756002878143103,-0.037828136027767134,-0.037702705954542816,-0.037185043448014536,-0.03628053390478513],[0.0,0.005555777676905859,0.011053756950701448,0.016436740712208034,0.021648728184670826,0.026635497516450363,0.031345169867028205,0.03572874911797297,0.03974063159409351,0.0433390804919987,0.04648666008044295,0.049150625155342464,0.05130326169784605,0.052922175191493705,0.05399052359901397,0.05449719257503324,0.05443691109190742,0.05381030627578596,0.052623896882432554,0.05089002548067528,0.04862673004900217,0.04585755632112051,0.042611312832705125,0.038921771217656616,0.034827314871776614,0.030370539638916186,0.025597810673777522,0.02055878009145691,0.015305870421763745,0.009893729242093474,0.004378660662470689,-0.0011819604228084737,-0.006730285223219727,-0.01220859287013767,-0.017559890904152865,-0.022728508184532176,-0.0276606740526087,-0.0323050777238932,-0.03661340208938482,-0.04054082637278758,-0.04404649241434098,-0.04709392973037502,-0.04965143492656961,-0.051692401517769576,-0.05319559672314153,-0.05414538235709091,-0.05453187751794529,-0.054351061381905326,-0.05360481503286736,-0.05230090189294968],[0.0,0.006930255358669589,0.013788413215212243,0.02050312611920099,0.02700453892059846,0.03322501549360297,0.03909984237532723,0.04456790199914784,0.04957230851887945,0.054060999609105855,0.05798727808499391,0.06131029770695849,0.06399548811620859,0.066014914480444,0.06734756810820161,0.06797958500850435,0.06790439012207147,0.06712276572361059,0.06564284328358191,0.063480018874099,0.06065679299902394,0.05720253651455157,0.05315318507548063,0.04855086528593775,0.04344345644381925,0.03788409243825336,0.031930608981991175,0.025644941929331845,0.01909248293905911,0.012341399185618662,0.005461924195783009,-0.0014743728115603279,-0.008395331481221114,-0.015228951027285786,-0.021904139278649697,-0.028351452271960443,-0.03450381669776678,-0.04029722768405192,-0.045671414657902094,-0.05057046835815372,-0.05494342247602116,-0.05874478387272422,-0.061935005858105005,-0.06448089960657598,-0.06635597943031671,-0.06754073831774055,-0.06802285087072209,-0.06779730152936998,-0.06686643675038405,-0.06523994059617026],[0.0,0.008143584248041682,0.016202448373626464,0.024092753622860792,0.031732414809839175,0.03904195427318104,0.04594532870571651,0.0523707202555665,0.058251283668558404,0.06352584169923249,0.06813952155587275,0.07204432575843522,0.07519963147056985,0.07757261311103635,0.07913858384796256,0.07988125242327888,0.07979289263550829,0.07887442371773193,0.07713540077453483,0.07459391537741862,0.07127640735281698,0.06721738972073868,0.06245908964558336,0.057051009134424985,0.05104941005294904,0.044516728816575764,0.0375209268459106,0.03013478354392732,0.022435139150251207,0.0145020953493541,0.006418181948971511,-0.0017325017019673794,-0.009865161623811423,-0.017895191343109566,-0.025739052078849554,-0.03331514182117604,-0.040544644261303546,-0.04735234874095498,-0.05366743269114852,-0.05942419842038975,-0.06456275658724359,-0.0690296492469172,-0.07277840599011615,-0.07577002738849647,-0.07797339071728546,-0.07936557373421893,-0.07993209314642677,-0.07966705528442455,-0.07857321741570539,-0.07666195906006834],[0.0,0.00925775805990823,0.01841920491671196,0.027389031321320118,0.03607392150904366,0.044383523991174706,0.052231391508323344,0.05953588036689867,0.06622099980267031,0.07221720253522868,0.07746210828897201,0.08190115275361662,0.08548815523290124,0.08818579907606844,0.08996601989405104,0.09081029752163605,0.09070984868823673,0.08966571839286169,0.08768876903268043,0.08479956739828344,0.08102817071125931,0.07641381393000307,0.07100450157680624,0.06485650833257244,0.05803379359461995,0.050607336088100086,0.0426543954532658,0.034257708490517624,0.025504628424928125,0.016486216144730847,0.0072962928678900795,-0.0019695359078590255,-0.011214875017361044,-0.020343542455536302,-0.029260569987308558,-0.03787319113032398,-0.04609180623218301,-0.053830914602201047,-0.06101000400045468,-0.06755438823049471,-0.07339598412200135,-0.0784740198201993,-0.08273566701348906,-0.08613659052204238,-0.08864140952982849,-0.0902240656617378,-0.09086809407658754,-0.09056679475574833,-0.08932330220542531,-0.08715055284746039],[0.0,0.010303147452196976,0.020499108205361338,0.030481810655483887,0.040147401787937964,0.04939532759119522,0.058129379150033536,0.06625869353541915,0.07369869907850912,0.08037199519480745,0.08620915760540404,0.09114946057834061,0.09514150867641244,0.09814377143914643,0.10012501543650006,0.1010646291994985,0.10095283764745994,0.0997908037810595,0.09759061658328823,0.09437516525417716,0.09017790108766018,0.0850424894678409,0.07902235560505017,0.07218012873753579,0.0645869905809205,0.05632193480370054,0.04747094523268132,0.03812610133772303,0.02838462030170333,0.01834784564132756,0.00812019290046448,-0.0021919366157284663,-0.012481262765142553,-0.022640742635857736,-0.03256468414530568,-0.042149845586355136,-0.05129650968141422,-0.05990952097084708,-0.06789927574344527,-0.07518265421041329,-0.08168388522518635,-0.08733533455314302,-0.09207820849060912,-0.09586316551319687,-0.09865082959031814,-0.10041219982571042,-0.1011289521623628,-0.10079363001311621,-0.09940972183375152,-0.09699162483154913],[0.0,0.011296788919914045,0.022476053993875984,0.033421494011362705,0.044019240313270064,0.05415904140134892,0.06373540991724388,0.07264872005877153,0.0808062440166914,0.08812311664960994,0.09452321836121899,0.09993996699503456,0.10431701050831994,0.10760881321910148,0.10978112952737468,0.11081136018223887,0.11068878738860774,0.10941468630761059,0.1070023117907129,0.10347676048556495,0.0988747097481322,0.0932440360772821,0.08664331704136533,0.07914122187839719,0.07081579710960603,0.06175365459832316,0.05204907050107783,0.041803004484777063,0.03112204941334456,0.020117322430539902,0.008903308975353106,-0.0024033282439760843,-0.013684962926715425,-0.024824228880937237,-0.03570524101882539,-0.04621480094361764,-0.05624357458610356,-0.06568722963937684,-0.07444752095877086,-0.0824333126352396,-0.08956152610925229,-0.09575800446169039,-0.1009582838902733,-0.1051082643456149,-0.10816477235008405,-0.11009601014429839,-0.11088188648864697,-0.11051422567841536,-0.1089968525980668,-0.10634555292983461],[0.0,0.012249008336796674,0.024370586606603063,0.03623863043521369,0.04772967304241743,0.05872416970353437,0.0691077414086457,0.07877236478136046,0.08761749587804198,0.09555111617627632,0.10249068987085522,0.10836402251823937,0.11311001209676844,0.11667928466911634,0.11903470803400648,0.12015177802351562,0.12001887342720105,0.11863737689099997,0.11602166053315371,0.11219893642679851,0.1072089735046973,0.10110368383123589,0.09394658254581925,0.08581212709603664,0.07678494263474883,0.06695894163952194,0.05643634691326702,0.045326628130100445,0.03374536298985359,0.021813034828979082,0.009653779196664556,-0.002605907564106806,-0.01483848429553255,-0.02691669187348706,-0.03871487712193657,-0.05011030002020067,-0.060984410603793955,-0.07122408227487059,-0.08072278869177701,-0.08938171199418138,-0.09711077083458611,-0.10382955752130521,-0.1094681745235311,-0.11396796163608072,-0.11728210623890944,-0.11937613030268279,-0.12022824907394247,-0.11982959770835823,-0.11818432349433301,-0.11530954270753138],[0.0,0.013166349305062861,0.02619572517294376,0.03895257914175635,0.0513041978757649,0.06312208382158094,0.0742832920063258,0.08467170907065076,0.09417926123045595,0.1027070386005224,0.11016632418338668,0.11647951681858024,0.12158093849051968,0.1254175175963841,0.12794934106574096,0.12915006958805714,0.12900721162835851,0.12752225338037398,0.12471064330522097,0.12060163141648228,0.11523796498363875,0.10867544381954522,0.10098233977842633,0.09223868650352626,0.08253544681337896,0.07197356638862566,0.06066292360415709,0.048721186431787564,0.03627258830544203,0.02344663568389309,0.010376760756656882,-0.002801066690653297,-0.0159497538103369,-0.028932510910165472,-0.04161427451712453,-0.05386311248278583,-0.06555159651258055,-0.0765581278401451,-0.08676820225535593,-0.09607560132559112,-0.10438349741759147,-0.11160546102404974,-0.11766635991541154,-0.12250314076274521,-0.12606548510022508,-0.128316332803057,-0.12923226763494883,-0.12880376085416279,-0.1270352703438417,-0.12394519423532857],[0.0,0.014053048273286524,0.027959898517014768,0.04157587364309276,0.05475932262355588,0.06737309412822692,0.07928596335068533,0.09037399717885172,0.10052184350791384,0.10962393128248793,0.1175855687836222,0.12432392873483636,0.12976890997884666,0.13386386676069828,0.13656619803035636,0.1378477906340656,0.1376953117838276,0.1361103477623492,0.1331093874204729,0.12872365063877073,0.12299876353786632,0.11599428381636841,0.10778308115446673,0.09845057912903428,0.08809386652681922,0.07682068730099817,0.0647483196788726,0.052002356081683354,0.03871539654939994,0.02502566926317308,0.011075592516565709,-0.0029897069041992714,-0.01702390351724228,-0.0308809954124184,-0.044416823152494664,-0.05749056950653621,-0.06996622441332812,-0.08171399993438525,-0.0926116804763736,-0.1025458942361788,-0.11141329264140228,-0.11912162551621447,-0.12559070078729692,-0.1307532187457678,-0.13455547218601518,-0.13695790513768763,-0.13793552437818668,-0.1374781594445751,-0.1355905684399194,-0.13229238853333236],[0.0,0.01491185044441656,0.02966856848555336,0.04411663560853685,0.058105744285546074,0.07149036166960838,0.08413124361597404,0.09589688328025603,0.10666487922313957,0.1163232087888615,0.12477139351012606,0.13192154441540382,0.13769927636397175,0.14204448189659602,0.14491195655125952,0.1462718691385884,0.14611007208456364,0.14442824861193,0.1412438952291305,0.13659013970893938,0.1305153964504156,0.12308286280954894,0.11436986163842028,0.10446703687263603,0.09347741053557959,0.08151531096974403,0.06870518344507333,0.05518029551790664,0.04108135060906028,0.026555024224405914,0.011752438046076844,-0.0031724122312171456,-0.018064258963078714,-0.03276817785078338,-0.047131199664711715,-0.06100390162861885,-0.07424196190909674,-0.08670766103841544,-0.098271314650996,-0.1088126226283318,-0.11822192061680489,-0.12640132089848466,-0.1332657307460924,-0.13874373766788384,-0.1427783523329822,-0.1453276014482917,-0.14636496441911545,-0.14587964925076863,-0.14387670482090018,-0.14037696835452115],[0.0,0.015744494340896222,0.03132519403708137,0.04658000844811758,0.061350237214851396,0.07548222126632338,0.08882894138066118,0.1012515476696528,0.11262080407531924,0.12281843285095907,0.13173834503963044,0.1392877441490382,0.14538809154096793,0.14997592349203542,0.1530035114256253,0.1544393584464308,0.15426852701199806,0.15249279433241086,0.14913063388144737,0.14421702321155236,0.13780308007198466,0.12995553061570897,0.12075601522743451,0.11030023919447703,0.09869697705626697,0.08606694099055158,0.07254152500880433,0.05826143802534993,0.04337524002079206,0.028037796528469636,0.012408667522378045,-0.0033495525325691776,-0.019072926198986206,-0.03459787855680345,-0.04976289892167955,-0.06441022108855779,-0.07838746462029318,-0.09154922010653878,-0.10375856190097243,-0.11488847259940965,-0.1248231644395354,-0.13345928387533962,-0.1407069867947127,-0.14649087319439352,-0.15075077158856787,-0.15344236499068342,-0.1545376519562059,-0.15402523788995237,-0.15191045358744476,-0.14821529977706271],[0.0,0.01655201633623934,0.03293183713692541,0.04896905826773738,0.06449683976034098,0.07935364149806659,0.09338490376565863,0.106444655179838,0.11839703127290047,0.12911768793107337,0.13849509498321402,0.1464316964822619,0.15284492560869242,0.1576680636376313,0.1608509340335394,0.162360424451595,0.16218083121523874,0.16031402268617878,0.15677941982726995,0.1516137941604762,0.14487088522182107,0.13662084149305387,0.12694949062618718,0.11595744655297498,0.10375906276833652,0.09048124267703003,0.07626211937989086,0.06124961863421108,0.04559991993820901,0.029475831749383903,0.013045097739840951,-0.0035213482908858524,-0.020051160690851166,-0.03637237491863697,-0.0523151965414839,-0.0677137676571396,-0.08240789236281185,-0.09624470332066215,-0.1090802520820644,-0.12078100662601322,-0.13122524053234677,-0.14030429933780147,-0.14792373090062194,-0.1540042680142083,-0.1584826530473779,-0.1613122960322662,-0.16246375935358148,-0.16192506399684864,-0.15970181416964996,-0.1558171389993911],[0.0,0.017334951170152595,0.03448956170144303,0.0512853670921673,0.06754763559692636,0.08310718601280193,0.09780214772159246,0.11147964467786764,0.12399738582381505,0.13522514538532007,0.14504611764932135,0.1533581321282905,0.1600747164701189,0.16512599605564626,0.16845942092504965,0.17004031247066256,0.1698522242088168,0.1678971128774906,0.16419531807978271,0.15878535068498847,0.15172349218860365,0.1430832091992324,0.13295438914366706,0.12144240514132454,0.10866701977642992,0.0947611391723292,0.07986943032966567,0.0641468161126617,0.04775686354060947,0.030870082151652548,0.013662150141484787,-0.0036879132690292842,-0.020999610223908255,-0.03809284200478919,-0.054789782651314435,-0.07091672893734388,-0.08630590745713954,-0.10079722002186989,-0.11423990920855626,-0.1264941267342609,-0.13743238833925636,-0.1469409000435929,-0.15492074197962508,-0.16128889748473668,-0.16597911674830562,-0.16894260602813485,-0.1701485352662253,-0.1695843588230142,-0.1672559459933845,-0.16318751994664943],[0.0,0.018093469292569275,0.035998706857259616,0.053529439196689906,0.070503288902422,0.08674367198123684,0.10208163491079393,0.1163576123132383,0.1294230869611566,0.1411421348463462,0.15139283923759014,0.16006855901657438,0.16707903809707325,0.17235134438578587,0.17583062851653578,0.17748069446449638,0.17728437610417372,0.17524371579370146,0.17137994312758376,0.16573325407892642,0.15836239282880593,0.1493440406331291,0.13877201808478298,0.12675630907017837,0.11342191657425624,0.09890756223735755,0.08336424319285152,0.06695366119916885,0.04984654040844538,0.03222085127252767,0.014259959063573757,-0.0038492837292631817,-0.021918481281702,-0.0397596543719759,-0.05718719598443357,-0.07401980223437776,-0.09008235852615522,-0.10520776132223726,-0.11923865657088664,-0.13202907670703112,-0.14344595919614012,-0.15337053082323945,-0.16169954332589676,-0.16834634751652133,-0.17324179471956383,-0.17633495614569875,-0.17759365271912608,-0.17700478984604184,-0.1745744936415814,-0.17032804719802658],[0.0,0.018827474215285498,0.03745908063176279,0.05570098912143238,0.07336342369941831,0.0902626368198296,0.1062228209559829,0.1210779375782497,0.13467344450213037,0.14686790363643898,0.1575344524057189,0.16656212353924496,0.1738569994964555,0.17934318951898426,0.18296361914473047,0.1846806239704168,0.18447634148554914,0.18235289690141115,0.17833238104186117,0.17245662052593885,0.1647867426331412,0.1554025393782021,0.1444016374110958,0.13189848237803925,0.11802314830948808,0.10291998442141581,0.08674611340760326,0.06966979684565025,0.05186868472187442,0.03352796728580962,0.01483844846110978,-0.004005439144265377,-0.022807656978141813,-0.04137259999133874,-0.05950713157501434,-0.07702259281809558,-0.0937367651818893,-0.10947576617358935,-0.12407585829742582,-0.13738515246440056,-0.14926518813953343,-0.159592373787877,-0.16825927263391377,-0.17517572035820017,-0.1802697631035252,-0.1834884060322303,-0.18479816464722426,-0.18418541314113882,-0.18165652614964423,-0.17723781243422357],[0.0,0.01953667363348547,0.03887010145625469,0.05779915208404643,0.07612690098850387,0.09366267916145433,0.11022405670115694,0.12563874068468195,0.13974636758229345,0.15240017156681535,0.16346851136206483,0.17283623974609558,0.18040590146190216,0.1860987470733656,0.18985555221899578,0.19163723374053093,0.19142525627662577,0.18922182509171326,0.18504986313398503,0.17895277256116154,0.17099398321497197,0.16125629274169587,0.14984100522369007,0.13686687728297134,0.12246888262085222,0.10679680784649101,0.09001369420236727,0.07229414139787549,0.053822491196759206,0.03479090965506376,0.015397387960655911,-0.0041563173278319305,-0.02366678321432439,-0.042931036535071,-0.06174866554867324,-0.07992390487887635,-0.09726767211984552,-0.11359953491651888,-0.12874958805591655,-0.1425602210414678,-0.15488775776175728,-0.1656039511957284,-0.1745973176044836,-0.1817742963296884,-0.18706022313285148,-0.19040010694954448,-0.19175920197775645,-0.1911233691487783,-0.1884992232201287,-0.18391406396026935],[0.0,0.020220632805723292,0.04023090437058771,0.05982264190411974,0.07879202674951802,0.0969417147695962,0.11408288937637773,0.13003722584606645,0.14463874648404285,0.1577355473400526,0.16919137851005844,0.17888706158440598,0.1867217294961696,0.1926138758711684,0.1965022029629532,0.19834625935144481,0.19812686077105557,0.1958462896902944,0.1915282715665705,0.18521772802322192,0.17698030951654387,0.16690171235461865,0.1550867871732038,0.1416584481434677,0.1267563942594087,0.11053565600778348,0.09316498253996826,0.07482508612448366,0.05570676214366274,0.03600890419269545,0.01593643493056304,-0.004301825791167738,-0.024495333343198776,-0.04443400867673135,-0.06391042383812295,-0.08272195990341627,-0.10067291488310505,-0.11757653966797933,-0.13325698083550178,-0.14755111010512298,-0.1603102214101713,-0.1714015779311726,-0.1807097929963552,-0.1881380304834224,-0.19360901223445565,-0.19706582200351352,-0.19847249757322022,-0.19781440488038052,-0.19509839025847936,-0.19035270921324415],[0.0,0.020878815600716513,0.04154042268972971,0.06176987243991125,0.08135671188511376,0.1000971733248017,0.11779629417988091,0.13426994524623792,0.14934674624546665,0.16286984874473842,0.17469856789757293,0.18470984603002552,0.19279953284617019,0.19888346893452097,0.20289836130335265,0.20480244183646745,0.20457590181929697,0.20222109801483534,0.19776252814553502,0.19124657603623335,0.18274102906946482,0.1723343729732134,0.16013487127763826,0.14626943901745657,0.13088232239719216,0.11413359815512417,0.09619750823749712,0.07726064710787096,0.05752002054956984,0.03718099615608263,0.016455166831072933,-0.004441850475360771,-0.025292657893771393,-0.045880338289884026,-0.06599071191788429,-0.08541456459584397,-0.10394982422291177,-0.12140366299531646,-0.13759450345120566,-0.15235390747484004,-0.16552832860827363,-0.17698070944069969,-0.1865919074573488,-0.19426193451441048,-0.19991099704535056,-0.2034803261770524,-0.2049327891198356,-0.2042532754708814,-0.20144885441223515,-0.1965487011680085],[0.0,0.021510616754203335,0.042797452181900744,0.06363905302010328,0.08381859791332055,0.10312615307043485,0.1213608562682001,0.1383330064805291,0.15386603739453694,0.16779835428226603,0.17998501511857304,0.19029923845600863,0.19863372236982063,0.2049017607517052,0.20903814533916953,0.21099984409642883,0.21076644888943835,0.20834038779776534,0.2037468998545572,0.1970337724773931,0.1882708443216046,0.17754927872802864,0.16498061532373265,0.15069560964218912,0.13484287283467683,0.11758732562439374,0.09910848258725491,0.07959858460851232,0.05926059894380387,0.0383061076905892,0.01695310663683151,-0.004576262613846855,-0.026058023647156557,-0.047268695331215416,-0.06798761675705078,-0.08799924283948804,-0.10709538669660564,-0.12507738547961933,-0.14175816712142528,-0.1569641965028022,-0.17053728078973088,-0.1823362151603686,-0.19223825180069432,-0.20014037688642766,-0.20596038226639815,-0.2096377206983261,-0.2111341357397435,-0.21043405974111254,-0.20754477580070133,-0.2024963419963523],[0.0,0.022115387713885427,0.04400070248958608,0.06542826490593667,0.08617515767534323,0.1060255447184471,0.12477291657087913,0.14222223876247153,0.15819198081870173,0.17251600477606382,0.18504529356458205,0.1956495012765911,0.20421830919387576,0.2106625734660196,0.21491525250032198,0.2169321044153727,0.2166921473024671,0.2141978775066261,0.2094752436563784,0.20257337671248093,0.19356407884396135,0.18254107644885556,0.1696190450906885,0.15493241649457173,0.13863398001409244,0.12089329311834854,0.1018949174353035,0.08183649870242689,0.060926710599462315,0.03938308385424087,0.017429743205996644,-0.004704924230775744,-0.026790644945215342,-0.04859765463373256,-0.06989908850621043,-0.09047334142770892,-0.11010637334241508,-0.12859393599583152,-0.14574369778995183,-0.1613772446659228,-0.17533193619842002,-0.18746259759223396,-0.19764302997872565,-0.20576732330009054,-0.2117509581231039,-0.21553168491985492,-0.21707017166804957,-0.21635041303370528,-0.21337989687938766,-0.20818952636575527],[0.0,0.022692457702307836,0.04514883903617265,0.06713552360563657,0.08842377740950175,0.10879213242899122,0.12802869062523947,0.145933328377443,0.16231977842824805,0.17701756767727841,0.18987379066336407,0.2007547002854081,0.20954709921315098,0.21615951751257337,0.22052316423475166,0.22259264306850374,0.2223464246116797,0.21978707034791845,0.2149412059987736,0.2078592445284344,0.1986148616827061,0.18730422951837272,0.1740450158967643,0.15897516035009288,0.14225143905558396,0.12404783384640233,0.10455372222703566,0.08397190722294263,0.0625165075605245,0.04041073012650298,0.0178845478804711,-0.004827692622020242,-0.0274897092061895,-0.0498657421915752,-0.07172300706992116,-0.09283411623146444,-0.11297944454568623,-0.1319494141866893,-0.14954667493348528,-0.1655881572628805,-0.1799069768760281,-0.19235417084528905,-0.20280024731975302,-0.21113653266737376,-0.217276302038997,-0.22115568159269064,-0.22273431299226804,-0.22199577326703074,-0.2189477456648037,-0.21362193972082935],[0.0,0.023241151135450208,0.046240517681920675,0.06875883040722822,0.09056182462477987,0.1114226773185944,0.13112436685041715,0.14946193070019487,0.16624459775196748,0.18129777294237315,0.19446485362488014,0.20560885875357018,0.21461385393758942,0.2213861575410049,0.22585531528065084,0.2279748331829341,0.22772266127443241,0.22510142297430838,0.22013838780210174,0.2128851876848289,0.2034172798147314,0.1918331616457231,0.17825334619516123,0.1628191083111804,0.14569101494854297,0.12704725474298217,0.10708178426197035,0.08600231021701685,0.06402812862913915,0.04138784342730889,0.018316988213971718,-0.004944424060883428,-0.028154398025633482,-0.05107147343665337,-0.07345723716085076,-0.09507880345820759,-0.11571123676121103,-0.13513989174628196,-0.15316264635274388,-0.16959200451857231,-0.18425704675754356,-0.19700520828534657,-0.20770386619628747,-0.21624171917871643,-0.22252994541558596,-0.22650312662415995,-0.22811992862181285,-0.22736353133779244,-0.22424180379740455,-0.2187872222582089],[0.0,0.02376080220422567,0.04727441373526587,0.07029621551782916,0.09258670491931661,0.11391398736674921,0.13405618881479447,0.1528037639672714,0.16996167623902173,0.185351426779356,0.1988129114485434,0.21020608642866692,0.21941242514100026,0.22633615131316365,0.23090523536809393,0.2330721437691004,0.2328143335253457,0.23013448671326647,0.22506048257413658,0.21764510747805066,0.20796550577165865,0.1961223772226454,0.18223892941017808,0.16645959595991472,0.14894853395815494,0.12988791617697965,0.10947603587686734,0.0879252439030882,0.06545973953687402,0.0423132380837942,0.018726539464108353,-0.0050549769002304476,-0.028783904840484587,-0.05221338528089134,-0.07509967438684961,-0.09720467930433069,-0.11829843511046051,-0.1381614972068239,-0.15658722426671307,-0.17338392798614213,-0.1883768672741324,-0.201410066136112,-0.21234793633845286,-0.22107668797308325,-0.2275055132476531,-0.23156753118593298,-0.23322048341044782,-0.2324471737689125,-0.22925564723090466,-0.22367910619365974],[0.0,0.0242507671998128,0.04824924647525914,0.07174577452808259,0.09449591000748286,0.11626297650599365,0.1368205247743517,0.15595468854020547,0.17346640942238295,0.18917350779427491,0.20291257805252808,0.21454068857400577,0.22393686867585919,0.23100336710942318,0.2356666689954507,0.23787826062120734,0.23761513414301505,0.23488002694367408,0.22970139315465823,0.22213310763934474,0.2122539055168307,0.2001665630571346,0.18599682846917448,0.16989211370485136,0.1520199608887885,0.1325662993279137,0.11173351123372906,0.08973832628013487,0.0668095669003464,0.0431857677792485,0.01911269430631575,-0.005159214194633494,-0.029377449860005865,-0.05329006320063728,-0.0766482842070603,-0.09920911037770264,-0.12073783474598933,-0.14101048760653412,-0.15981616653841785,-0.17695923048563017,-0.19226133506900644,-0.20556328795570333,-0.2167267049842565,-0.22563544981606026,-0.23219684213614528,-0.23634262183398605,-0.2380296591336776,-0.23724040328578758,-0.23398306515300818,-0.2282915317901162],[0.0,0.024710435006307513,0.04916380002771237,0.07310569945461615,0.09628705860570935,0.11846671492576077,0.13941392687121268,0.15891077438289977,0.17675442597306412,0.19275924884148501,0.2067587400685616,0.21860725879457346,0.22818154136474306,0.23538198367759924,0.24013367739598862,0.24238718924072633,0.24211907525966184,0.23933212472206727,0.23405533110104187,0.22634359044581115,0.21627713028184395,0.20396067498010784,0.18952235627837546,0.17311238028874987,0.15490146485892684,0.13507906354378782,0.11385139466387297,0.09143929595502477,0.06807592712791813,0.04400434423843081,0.019474971103400174,-0.005257005932624442,-0.029934292776615176,-0.054300164293834476,-0.0781011350955598,-0.10108959662277128,-0.12302639309143573,-0.14368330950106556,-0.16284544582306146,-0.18031345266945245,-0.19590560518371014,-0.20945969357833316,-0.22083471065138066,-0.2299123187262198,-0.23659808075347633,-0.24082244276767084,-0.2425414574777525,-0.24173724146380365,-0.23841816122356146,-0.23261874613326033],[0.0,0.025139236073062635,0.05001694121653552,0.07437430528466393,0.09795793139424187,0.1205224720875499,0.141833181754732,0.16166835876200425,0.17982165187951185,0.19610420701438025,0.2103466319158607,0.2224007584131596,0.23214118385286822,0.23946657569996257,0.2443007257302226,0.24659334284699302,0.24632057627437912,0.24348526368394297,0.23811690167356442,0.2302713389055843,0.2200301950966041,0.20750001190336478,0.19281114453828943,0.1761164056455429,0.1575894755457676,0.13742309538819178,0.11582706200728338,0.09302604534405186,0.0692572511380112,0.044767953205512225,0.019812920976886073,-0.005348230945509563,-0.030453743635003395,-0.05524243699636809,-0.07945642689982035,-0.10284380802611667,-0.1251612745123082,-0.14617665113480194,-0.16567130869707877,-0.18344243849407366,-0.19930516219129174,-0.21309445516895933,-0.2246668633220924,-0.23390199545610263,-0.24070377575210372,-0.24500144327215373,-0.2467502881062807,-0.2459321165029384,-0.24255544014399982,-0.23665538759520624],[0.0,0.02553665009968939,0.050807634853790334,0.07555005271210535,0.0995065009626357,0.12242775356834426,0.14407535393954357,0.1642240956685283,0.18266436541863512,0.1992043235939584,0.21367190010356954,0.22591658445771173,0.23581099157306878,0.24325218699757148,0.2481627577680346,0.2504916177601748,0.25021453915271485,0.24733440447659416,0.24188117662714503,0.23391158715120466,0.22350854605200604,0.21078027925182163,0.1958592026855926,0.1789005447386804,0.16008073135993095,0.13959555065019771,0.11765811602065435,0.09449664911076525,0.07035210553087598,0.04547566812952714,0.020126133863776423,-0.005432778542310338,-0.030935172141767532,-0.0561157379695399,-0.08071251513034014,-0.10446961605554535,-0.12713988857747355,-0.14848748712669368,-0.16829032630344035,-0.18634239129790126,-0.20245588112339213,-0.21646316236572555,-0.22821851312334107,-0.23759963899547204,-0.24450894534707046,-0.2488745526147437,-0.2506510440911734,-0.2498199384085929,-0.24638988180593807,-0.24039655817945896],[0.0,0.025902212609389366,0.05153495681796986,0.07663156758463512,0.10093095742350118,0.12418033257446527,0.14613782289167493,0.16657499809039789,0.1852792441750309,0.2020559753255604,0.21673065823156584,0.22915062781401763,0.23918667544537667,0.24673439314527293,0.251715259767694,0.25407745787975766,0.25379641283383647,0.2508750484249056,0.24534375647343726,0.23726008065027715,0.22670811783276823,0.21379764321999123,0.19866296830880764,0.18146154361151848,0.16237232065148235,0.14159389027733843,0.11934241666350194,0.0958493884901402,0.07135921069776932,0.04612666187016329,0.020414243697175802,-0.005510549908202816,-0.03137801562836418,-0.05691904654469482,-0.08186793173667109,-0.1059651205513806,-0.1289599227859511,-0.15061311669208982,-0.1706994376708887,-0.18900992176726317,-0.2053540795839603,-0.21956187799926,-0.23148550907197488,-0.24100092773130544,-0.2480091422545049,-0.2524372441071337,-0.25423916643216915,-0.25339616329718345,-0.24991700471383874,-0.24383788540085471],[0.0,0.02623552054458521,0.052198105187046956,0.07761765745840896,0.102229730216524,0.1257782767681333,0.14801831459856427,0.16871847399749612,0.18766340506668538,0.20465601806883296,0.21951953381058165,0.2320993227289005,0.2422645135482945,0.24990935477350235,0.2549543148536693,0.25734690957266115,0.2570622480579321,0.2541032917276554,0.24850082348221136,0.24031312746054379,0.229625382692975,0.21654877695848745,0.20121935006527908,0.1837965785882865,0.16446171678707294,0.1434159109640028,0.12087810687959473,0.0970827719948317,0.0722774562366179,0.04672021666246115,0.020676932816360318,-0.0055814592949565535,-0.03178178582967285,-0.05765147701938914,-0.0829214027932716,-0.10732867261800819,-0.13061937042600405,-0.15255119617954854,-0.17289598658997798,-0.1914420887680443,-0.20799656211170242,-0.2223871855244221,-0.23446424908233876,-0.24410211151099875,-0.251200507268804,-0.25568558963918214,-0.257510698979883,-0.25665684813707695,-0.2531329199743484,-0.2469755749616307],[0.0,0.026536236988294458,0.052796409632103976,0.07850732556620463,0.10340150650584236,0.12721997090266796,0.14971492820660276,0.1706523567044299,0.18981443811709192,0.20700182362785619,0.22203570776718554,0.2347596885790748,0.2450413947169905,0.2527738625422522,0.2578766488984935,0.26029666798567763,0.2600087436274222,0.2570158711862294,0.25134918640184145,0.24306764147690463,0.23225739177971905,0.21903089954438695,0.20352576389283342,0.18590328935032646,0.166346807746655,0.14505977096072464,0.12226363435031384,0.09819555288629939,0.07310591395915059,0.047255732524636815,0.020913935687821704,-0.0056454350253803106,-0.03214607460348187,-0.05831228903241531,-0.08387186342213539,-0.10855889393886486,-0.13211655408154835,-0.1542997665241219,-0.17487775272763034,-0.1936364337978204,-0.21038065761137761,-0.22493622904135038,-0.23715172216072422,-0.24690005557125705,-0.2540798144689589,-0.25861630569237926,-0.26046233477772107,-0.2595986969368461,-0.25603437685038266,-0.24980645519742753],[0.0,0.026804095092269723,0.05332933923274761,0.07929978243880523,0.10444524648649817,0.1285041356547693,0.15122615818332866,0.1723749301320761,0.1917304345533463,0.20909131039347545,0.22427694731156994,0.23712936462245474,0.24751485481660065,0.2553253745590064,0.2604796686960598,0.2629241155770025,0.2626332848958678,0.2596102022504968,0.2538863176663467,0.24552117841254706,0.23460180951399678,0.22124180840421642,0.20558016313755295,0.18777980645522374,0.16802592074768113,0.14652401154727673,0.12349776959325148,0.09918674371062494,0.07384384871271472,0.047732734253314285,0.021125042001142305,-0.0057024203290117524,-0.03247055868904888,-0.058900896195728254,-0.0847184702082835,-0.10965469284633401,-0.13345014518927167,-0.15585727608824013,-0.1766429775141643,-0.19559100964972032,-0.2125042504962961,-0.227206746592612,-0.23954554351076945,-0.24939227708653355,-0.2566445088298082,-0.26122679162263396,-0.2630914546179345,-0.2622190991734792,-0.2586188006617121,-0.2523280140559605],[0.0,0.027038901276446738,0.053796508842720164,0.07999445537055039,0.1053601958514553,0.12962984296330532,0.15255091236840904,0.17388494938296908,0.19341000969188754,0.21092296830130852,0.22624163270817887,0.2392066383028998,0.24968310628662976,0.25756204685543693,0.2627614930535571,0.26522735349994797,0.2649339751169698,0.26188441000696167,0.2561103837003155,0.24767196510537298,0.23665694159375467,0.22317990572229374,0.2073810630926773,0.18942477375098465,0.1694978423013758,0.14780757452238524,0.12457962070341735,0.10005562813785385,0.07449072719458438,0.048150877121102734,0.021310099190567734,-0.005752374022782032,-0.032755003582921466,-0.059416873125101036,-0.08546061131211276,-0.11061527741058502,-0.13461917996779693,-0.1572225992654832,-0.1781903852281345,-0.19730440375898184,-0.21436580605370872,-0.22919709728354323,-0.24164398312660987,-0.251576974937527,-0.2588927368561926,-0.26351516084138826,-0.26539615844530723,-0.26451616109194015,-0.2608843236547762,-0.25453842921593844],[0.0,0.027240537752155347,0.05419768411007181,0.08059099588424344,0.10614589562358184,0.13059652812608408,0.15368852620935655,0.17518165696786248,0.19485232098709585,0.2124958785146137,0.22792877838808034,0.2409904675724536,0.25154506144083083,0.2594827574225288,0.2647209773117952,0.26720522635337773,0.2669096601676265,0.2638373536172418,0.2580202688185653,0.24951892263020317,0.2384217570781956,0.22484421926738674,0.2089275603509493,0.19083736605274065,0.1707618340299019,0.14890981599680836,0.1255086449787356,0.10080177029896681,0.07504622490327165,0.048509951369930845,0.02146901442361864,-0.005795271047814699,-0.03299926659556231,-0.059859960984777014,-0.08609791444439235,-0.11144016576194822,-0.13562307198004875,-0.15839505115224883,-0.17951919962466684,-0.1987757566149651,-0.21596439044895402,-0.23090628267041968,-0.24344598834256453,-0.25345305318779315,-0.26082337074225626,-0.26548026540627423,-0.26737529012334366,-0.26648873038923665,-0.2628298093478356,-0.2564365918400321],[0.0,0.027408964412741627,0.0545327852387361,0.0810892853245476,0.10680218952267731,0.13140399886387077,0.15463877342823978,0.1762647949641648,0.19605708155496482,0.21380972817250943,0.22933804876830172,0.2424804976172218,0.25310034992562214,0.2610871242199111,0.26635773171809307,0.2688573407270134,0.26855994707272607,0.2654686446296286,0.259615593133969,0.25106168361657044,0.2398959049353764,0.22623441799796573,0.2102193473051758,0.19201730238776407,0.17181764451803988,0.14983051672838166,0.12628465763093827,0.10142502178199954,0.07551023134708955,0.04880988557786673,0.021601756091141802,-0.0058311028716449215,-0.033203299141654284,-0.06023007164203426,-0.08663025284186633,-0.1121291938253895,-0.13646162154613595,-0.159374398541118,-0.18062915639494065,-0.20000477555713048,-0.21729968571440778,-0.23233396278644428,-0.244951200729439,-0.25502013867458534,-0.262436026473823,-0.2671217144466993,-0.2690284559913779,-0.26813641470999805,-0.2644548707726044,-0.2580221243727856],[0.0,0.02754422012890548,0.054801889565729435,0.08148943869023581,0.1073292290129049,0.1320524415304921,0.15540187332985494,0.177134613346149,0.19702456943868743,0.21486482049454286,0.23046976909026168,0.2436770723169116,0.25434933068151666,0.262375517514738,0.26767213401424705,0.27018407790753374,0.26988521669719107,0.26677865952503765,0.26089672482679266,0.25230060411380295,0.24107972537960037,0.2273508227539587,0.21125672208311677,0.1929648550701352,0.1726655174332264,0.15056988920295342,0.12690783775373193,0.10192552642535199,0.07588285361273785,0.04905074896585527,0.021708354828201205,-0.005859877763795273,-0.03336714830927623,-0.0605272905136366,-0.08705774936136765,-0.11268252061969475,-0.13713502219247786,-0.16016086745283079,-0.18152051170265573,-0.2009917442271907,-0.21837200001898693,-0.2334804671217653,-0.24615996767082893,-0.2562785930610123,-0.2637310762310263,-0.26843988678792097,-0.27035603757876014,-0.2694595943185042,-0.2657598829722897,-0.25929539273464813],[0.0,0.027646423481572,0.05500523301926944,0.08179180680238703,0.10772747615850915,0.1325424246263668,0.1559784949365165,0.17779187469798757,0.19775563285090506,0.2156620804976165,0.2313249315518986,0.24458124072836776,0.255293098710616,0.2633490668627363,0.268665336558506,0.2711866010673847,0.2708866309268369,0.2677685468152109,0.26186478708639643,0.25323677030400704,0.2419742562858453,0.22819441230589382,0.21204059416841048,0.19368085483497446,0.17330619611968,0.15112858164061002,0.12737873169944502,0.1023037230297292,0.07616441838432438,0.049232752702815435,0.02178890409167402,-0.005881620951689976,-0.033490957747705385,-0.06075187817628827,-0.08738077879617079,-0.11310063125562309,-0.1376438643005672,-0.16075514739769947,-0.18219404701376388,-0.2017375279169149,-0.21918227347839095,-0.23434680083571016,-0.2470733489127193,-0.2572295196548645,-0.26470965540542357,-0.2694359380934489,-0.27135919879824183,-0.27045942926821875,-0.26674599007268734,-0.2602575132210675],[0.0,0.027715772962033015,0.055143210516976,0.08199697689650405,0.10799770440369061,0.13287489975802383,0.1563697581172367,0.1782378555008467,0.19825169160532555,0.21620305655723665,0.2319051969823076,0.2451947588561765,0.25593348692476114,0.2640096630146761,0.2693392682705278,0.27186685722798887,0.27156613462940926,0.26844022898118125,0.2625216600069599,0.25387200033533147,0.2425812349414949,0.22876682500687373,0.21257248593560507,0.1941666922405616,0.17374092485302176,0.1515076790897456,0.12769825400116436,0.10256034609875178,0.07635547249474443,0.049356250262052405,0.021843560317987697,-0.005896374663233901,-0.03357496790986952,-0.060904270806436775,-0.08759996850857002,-0.1133843377546796,-0.13798913610341915,-0.16115839253936834,-0.1826510704154316,-0.20224357502857304,-0.21973207974183484,-0.23493464645329934,-0.2476931183521291,-0.2578747652707826,-0.26537366451631694,-0.27011180281558056,-0.27203988791043454,-0.2711378613597345,-0.2674151072132437,-0.2609103543865797],[0.0,0.0277525466670746,0.055216375359297916,0.08210577172054971,0.10814099738463506,0.1330512001764832,0.1565772318676641,0.1784743441722762,0.19851473493596172,0.21648991802928785,0.23221289229079203,0.24552008695553246,0.2562730633302732,0.2643599550122736,0.26969663166865726,0.2722275742692195,0.27192645266736637,0.2687963995204396,0.2628699776997534,0.254208841529385,0.242903095377954,0.2290703562761487,0.21285453031186913,0.1944243155325093,0.1739714469291316,0.15170870176048307,0.12786768596805906,0.10269642471079574,0.0764567820857733,0.049421736878340596,0.02187254268284153,-0.005904198061952654,-0.03361951568302344,-0.060985079510328125,-0.08771619746628298,-0.1135347778018916,-0.13817222216769603,-0.16137221992207698,-0.1828934146068849,-0.20251191485026387,-0.22002362357500407,-0.23524636128097376,-0.24802176131249642,-0.25821691739364516,-0.2657257662916524,-0.27047019122418087,-0.272400834531381,-0.2714976111582664,-0.2677699176054992,-0.26125653417480543],[0.0,0.027757101515582503,0.05522543767087366,0.08211924721727792,0.10815874587692623,0.13307303702150752,0.1566029298902456,0.1785036360282635,0.1985473158934885,0.21652544913898897,0.23225100391199408,0.24556038260173554,0.2563151237939175,0.26440334272587995,0.26974089525697154,0.2722722532450031,0.27197108222198163,0.26884051535939213,0.262913120872885,0.25425056320546857,0.24294296151401787,0.22910795213295443,0.21288946476856685,0.1944562251555838,0.17399999975330915,0.15173360074226444,0.12788867207595137,0.10271327962009372,0.07646933044985608,0.04942984815285457,0.02187613248379199,-0.005905167080329796,-0.033625033439742574,-0.06099508860252945,-0.08773059376641143,-0.11355341154096682,-0.13819489949340327,-0.16139870491547048,-0.18292343173672243,-0.20254515183944022,-0.22005973464926212,-0.2352849707660969,-0.24806246754255867,-0.258259296889661,-0.26576937816715435,-0.270514581771892,-0.27244554194326653,-0.2715421703298596,-0.2678138649745208,-0.26129941254374006],[0.0,0.02772987201248748,0.05517126194128104,0.08203868886737209,0.10805264297911622,0.13294249339571118,0.1564493036205235,0.1783285253342677,0.19835254250370948,0.21631303933877358,0.23202316746349838,0.24531948975511336,0.2560636806289051,0.2641439650803053,0.26947628151341724,0.2720051562587323,0.271704280682168,0.26857678488159215,0.26265520512348617,0.25400114535851404,0.242704636337351,0.2288831989940794,0.21268062184105296,0.19426546509435233,0.17382930709185967,0.1515847512469844,0.12776321427229143,0.10261251868279825,0.07639431462484317,0.04938135785200276,0.0218546721660841,-0.005899374156843162,-0.033592047540562726,-0.06093525288974973,-0.08764453072870018,-0.11344201651763305,-0.13805933135991644,-0.161240374027335,-0.1827439852571312,-0.20234645660334727,-0.21984385774214493,-0.23505415801945284,-0.24781912016985413,-0.25800594650579517,-0.2655086604513185,-0.27024920904781735,-0.27217827496211555,-0.2712757895493132,-0.267551141632852,-0.2610430798298004],[0.0,0.027671368585716106,0.055054863716240104,0.08186560676932236,0.1078246776324628,0.13266201639157366,0.15611923284437013,0.17795229461028741,0.19793406587172016,0.2158566703353051,0.23153365383066052,0.24480192404842652,0.25552344723793474,0.2635866842132599,0.26890775072845574,0.2714312901502464,0.2711310493495027,0.26801015182033117,0.26210106518543874,0.253465263425823,0.24219258734871377,0.2284003099470071,0.21223191637353533,0.19385561122246156,0.1734625686469946,0.15126494351796782,0.1274936643137886,0.1023960307029806,0.07623314081237871,0.04927717494586945,0.02180856401195684,-0.005886927882160525,-0.0335211763193575,-0.0608066940163577,-0.087459621639214,-0.11320268087615777,-0.13776805904612133,-0.16090019523349675,-0.18235843896414486,-0.20191955376365836,-0.2193800395526213,-0.23455824971829642,-0.2472962808382166,-0.2574616153963265,-0.2649485004020084,-0.2696790475698116,-0.2716040436143894,-0.27070346223090375,-0.2669866724346146,-0.2604923410922354],[0.0,0.027582175521924965,0.05487740549055661,0.0816017295323054,0.10747712657727951,0.13223440719494062,0.15561601405159603,0.17737870135573663,0.1972960654161572,0.21516089998671095,0.23078735189434885,0.24401285452480648,0.2546998190513119,0.26273706581196177,0.26804098094472206,0.270556386247236,0.27025711321202983,0.26714627526525814,0.2612562353768084,0.2526482693787252,0.24141192849455478,0.22766410771140336,0.21154782968671373,0.19323075684113683,0.17290344711663802,0.15077737154570456,0.1270827142554682,0.10206597779395445,0.0759874186909587,0.049118339932170026,0.021738268513738668,-0.0058679525599786495,-0.03341312758268063,-0.060610695928242565,-0.08717771322590759,-0.11283779491450253,-0.13732399155300767,-0.16038156597477346,-0.18177064339382465,-0.2012687068934093,-0.2186729123354868,-0.2338021986084622,-0.24649917125963575,-0.2566317399163812,-0.26409449246147154,-0.2688097916666018,-0.2707285828755037,-0.2698309043340852,-0.2661260948584792,-0.25965269668056956],[0.0,0.027462948527115644,0.054640191854729694,0.08124899705927599,0.1070125448475909,0.13166280939012953,0.15494334667317153,0.17661196236196092,0.1964432314202103,0.21423084327361758,0.2297897481199659,0.24295808205710495,0.25359885100113655,0.26160135587650435,0.26688234425131185,0.2693868764370657,0.26908889704255107,0.26599150603579463,0.26012692649417823,0.2515561693782065,0.2403683988163882,0.22668000450435935,0.21063339086835686,0.19239549558973237,0.172156052902737,0.15012561973300387,0.12653338521139595,0.10162478635149642,0.0756589546955556,0.048906020492194,0.021644302451301313,-0.005842587688057875,-0.03326869565468797,-0.060348699512365885,-0.08680087794857379,-0.11235004110487745,-0.13673039345864885,-0.15968829897272768,-0.18098491974634892,-0.2003987007166672,-0.21772767456179348,-0.23279156282675087,-0.2454336514137152,-0.25552242092525973,-0.26295291489965644,-0.26764783170407985,-0.26955832872642194,-0.26866453049947364,-0.26497573547130554,-0.25853031927075454],[0.0,0.027314411938676567,0.05434466394803954,0.08080955229880661,0.10643375490753221,0.13095069559390682,0.1541053173518318,0.1756567357831078,0.19538074508927988,0.21307215055106193,0.22854690322986554,0.2416440146835004,0.25222723177669004,0.26018645416285313,0.2654388796906667,0.2679298658194071,0.2676334980815083,0.26455285967845543,0.25871999940532475,0.25019559823766246,0.23906833804929697,0.22545397902850095,0.2094941553904165,0.1913549019143036,0.17122492663448285,0.14931364765468075,0.12584901450937935,0.10107513673719529,0.07524974433694284,0.048641506526008994,0.02152723669479358,-0.005810987365099713,-0.03308875799979683,-0.060022296470338146,-0.08633140518707234,-0.11174238268829369,-0.13599087103773225,-0.1588246060185779,-0.18000604151295385,-0.1993148207646407,-0.21655006881577354,-0.23153248226861103,-0.2441061946319664,-0.2541403978465485,-0.2615307031200021,-0.2662002269144781,-0.26810039078888304,-0.26721142677481075,-0.2635425830285814,-0.25713202761992804],[0.0,0.02713735561590918,0.053992393271948415,0.08028573204572453,0.10574383453492595,0.1301018525480546,0.15310638239872085,0.1745181011393562,0.1941142563088249,0.2116909832923486,0.22706542618556372,0.2400776400987331,0.2505922551108757,0.2584998845632078,0.2637182630410007,0.2661931022050907,0.2658986555695747,0.2628379863501398,0.2570429355965219,0.24857379094062085,0.2375186594063831,0.22399255080629793,0.20813618126020417,0.19011450928375653,0.17011501967605808,0.14834577305968394,0.12503324136431015,0.10041995177205393,0.07476196363526064,0.0483262046151095,0.021387693753979753,-0.0057733196292237875,-0.03287427145586036,-0.05963322248547802,-0.08577179141335897,-0.11101805095380592,-0.13510935678035296,-0.1577950798925986,-0.17883921398403796,-0.19802283068567222,-0.21514635714277008,-0.23003165223047142,-0.2425238598087489,-0.2524930197367663,-0.25983541988677133,-0.26447467509212375,-0.26636252180484077,-0.26547932019560294,-0.26183425847272357,-0.2554652572944926],[0.0,0.02693263153661982,0.05358507491869115,0.0796800568721458,0.10494610355950995,0.1291203648027591,0.1519513485916859,0.173201537429879,0.19264985929969014,0.2100939875400071,0.22535244571052307,0.23826649554497403,0.248701788352407,0.25654976268646223,0.261728773742301,0.2641849427317274,0.26389271740020315,0.26085513785453107,0.25510380493572227,0.24669855146607336,0.2357268197905692,0.2223027520882308,0.20656600291717064,0.18868028634680062,0.16883167279180766,0.1472266532664382,0.12408999119722804,0.09966238414240752,0.07419795974380848,0.047961631961627456,0.021226345095920404,-0.005729765733911926,-0.032626268111269036,-0.059183349743963,-0.08512472943449007,-0.1101805313152788,-0.13409009244738973,-0.15660467457438604,-0.17749005182019575,-0.19652894741037508,-0.2135232940668419,-0.22829629456051898,-0.2406942609853491,-0.2505882136191689,-0.2578752227380152,-0.26247947942457506,-0.2643530842308378,-0.263476545490261,-0.2598589820953592,-0.2535380286311598],[0.0,0.026701150127852284,0.05312452027091278,0.07899522027196573,0.10404410956521393,0.1280105971254164,0.15064535247361982,0.17171289953608626,0.1909940663717349,0.20828826528285507,0.2234155795884231,0.23621863535070145,0.24656423858299273,0.2543447609061909,0.2594792592387335,0.2619143178714882,0.2616246041672191,0.25861313210352554,0.25291123091753964,0.24457821917857928,0.2337007876794768,0.2203920975665471,0.20479060309061708,0.18705861122638645,0.1673805931448267,0.14596126510487267,0.12302345872945825,0.09880580282204722,0.07356024084040981,0.04754940985410068,0.021043908253122152,-0.005680519367392172,-0.032345850859989485,-0.0586746788717652,-0.08439309679533787,-0.1092335483005319,-0.13293761080224162,-0.1552586839072368,-0.17596455487119925,-0.19483981437677983,-0.21168809750062179,-0.22633412655590268,-0.2386255345581007,-0.2484344503439303,-0.2556588288529813,-0.26022351273275157,-0.26208101422287755,-0.26121200918439125,-0.2576255381344695,-0.25135891219525247],[0.0,0.026443876359266837,0.05261264922906823,0.07823407710313995,0.10304161266756014,0.12677717577151842,0.14919383831167582,0.1700583930984665,0.18915377997966448,0.20628134398153075,0.22126290197542922,0.23394259636976658,0.24418851654274704,0.25189407114769835,0.25697909701446753,0.2593906931106583,0.2591037708867618,0.2561213152797791,0.2504743536600365,0.2422216330442481,0.2314490089328763,0.2182685521298937,0.20281738283696774,0.18525624415133027,0.165767829807658,0.14455488356096088,0.1218380899831611,0.09785377861603285,0.07285146536487985,0.04709125671056183,0.020841143744622582,-0.005625785821527151,-0.0320341886690724,-0.058109330350012436,-0.08357994343111314,-0.10818104956947822,-0.13165671616084706,-0.15376271888239068,-0.17426908243078357,-0.192962473023498,-0.20964841777342308,-0.2241533278479926,-0.23632630436529936,-0.2460407082399238,-0.25319547764690203,-0.2577161793978679,-0.2595557832915896,-0.2586951513831033,-0.2551432370814309,-0.2489369920046104]],"type":"surface","uid":"01cb1894-3263-455b-83fc-e6ab3a014810"}],                        {"autosize":true,"height":700,"scene":{"xaxis":{"title":{"text":"X1"}},"yaxis":{"title":{"text":"X2"}},"zaxis":{"title":{"text":"Y"}}},"width":860,"template":{"data":{"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"contour"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"heatmapgl"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"heatmap"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"histogram2dcontour"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"histogram2d"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}}},                        {"responsive": true}                    ).then(function(){
                            
var gd = document.getElementById('b8cd95cb-bcfb-40ff-924d-f8aa93fd952a');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })                };                });            </script>        </div></div></div>
</div>
<p>We now fit a regression tree to this data and plot the predicted
regression surface.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">tree</span>
<span class="n">fitted_tree</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">DecisionTreeRegressor</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">Xsim</span><span class="p">,</span><span class="n">ysim</span><span class="p">)</span>
<span class="n">fig</span><span class="o">=</span><span class="n">surface_scatter_plot</span><span class="p">(</span>
    <span class="n">Xsim</span><span class="p">,</span> <span class="n">ysim</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">fitted_tree</span><span class="o">.</span><span class="n">predict</span><span class="p">([</span><span class="n">x</span><span class="p">]),</span> <span class="n">show_f0</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>
<span class="n">fig</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">AttributeError</span><span class="g g-Whitespace">                            </span>Traceback (most recent call last)
<span class="nn">File ~\anaconda3\Lib\site-packages\IPython\core\formatters.py:922,</span> in <span class="ni">IPythonDisplayFormatter.__call__</span><span class="nt">(self, obj)</span>
<span class="g g-Whitespace">    </span><span class="mi">920</span> <span class="n">method</span> <span class="o">=</span> <span class="n">get_real_method</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">print_method</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">921</span> <span class="k">if</span> <span class="n">method</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<span class="ne">--&gt; </span><span class="mi">922</span>     <span class="n">method</span><span class="p">()</span>
<span class="g g-Whitespace">    </span><span class="mi">923</span>     <span class="k">return</span> <span class="kc">True</span>

<span class="nn">File ~\anaconda3\Lib\site-packages\plotly\basewidget.py:741,</span> in <span class="ni">BaseFigureWidget._ipython_display_</span><span class="nt">(self)</span>
<span class="g g-Whitespace">    </span><span class="mi">737</span><span class="w"> </span><span class="sd">&quot;&quot;&quot;</span>
<span class="g g-Whitespace">    </span><span class="mi">738</span><span class="sd"> Handle rich display of figures in ipython contexts</span>
<span class="g g-Whitespace">    </span><span class="mi">739</span><span class="sd"> &quot;&quot;&quot;</span>
<span class="g g-Whitespace">    </span><span class="mi">740</span> <span class="c1"># Override BaseFigure&#39;s display to make sure we display the widget version</span>
<span class="ne">--&gt; </span><span class="mi">741</span> <span class="n">widgets</span><span class="o">.</span><span class="n">DOMWidget</span><span class="o">.</span><span class="n">_ipython_display_</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

<span class="ne">AttributeError</span>: type object &#39;DOMWidget&#39; has no attribute &#39;_ipython_display_&#39;
</pre></div>
</div>
<div class="output text_html"><div>                            <div id="ac00dae5-4c3e-462f-9907-94668cb5782a" class="plotly-graph-div" style="height:700px; width:860px;"></div>            <script type="text/javascript">                require(["plotly"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("ac00dae5-4c3e-462f-9907-94668cb5782a")) {                    Plotly.newPlot(                        "ac00dae5-4c3e-462f-9907-94668cb5782a",                        [{"marker":{"opacity":0.3,"size":2},"mode":"markers","x":[0.4844045748705237,0.9586977481615169,0.2055554875208363,0.10760929569823763,0.09770208348775411,0.2028631865333661,0.7054367808241943,0.4445273699705864,0.41868072504199805,0.769147920934262,0.9181619004615663,0.4897510409622713,0.8379819925754267,0.3747727171884657,0.7465357657969414,0.6864193150269382,0.9313814677470342,0.8877163072076063,0.2061542792709511,0.3943258226392107,0.4708072453628467,0.284382171966029,0.07176138139184329,0.6561022964469442,0.38760992746317313,0.8417163798205305,0.13643314267320095,0.4900877100777269,0.17205969611823624,0.9090422467104429,0.09383023672091262,0.9838726832468947,0.9175851088120485,0.17955905162813535,0.11406244687399325,0.6852131045676564,0.632600365936691,0.6697954141460651,0.5736884747315776,0.2969665210033675,0.5632058347200561,0.48902671761870997,0.005835143963222822,0.2539063652979733,0.9024956924947598,0.9586688228452025,0.309677591937659,0.6715051303853414,0.42519576765991507,0.18446568458193524,0.28561608852833587,0.7877804280361895,0.7495789411386414,0.23198699910987042,0.05220814099428861,0.8841818155675086,0.9570338375936531,0.5015278296026258,0.22851717571492058,0.3888430231562414,0.05090540461011084,0.39399510270434046,0.573525759793514,0.5363637182713147,0.07339946794498553,0.8836313082046855,0.3449866319190833,0.13850058295707657,0.769856600465761,0.5693294848265762,0.954839995947369,0.21233459535725252,0.8825645311282156,0.4489664265970652,0.43501111004195303,0.8362171242351882,0.12510825108093437,0.4858142140589631,0.6420714924753205,0.19759334354442715,0.1025007084409435,0.655681069383328,0.2643564433915796,0.0638817148978843,0.8727612269203202,0.23866710204030073,0.013040596925155468,0.5593957517069296,0.8741556364840274,0.07269372626937243,0.5451505372960254,0.1723776179140578,0.33503204889916094,0.9488333430251332,0.6530151627210725,0.30799169948944893,0.35026376172346807,0.35959726861246843,0.6649437273759881,0.9041084996684016,0.18338486639512575,0.5099061539855396,0.051243238777537226,0.47866650335033467,0.8819205572950931,0.12020475233013983,0.9299468478136297,0.5396309902398457,0.27960216656892767,0.332335746606661,0.8907670324425042,0.8885552917155373,0.045887619802298274,0.3775961423457346,0.8507081862269633,0.12222907620616841,0.38230014117465516,0.5173029661689659,0.6625286605152041,0.8082379360728806,0.09261178295280525,0.6081880105939012,0.49708259130302923,0.3747071050047117,0.04238554735685862,0.03489953543658331,0.3493513444340415,0.8227803081809258,0.5334258103016877,0.8161229949038386,0.6803703980384185,0.7054951311359544,0.8040312737940131,0.7945663593618552,0.30559665152803805,0.5368760659325199,0.9582685739615082,0.8212492764071748,0.5927255120433825,0.7929700730836142,0.31319295918553147,0.7390148605123282,0.9332280578311595,0.6324223093786644,0.6427809864230523,0.3908102690280656,0.259136455990853,0.5942194213187497,0.9601617390954059,0.9293815749596901,0.3176267157912146,0.7575373791439968,0.4203620464025358,0.0038321666756871986,0.6362643283008802,0.12358474303454337,0.690679055089639,0.656074008299083,0.46471873677873854,0.6456587388465224,0.9882361942847873,0.7392638421024736,0.8467468251110911,0.7951514194365569,0.685387364494464,0.3295093725582585,0.3678288288214937,0.6133554337964884,0.46383293375844237,0.07035898847232036,0.2451214023010112,0.7633682325078416,0.9880519522818489,0.06981438062937728,0.06477797252964712,0.6588277949587507,0.8005946356467343,0.5078898185605967,0.7530089949908633,0.2648524486591123,0.33534806891660185,0.8722756778329084,0.6370438207611248,0.40249559258911893,0.2068731093245484,0.008291901376815347,0.9008167281710352,0.07250550832345193,0.09149147867206164,0.9858302945486279,0.6097056392067616,0.2661663243465028,0.17840963254399433,0.31205278774581113,0.03450755859015486,0.3692299003004734,0.4377914214810279,0.44757466515367583,0.46435795124541124,0.5062039401042614,0.15078902384487036,0.31966661647128347,0.32080065667807445,0.12230780109774975,0.16782286112198674,0.3966062567983192,0.5803021024523578,0.7703894135406257,0.42298155527381964,0.24345245150901518,0.1732474753317278,0.4321410487005448,0.5163578070549961,0.8412348425297119,0.5819449257113318,0.9448281122465088,0.7968995847176008,0.6040864840790531,0.5219846938117574,0.4338724176029235,0.14967702516137704,0.9458511416515691,0.7749821849675984,0.32020259200021683,0.4603637025177245,0.3500780400406943,0.9264323466272127,0.2882766504466445,0.22413091191562606,0.7647554880728069,0.6851578276378527,0.2793538691283538,0.9139469478713074,0.6841030598046993,0.831602128092962,0.7267482571206834,0.15823108722486767,0.5935484794184336,0.19305122386234774,0.48507251025204356,0.7589017512827573,0.8815247536078129,0.7793842581559175,0.27664185556756304,0.14533871246448948,0.19212419614578724,0.4839898434330794,0.3439377122832449,0.5249896365091116,0.6090188371166414,0.6256095622507819,0.5061851368564624,0.8937914237988464,0.6972144535193744,0.38992936842177417,0.4936679114905853,0.9475341840586817,0.3566034104398881,0.1479928476422223,0.2017644905138708,0.5668659597473766,0.11158228968895001,0.9335598195506315,0.4020581041250766,0.6423226699992243,0.12418831378276829,0.2918434651865064,0.20969668299150512,0.7158345193386415,0.5253184515847004,0.6849254891638962,0.43393461857727766,0.3865979082777692,0.5376702105285024,0.06063040824028021,0.2854367376861979,0.5216274877411128,0.9165967387368656,0.6615347270636177,0.7229808942202652,0.7215417610067512,0.14518814180058504,0.27437975204408127,0.8601561086214489,0.5833716931355948,0.9767131030830919,0.03270723219272831,0.3356511064755223,0.46453794799951864,0.4568646379955388,0.3580943488538604,0.1320556654931604,0.6173798899537382,0.4469943078202775,0.21538418321621833,0.5870415013409483,0.5401632892744763,0.20822194923416837,0.3387263006826223,0.9653453150788242,0.48045577210570367,0.8535699764031917,0.34079741207785197,0.2767431844492315,0.6683977905405332,0.6235356922438546,0.12722057381434726,0.3033512872102826,0.2597271566425441,0.8773296488057978,0.9182247131915735,0.8208890986136065,0.9323317345162774,0.45302882885214724,0.0475485092854222,0.11261584939787339,0.5389809186632206,0.7605680408902005,0.8827579915719641,0.5921553031022593,0.5744117433619831,0.02572485055902607,0.039393487843362274,0.23460803437880628,0.3921668575796413,0.773221313498433,0.7298102600750161,0.8772646543013878,0.14043056412481203,0.5029595134720793,0.29235005425171123,0.011152188611060043,0.48796748982375804,0.1888676063779663,0.6522703988333798,0.9844847028799173,0.506520336757554,0.8388638066972706,0.5090609923343784,0.16425488231794994,0.9233244270787259,0.2794994540184935,0.644770064551952,0.9630013187850822,0.7134200997782123,0.48712227346792136,0.7848804359285,0.4556873251317981,0.2820255946873583,0.23944164109304977,0.6903492750712781,0.2966746985737464,0.05987856588668583,0.4669583269796852,0.34795688475252207,0.0696775376830494,0.40472624445214733,0.03414575711964818,0.798670819950402,0.6745559444955644,0.17053443872428753,0.4562356588455887,0.889665230550749,0.5445035880206595,0.1958989768425441,0.3337096670761909,0.3277734195782146,0.2572972691161861,0.5372294308548715,0.5561492182110137,0.9445798925309924,0.9763756697446204,0.8374647809062765,0.45180057758143655,0.9466930877440759,0.6044131008577674,0.04182429677111543,0.31745667470870853,0.07020392046629986,0.35108022531944083,0.904883965656573,0.44927101803051916,0.864561038235956,0.6134161436949644,0.7574122576161382,0.20428656335283413,0.6384185628008567,0.09404589718250056,0.7171901986209599,0.30508299361648417,0.3669759052797198,0.020654491956852983,0.7966010680889847,0.8629937855700188,0.8118177061032901,0.10249063222197108,0.049769555299867596,0.04970179197887581,0.20055105566811615,0.281384637662492,0.09074010591461101,0.05468436330261772,0.6862365096961834,0.6269027747339356,0.4967495798497922,0.47699885159140876,0.34936915133148605,0.8942371834413897,0.6289008378278225,0.20621022431885005,0.8981755169230027,0.045257756734975674,0.8971269837093373,0.6324491641009804,0.5547205850993594,0.7911299520649747,0.527692136607595,0.1277613738042016,0.4006453927975314,0.8215246577787494,0.22276917175158484,0.37997851202354826,0.12527084142520029,0.5849587557646077,0.8296306995647156,0.07017261194029611,0.9620210140945895,0.5692635418988375,0.602365965441481,0.11999096059697834,0.20421683049463935,0.347320544747028,0.8689959763384005,0.3322618575065297,0.28768722171046646,0.8760164140421665,0.9708529957855644,0.6697494719603226,0.7573543945073639,0.04366907376275131,0.2645882302593012,0.27206296523202145,0.08283466061849654,0.721472784518763,0.03786336498763898,0.3063712352923167,0.5840299098161325,0.8707806527044445,0.28763430020570235,0.20642845600981574,0.3459571634229023,0.9685702339983192,0.30435509555910345,0.519571197972387,0.6476795980401022,0.4369178201689744,0.8881639606731936,0.32135353174504244,0.1259645770917166,0.9925068165861969,0.3198641889569286,0.8808522584528592,0.10359683732367486,0.8871325106163624,0.7941808922011279,0.678650337091879,0.7134926044596495,0.25776071828245695,0.676276964097282,0.6063679111152904,0.6286211273373701,0.3518916545989007,0.062004986211611324,0.3336623268119755,0.029000135673992844,0.08106101644993824,0.915708153733722,0.7885006827205872,0.9168075588014064,0.11304949437937739,0.13887487589357939,0.7449296070018201,0.8038887040943354,0.3605964399455741,0.1808076539904162,0.4453100278217943,0.7263350831722821,0.20635913451594334,0.6735312631113387,0.6618630665585741,0.49243201447238705,0.13672555094786942,0.4858816628951157,0.6373830071562053,0.05878343751244364,0.8580325512869884,0.24188708207173837,0.9170892484740407,0.5440696783345272,0.6753565193920577,0.3451576490369319,0.27541289017408643,0.8564037558038639,0.11929485571391008,0.08262901471872997,0.6728134953741576,0.44333772418325756,0.7877538436932675,0.14281484663284438,0.26567712374000574,0.8035827714870989,0.7121935758110665,0.43995180855692106,0.6020893554134871,0.5863213726220518,0.16963598047882056,0.8567240493750403,0.8939651335009395,0.11185986975080753,0.2536233625735239,0.9684408521319078,0.8445507591564817,0.09400829983189463,0.48979556397903246,0.9483644164500443,0.7769745856664374,0.9420719448961886,0.941898900874941,0.12798170117073093,0.08038230387979306,0.038552087075738095,0.35526478964754027,0.8701712004017016,0.17553687515794825,0.05251680439599138,0.9371202771727544,0.6054545136020333,0.8774095118057068,0.46295575352464335,0.7059712514496779,0.3451077541393983,0.5017281024364931,0.5275201214478956,0.5401429860153708,0.5616644718849417,0.7503720495678382,0.9494613237942177,0.9264567158569043,0.40076960006505824,0.8059236650967564,0.3928281964501311,0.6555320433292695,0.7223891935801197,0.5477541852857578,0.7833341263991372,0.3881324837302702,0.8195816524969755,0.44957992731567487,0.8688627748056985,0.43321620965135477,0.3346945932425377,0.540454890972907,0.8515712566528655,0.6748553004578335,0.24754473037737168,0.522454072805936,0.6495208194281904,0.2392353864377902,0.49343683973894126,0.4095048212065501,0.5568554277604935,0.3467184939855067,0.8847224570609692,0.3826304904161113,0.19984530110451781,0.40263045731198266,0.8809797489943402,0.11685749018611835,0.3510562603379448,0.14495969036855083,0.7825178212815891,0.21732118504221032,0.8948580082895528,0.19360964964297644,0.31945635972658426,0.5009657709495436,0.8388286907181597,0.17829231352279706,0.1939832943072307,0.4208655721270913,0.5120566002371391,0.07315450809259616,0.3799399564534355,0.06744218524434376,0.6631848411189697,0.30357062750249253,0.7320445718373706,0.9460174460635802,0.3960366302682581,0.3256771815273638,0.8358586700379296,0.6060111475505157,0.369801035984602,0.5846422132529316,0.173278575626846,0.5419068927848736,0.10272346839975222,0.5559903749457785,0.6681351578402671,0.5269176785644276,0.02072533517355768,0.012700991782921833,0.9114659538203155,0.1904426087407849,0.7900998036293029,0.03661430788896036,0.2752507657712332,0.8994532439443835,0.542376594821776,0.3338124982900914,0.9090256006440588,0.38941391168878425,0.17096697271422767,0.8338298100765758,0.6762219259793848,0.48876477391962214,0.2563050327910755,0.31029029428418065,0.5389563040081502,0.18442448290069235,0.25310165081886893,0.29369187762027593,0.014364039636297843,0.08182242641033055,0.6813655261606929,0.8014983069288399,0.5746634174689966,0.8845673912074655,0.8353935042261038,0.6592663827984562,0.5628560406102546,0.69384667078497,0.1607782978820772,0.2189770048314702,0.08982572943212486,0.543491927728627,0.5194676269529953,0.14374641156086787,0.09452043051113812,0.7549899504546875,0.8033861559208882,0.1371413826236001,0.40232769049501116,0.21136508843015278,0.9864509358566289,0.9823901300706706,0.09180347339912642,0.051146788967287304,0.8641588280703245,0.20015411036111808,0.8022743483329782,0.2609279250626495,0.7135928713739746,0.382098073503661,0.40842482882949227,0.057379926111777335,0.4676102944856123,0.6363713687288572,0.017591342257227915,0.14318807227319752,0.892372823112324,0.055723160656975135,0.4142761999160417,0.0693349820367064,0.6755723124253319,0.7609381116115256,0.7986444058258108,0.2781645379466644,0.32676008673389156,0.3633981115689433,0.4550955005734494,0.972525270278762,0.1698672906607157,0.9284374144320334,0.3153432084438965,0.8085128534026155,0.46696320183214646,0.35116819412831624,0.7241602103697014,0.472944346533543,0.5462026592044662,0.2131948690384563,0.7605048557350754,0.8742711316883571,0.6992376457306132,0.783548425223813,0.28256757424196677,0.7492394789670823,0.19780685355623917,0.0826920014791146,0.3116290877940766,0.7896933430178543,0.5590072560256973,0.6718082720289893,0.680385287824961,0.3625408499682714,0.8229412657241849,0.10528438218689484,0.8903997237920319,0.8221194561485357,0.7849653830358846,0.8906193750031622,0.3365068060350167,0.9119431259499212,0.6781776681125643,0.01913904875892125,0.9461785425100547,0.34757211015743694,0.5308466828293874,0.17589388325688593,0.9761799428077125,0.4138464731999133,0.030098443593936763,0.6928209493024219,0.3306805506249577,0.16430981371166498,0.14631209824571467,0.6618735093301527,0.4735417213991254,0.35091360910363867,0.19538174557721133,0.8242018213020817,0.6179047953788511,0.11840387766739668,0.34983606503876785,0.64303272544677,0.11016053279001137,0.5282841481141637,0.07714991477537969,0.6441841449360506,0.7139850643633573,0.6074645165697947,0.9172559017833514,0.5533987469153113,0.15284786472468703,0.3754523300697771,0.639605447152014,0.7072385872963604,0.49240491081124216,0.7146377738759833,0.4805637278557464,0.36567477634375867,0.043229922876628635,0.8284406386106195,0.4662916231377987,0.1568014208332703,0.38155077223615486,0.03981790510521843,0.6149540274447421,0.4899456664461618,0.3105480957045328,0.19580057049339472,0.43747603976787186,0.2872400801607973,0.143483081536513,0.5566891769089648,0.22702322005922138,0.9951205291825842,0.8412543600565313,0.2989625676815114,0.08275121633498617,0.5124768818507969,0.25906108133706696,0.521507696249333,0.18252535616763765,0.34700346011306904,0.8968123078470817,0.1679444368142028,0.5507408646761591,0.11645163480543308,0.23597049047559548,0.32591431292639406,0.9031423883165206,0.24628658298546424,0.6532335475824598,0.27754627092903184,0.37345761994867743,0.38203367302721736,0.12304583560186844,0.3829199492535681,0.031059737757776373,0.464441076142885,0.33191238337283757,0.21258024813468557,0.30788728146705713,0.47838007928304627,0.7465593772490312,0.6011026261246659,0.7540435466968942,0.19707625338166512,0.7291529960288993,0.9593974408526643,0.8294444960348746,0.17297485788467537,0.9418171196142681,0.4597707187651493,0.8813031520955649,0.23089991627208972,0.5988720761287131,0.6968216569622698,0.027651476963143184,0.8552812778125644,0.1683600020158551,0.6985142754263128,0.07939408949309756,0.053918700313969614,0.9724186016583345,0.5272142618187037,0.6897416488385948,0.09293235016006085,0.37241100927778925,0.5272811421087271,0.4199065748234331,0.7737964068901912,0.19349350665274923,0.6351014124715372,0.03682457622174928,0.40169397965918185,0.8849606103976789,0.9228030576160801,0.9516490094693743,0.6007810126004712,0.050375339272657804,0.46275500282269144,0.5577862377339574,0.3584064516025649,0.42359319126784634,0.5298334337062722,0.5989801289806495,0.4093895358051829,0.22291534543571712,0.4792408409376556,0.8437903360619654,0.6492030466643566,0.7111380550755002,0.2557851345669685,0.928655866135429,0.028109747667828033,0.48094590316379104,0.9617706639555195,0.8064657022577484,0.04812605520136437,0.7358004316303475,0.5619108167228065,0.9544355979272046,0.5063273814855508,0.8351884815622893,0.14775326344526718,0.966457684484706,0.4527205292226375,0.2939728188408156,0.9267139642922787,0.017527045381997675,0.16981025039869646,0.5442244566995993,0.7423954967722665,0.8117985617531753,0.2649252660135807,0.926166591068579,0.5825939830622288,0.8905361196079457,0.36159153535311706,0.670077576359443,0.5976941494760178,0.0043733706682478335,0.5382384579850837,0.1776856326171149,0.8232230574493989,0.24519670347798583,0.6899550750359581,0.5920582370887454,0.6480088677361296,0.8051312530189572,0.8662827044075477,0.8166023370665387,0.4848657677730932,0.4261757504747239,0.9598301922456136,0.30887898538515224,0.9446864008470858,0.9398836513919321,0.3252778712334905,0.2082985253667723,0.26587794622361716,0.35352278353108324,0.7186393783031059,0.7949667343083054,0.9965226484847731,0.4424865337501511,0.10815134434037832,0.4233636577213533,0.5529478874437154,0.6475462880276855,0.5915696816794955,0.15796099941508646,0.38108302403758654,0.3945887231224311,0.6076369906845083,0.5017620451594217,0.8019231764994349,0.028677417931429416,0.24379436212115113,0.8060757992982622,0.12629107798755412,0.10994772516902851,0.7384683418164675,0.7014921558121836,0.9808492618146839,0.711316701731178,0.5692668552894032,0.7348097400938671,0.13622283167334626,0.06446496715519112,0.467959646324169,0.7151898529871077,0.13185691444379444,0.9214061627448277,0.496229585911636,0.765419220999571,0.9133072865882882,0.20146860484566764,0.07462975461251697,0.7369167261529006,0.22908076706206515,0.2927471476203918,0.46726318781817267,0.3625408052940895,0.2531979907248225,0.7869677645643929,0.022329085546181626,0.9844372556621794,0.9552442374854703,0.33714492332496515,0.9128620545541428,0.828957347779843,0.043403934171116876,0.0421858956847847,0.6040491025175334,0.4326199199815487,0.26568249519337117,0.3668294191844518,0.09713417976861227,0.1766196678389883,0.7314409195083201,0.06314597233111763,0.924894123236493,0.3790416373407324,0.7234028053355775,0.052114899170672135,0.8015604664217599,0.13263938029522993,0.5867281596621167,0.5166786356998293,0.3516829375195728,0.6457295414374307,0.2556736273109843,0.8818749816465424,0.24368253548994256,0.9439506524336253,0.20514098106843226,0.7644798822544999,0.5891203347862738,0.8860012041905608,0.5617033911079499,0.6180397180171778,0.3816482620967193,0.9233503527344971,0.18101505692142028,0.4777491860469604,0.21710213124609068,0.35335215334364856,0.9825097751244545,0.7891410309160888,0.7615463051537203,0.43592595268497125,0.4923663674685269,0.4412125447113947,0.19148700531194163,0.5481432093636606,0.5052164181839228,0.947843833574237,0.033931744062735136,0.2562127396676964,0.6195412300389918,0.8062116686572761,0.9890291475335391,0.4235089183808133,0.7915279609336199,0.006405912496410537,0.07028425185840292],"y":[0.6939871084070043,0.2271522613535586,0.5233219965660161,0.14812778387637016,0.9999463825872611,0.31333753541966247,0.8020993250573836,0.12641431815807036,0.42551794973226864,0.6448095273185173,0.524947885503153,0.1349773061614984,0.342493824905154,0.385823400509979,0.5756822468755465,0.42018740381477504,0.5944896706606343,0.7861618614255891,0.10361077121335982,0.9215655246080882,0.027384641785446417,0.5752093036946143,0.6087609500833933,0.0802882916859432,0.5860490673259945,0.8905228822750644,0.47149070597707055,0.7523864693949527,0.4363434021235576,0.9483659960051731,0.05201196238514383,0.762184182545624,0.45135345551720696,0.6805777511827544,0.5964860965171513,0.6518773161869286,0.45071949722859794,0.5688824352062263,0.6102424038891904,0.5935361504133866,0.7157154666308788,0.1025484757663292,0.12436359762565441,0.054992780302851374,0.8932791067053948,0.3842741855681169,0.6903090546616547,0.3797987976011984,0.44752422720290375,0.4363294815269949,0.15524459287424353,0.6782801807514466,0.15058051474203005,0.1811042833917006,0.42466981348207766,0.6887479940470127,0.7703237512711255,0.36672834059885673,0.6074036274329695,0.07853508562236056,0.7424805742910434,0.7907302403313882,0.9646447027701633,0.539400966754535,0.9695790623097985,0.47032589370035116,0.29890404508793134,0.8268372025435791,0.7302514915776382,0.7113514273739987,0.05705515076552592,0.3155274224389084,0.5526355383810474,0.01887541142323146,0.09505467137351986,0.6442048254159709,0.8551779984359056,0.125439203268344,0.21068341726216844,0.9696789313139879,0.5084043789371421,0.13931045625546934,0.790575795361949,0.5850483259587705,0.8397110155801961,0.7998554816122949,0.5397330830042527,0.32591767084736145,0.2380574726785588,0.3846910116822736,0.13798115746807527,0.273199845076329,0.5919736729257196,0.5060749640659528,0.18841506374751293,0.3364457214825065,0.5960235422757206,0.7422577050659276,0.4376793239320397,0.06663657681290436,0.36181339632539267,0.3214314876084401,0.11124603245971798,0.194331006912981,0.2659152559070539,0.46408796785754725,0.06755313365182691,0.9868554776382824,0.9835763815781587,0.008311037700919122,0.25109534266642286,0.5461330188418786,0.3967045326587797,0.547079515754605,0.3639562163068477,0.494521343447219,0.44411607124005315,0.1731416543063885,0.30863874322452844,0.016566298751011344,0.4760579905985275,0.6461740397204361,0.071144866618993,0.7712899999875976,0.37461065561782847,0.831788530825519,0.19705831908988847,0.2374701528393811,0.7449515073500977,0.8418593118871404,0.26884542487840846,0.571597169872879,0.0708984166470904,0.7885769962434673,0.598084121809462,0.10676950535143948,0.37394852995572647,0.2742569136607068,0.3987462590099148,0.1859572812290392,0.1166596969639837,0.615628790315536,0.032555036347929,0.6406674246831182,0.25316951286789646,0.3239261576139958,0.501305598879507,0.02924043944175514,0.08104323558731341,0.8196352363834664,0.4767387130338542,0.8241363876128727,0.7750297892974335,0.2580230671927237,0.08203407344518676,0.931552368225893,0.6532242283951674,0.9024038869679272,0.12090452401655505,0.22825374352893157,0.41149351144615465,0.5164362673165933,0.8622817284966545,0.307290338382899,0.3616979540437515,0.19277000294044566,0.3393699987043126,0.34184801535886933,0.32307106374485894,0.34902444980415925,0.8523608041710469,0.30546487661528987,0.48320920978651705,0.6918242030303536,0.9782735024122107,0.8593908379673305,0.9412235214799178,0.16400607530720812,0.6710605703668885,0.5513836033383531,0.9575534575574538,0.36425605237917746,0.19829574845857412,0.5980359822939424,0.42778494633683384,0.05783047120270279,0.5153569781923284,0.4630875357625378,0.17761046052059437,0.555500838299878,0.9637524038810417,0.027599862441618495,0.7936707072929139,0.058063637871596074,0.8602674129248385,0.8858896725410799,0.5310814321981795,0.675744225608767,0.2046501168893372,0.548826533293952,0.13720525811412077,0.1871535624078824,0.20388560437482917,0.9874174617170715,0.31735742103944575,0.01925542322785123,0.8647959727527458,0.7927441160067127,0.06234783778744657,0.0753663306138741,0.25298555387613286,0.7321938007783205,0.8886896423244345,0.28800232196451137,0.869853168871922,0.7700518917677651,0.8283045760201462,0.4824212700947671,0.9423560315136015,0.7031057086783401,0.7693040753769315,0.03475943060588493,0.37858075771652766,0.45572206177449126,0.33448126132562084,0.9277442103781877,0.6411094511123023,0.8713288879882101,0.7071424459453083,0.21068234171777434,0.7597017623317527,0.29354173307129616,0.8614681834309871,0.5819453915539455,0.7074795004510037,0.11328297550357591,0.3511261097331069,0.2590293313437847,0.3144073379989223,0.14915754061628395,0.8165894494318074,0.3061786841140045,0.26529854158692645,0.6601575395837954,0.05089735035575327,0.9684104694264415,0.830175542201278,0.008075913476424446,0.8812465356796482,0.5158038074241449,0.3127707588718096,0.9786518775012413,0.8811623443609117,0.057866303073144,0.029542074451331946,0.9473785149860482,0.14030313105788472,0.37667955233109396,0.5597553182189383,0.7312557589598454,0.9401180339141867,0.7454073347721835,0.2226752091893751,0.20054399888468166,0.41700018467496014,0.24282459966729908,0.873169777660923,0.5744858288303903,0.11707920198955946,0.6967748611142367,0.45716491608099896,0.4635704161412926,0.6586200084709177,0.5605908342096063,0.6371520983980127,0.5267026518735105,0.7983828968579625,0.3757539916261524,0.07214372919586576,0.4965155026754219,0.578809405164485,0.4871160745783292,0.5654414335731723,0.30173520372650664,0.48299229584330805,0.4228597983412218,0.7922978081968135,0.9526207970441777,0.07331697954491356,0.7871434063910857,0.4688951188805047,0.3450525900269651,0.2756610091239967,0.64623623359554,0.008223980636105743,0.4175120438764387,0.5068265964184139,0.6689136761138024,0.4920434677061645,0.8799463287728813,0.5800379238188971,0.5670397090811659,0.6113824762576469,0.9003924354429378,0.1967571484899957,0.998052314366261,0.836008215668953,0.09700696812533782,0.9796898225578985,0.0036687821115585217,0.8057842675440133,0.034867276464950026,0.8113186065700637,0.10430646131383614,0.05617031909653902,0.5947546471666323,0.09857438502273741,0.519957089788786,0.2862483896682062,0.603116706673105,0.6188857254759236,0.5142746142697854,0.03382314277249199,0.4517748112045503,0.5504493047650637,0.9920681808293779,0.9848168326762882,0.9807526083907553,0.8019080423157973,0.08220725753659386,0.005999297222298083,0.0517815190833556,0.9344732147633672,0.4357035064452216,0.7546467183019558,0.483009611367337,0.11821205531927315,0.798112459028997,0.087760362715252,0.5025973420280744,0.33216198154595955,0.7536967605187304,0.5973829537877308,0.2155578092136401,0.37278549650790105,0.14894906986703493,0.8113130179803756,0.44362376508787593,0.027492456244333607,0.04611599340435901,0.29117698421119675,0.9276577008212697,0.8403911801523029,0.011894644525997422,0.948292611485691,0.7409325211415898,0.004590962281041944,0.5066457634424218,0.5485352984484081,0.7953087984304065,0.6195889304951105,0.7455045265506325,0.9077189779188125,0.5878710517432219,0.45179565380547837,0.8488991471983862,0.5900194518963565,0.14180231138007815,0.7339052835167824,0.38205162724371655,0.06662807165431861,0.24363829481184562,0.7757468081042175,0.41524797903274224,0.07974853994749165,0.339908144856535,0.19330144480057243,0.33950690136195727,0.9792639246462835,0.07130054724206847,0.07980761810605752,0.9897752118673355,0.7221551048292623,0.8662333024818587,0.668539378904915,0.05656359469341965,0.7994249342595116,0.8671625698807839,0.4283328494265646,0.7188355685557251,0.903400664103495,0.077830515100491,0.08423400025081784,0.8171100000729425,0.18947962953765585,0.8444081027381694,0.476140076103468,0.3178036540882667,0.736479903654524,0.051108006826030006,0.6814439651484991,0.7209615883721735,0.03790620642840925,0.2380166927020626,0.37884977729624525,0.5614675161507691,0.6495080028468733,0.41382364170103914,0.667276620694801,0.17568799633715182,0.9107141094214021,0.007850782777502396,0.07485927941911452,0.847923916204841,0.7604238955574301,0.6077976859975931,0.6090575144296719,0.9411690500929445,0.4490087130366883,0.2645165234938013,0.20054741030121337,0.023588988431898383,0.1412473024820553,0.25693002392493514,0.607446193020867,0.28575068568600615,0.7644358802436321,0.8117721852469097,0.9504226861546153,0.5857308095738376,0.1269423756220026,0.29989735782593296,0.9068475872733941,0.8236091407011936,0.23001578609848738,0.11904933983047594,0.3279546834782706,0.3007704355267261,0.08578123747861888,0.22358828909549433,0.9407788265621243,0.31352987390892273,0.43283905352652996,0.7260045914871704,0.4341156450395808,0.6908301357513027,0.5854257921820187,0.451795250533218,0.6696425826999063,0.24057143553102323,0.6975495684464544,0.6927389976450429,0.16727588884998668,0.27222957424166316,0.43126341656162803,0.2738391593773628,0.49330083938580593,0.2952341995822708,0.8325196738465116,0.5689224236431761,0.47132563310670883,0.7722067791516755,0.3721992320982993,0.04766062606495114,0.49982205072199914,0.15838920352102737,0.6086496633827504,0.30431584793611866,0.18482404918156004,0.007332677031389623,0.03879866836038515,0.8625141998142577,0.932902038411531,0.6211057486484287,0.8078672955662246,0.21819867573260732,0.32079354481728195,0.006847280121651478,0.2715198174478032,0.855513570279846,0.5587062745070444,0.3130564399130641,0.26085538417738785,0.7339063632194525,0.6875922021498163,0.023150672021863383,0.3496141344169442,0.6695599719966395,0.7804575540894468,0.4590368051972976,0.11219096913827009,0.6527842425503587,0.8111273751653583,0.7795469987271125,0.2117847244953861,0.3604637008297158,0.13320357093257673,0.9194584577044573,0.6683807622788288,0.3862710003736298,0.629704606771983,0.7232428323996398,0.3375886360462048,0.24773665422992186,0.5703550235871364,0.5052727197181858,0.056170234085179405,0.6022350631541371,0.05916306662720516,0.2858271071033741,0.8627136298132587,0.19044624942934962,0.05764319836801224,0.6435038345131568,0.045832403432489954,0.4635048013573927,0.6092648743921207,0.55097115512791,0.021238205398385168,0.8865099146778919,0.28523512816916685,0.8804899512265347,0.31821872853990674,0.21261508879476088,0.6244005348413189,0.20129717115380585,0.7064772818938488,0.9702686450609999,0.7570947398876666,0.8151997554641768,0.7919695294019158,0.23182511551141738,0.309273295662711,0.36983738806936184,0.00608677701212923,0.6009714029043044,0.9490988941709088,0.28681912369695395,0.8837271717116046,0.6981805236358404,0.3329885862002888,0.6555532256084721,0.5331150916438719,0.12354066731446722,0.8441201798743455,0.6442737854389101,0.9254492290265665,0.07579655744117064,0.21998403036037617,0.532698049470447,0.9437459478289608,0.22496521213071663,0.984796896542077,0.3071854476675083,0.6628763145222702,0.8370351130257355,0.35512710646780077,0.21235770967706358,0.3279957420794012,0.46949762581448107,0.056731806006103835,0.2602632752070677,0.7862444742992823,0.6477999588600657,0.6554543399441162,0.8248336838781081,0.6017880237799448,0.8139447700165909,0.4040301740420056,0.5407767591349624,0.8260181225693561,0.32030849838664543,0.43004804054985146,0.41861920902112804,0.10242775408599825,0.08458745966461667,0.6946178227370605,0.6646384915145723,0.655725629676886,0.619759777312031,0.9899312680155343,0.22195597475023576,0.21559237307264045,0.5277495752045133,0.20084376914168944,0.9153402203422498,0.4015791187180078,0.3005958887031067,0.26021893901581705,0.27202053761573297,0.5528142866104536,0.8589343280707331,0.6577930846226309,0.3989661577676048,0.8219020989798701,0.9318782075338317,0.6764125407389914,0.08737745738991898,0.9731595598556888,0.6263807537311658,0.6694153951188537,0.09442400776233129,0.6489455976788674,0.42410562953594855,0.6226585491342261,0.9784213652448319,0.17912115870640655,0.5471045982978952,0.2770268068581271,0.03246661516950866,0.21536372850143126,0.6014500674316204,0.6812341901967907,0.8080557247074055,0.12802901218623963,0.021823048165833248,0.010105145027369034,0.6931521711319277,0.3543300965116417,0.42059428503147356,0.025488923168810684,0.6064494305076034,0.0664674861717427,0.8212678302538655,0.744902924227854,0.5224889721049337,0.6768436207722572,0.18704590693614753,0.5396010971315826,0.2727160288095085,0.8811514391514425,0.6491943792607952,0.9382089419132712,0.22893095483478498,0.6022703328111875,0.031141493353981642,0.6066550629900087,0.353230784704452,0.12321060182705146,0.1956368163183415,0.2387441783984079,0.0006809801555976991,0.8701697952797508,0.9873119336676988,0.5047057394505934,0.11941024349760443,0.5673844839530027,0.4245740890275884,0.45446961421436527,0.09336092145946384,0.06945409642332556,0.07468303665271181,0.7664963034818013,0.40378589794602127,0.5010379161692404,0.38601732486750473,0.7046367717608224,0.2634385837778972,0.35198907249245226,0.8414805463985711,0.6418420391054396,0.13241770331091196,0.8114955201542734,0.1957004222059231,0.6854406093784646,0.5227166584858417,0.6500633454589255,0.40354402460298033,0.5582994325162763,0.1020854043220144,0.24890575129520887,0.4000202884072307,0.4216485626218305,0.2658694664930634,0.7770721848939017,0.9566892279429544,0.25418138871500484,0.27115613944504313,0.9622306906327125,0.2498422138614943,0.994603723112545,0.6547530412519179,0.23007498682458816,0.19911091027965866,0.6072029086458108,0.8666295009054484,0.00854689355560867,0.2072561826287419,0.8693551067549572,0.9358978525205429,0.6267606032457219,0.3056914103548124,0.5079564608022394,0.23181711827763607,0.3925922783788931,0.9529995666626355,0.7579162099442789,0.8998387858605639,0.24579532453174024,0.5720042245310277,0.4802050449882971,0.9129387621519611,0.31746291393024284,0.37311274161112684,0.7586615105293761,0.8785758984860313,0.10034295765120671,0.6811945255259229,0.9429808480220508,0.6806464875727489,0.11011079613066277,0.8345816952455672,0.5223100990083168,0.7124949837658078,0.15401552360937598,0.25257324133747805,0.7778151098441795,0.4267232207239967,0.0583483638363661,0.8268291800220093,0.22672434468470315,0.48865886434522254,0.4184448288866902,0.9486613911960122,0.802900701172625,0.2799400939371621,0.1300979740027135,0.5931528026536643,0.5267302446356751,0.7553806106926467,0.7515924370714745,0.5912534343750245,0.6126266921513915,0.5412222347617491,0.49823894788189593,0.11094061870583205,0.7951348903919472,0.5204771964899276,0.12793258024846343,0.6478227038213713,0.6616625295364585,0.0652249993365488,0.427726893765914,0.26581827070775343,0.13851156334616477,0.9019432485796642,0.4915843092920761,0.8198489753603022,0.8394823692498863,0.4185591353151792,0.5356618291355653,0.3639975489705426,0.3496654314439068,0.6047271472422074,0.37760630702788733,0.25730549963018456,0.6254784322557833,0.9777544812285601,0.21710173928987087,0.1918709098024698,0.1660164554777579,0.9242466459678745,0.25339032298628217,0.6167663166084785,0.28017087289683795,0.81451510499305,0.32766987211378995,0.9807269613848895,0.9614635532070477,0.20686219817475326,0.6996989811325608,0.6628897327750327,0.1352716981293115,0.853142371137291,0.6957519700226229,0.5207421755646241,0.7397238709938847,0.5418609517826874,0.9824216073348266,0.24633969697535563,0.06547837563493308,0.3138632507317718,0.9648815750885072,0.9795360953661663,0.12643289481754982,0.3369097421444074,0.38805997498739486,0.8411323618545794,0.16746771992511333,0.3895415743707853,0.22416166012954442,0.023258494110383165,0.6451408537948016,0.14523361921665567,0.07872853634761257,0.12412547766048598,0.5875470909581881,0.20181788453822602,0.12307289253247011,0.4996263477362497,0.4096273794376236,0.6559618739532622,0.7768138875494531,0.6414540105971732,0.2348265340249419,0.6138443604368271,0.8951435175897303,0.3237243743517372,0.7379866154011586,0.47353904463921515,0.6000654380665464,0.5674636089510716,0.04519061563721105,0.6297816889350842,0.47375471405793634,0.5749139672816105,0.17504115665531073,0.24439527181859844,0.9986316714391124,0.46658024714525625,0.6078804518786459,0.2472989056629945,0.3864716770400669,0.9757861511477887,0.2127296338204887,0.7820611425315216,0.7117669780407277,0.7195977666276436,0.00867835155855734,0.44085823678605895,0.9664383136731405,0.6272308832465189,0.19037146294088236,0.18829724723273011,0.7813168907639595,0.49016694528761073,0.18323104709158966,0.18280928645128847,0.8481752225253948,0.027388132119428144,0.9546081666540706,0.23956616066100023,0.6591021471993892,0.44438456645326063,0.24508539471046342,0.41661896202601634,0.20870094023757169,0.5276668643936638,0.3394791922290794,0.3531558295515522,0.5133037179824216,0.41831430776918876,0.900059751130457,0.6816114300465268,0.4383626469128211,0.783559649816939,0.9728139932159607,0.49919797226592544,0.8103609606674295,0.3969945790869793,0.12054049179086024,0.6101891808233549,0.1281477114853159,0.20428066545240064,0.6122108872190203,0.005092058800514221,0.2184850670543742,0.530530478806757,0.04958080561108558,0.46790597673970424,0.04840216751799986,0.4670473858191544,0.9027480589235869,0.921876404197032,0.7564357973812698,0.4708209382289724,0.9102457172327787,0.14031121494052745,0.6259851622915826,0.7801423609434038,0.9271353789575544,0.6060520599318959,0.3457561483454573,0.4501220818195931,0.3523190031833112,0.7460244601643241,0.05189694318046878,0.05634864359149783,0.5078279021418522,0.7965824145532924,0.5673080255253885,0.511350471281945,0.38445664194051454,0.9106008995805975,0.5222650221571933,0.2541970526755166,0.8047529617788436,0.3511795874480902,0.5789116810159354,0.05908370949084463,0.7384330505540163,0.0463089432786713,0.5584023446474418,0.06262570463046346,0.31554756678494267,0.05249601369960333,0.19492831798497523,0.27818896200949905,0.8555138891541422,0.5639406306036329,0.25418274110697003,0.1077448828935541,0.1918930805760204,0.7620431596726251,0.7349213994039194,0.5453037107231264,0.3885308944470528,0.041589092977295294,0.16077887985714368,0.8346785597412415,0.8459734940351387,0.8048133574726419,0.49104385195160216,0.7135872622121046,0.8004920187544523,0.22110848049881748,0.417012705758508,0.0028391099571177802,0.01808817601476953,0.024421926174491015,0.6531093805750807,0.10425141845878816,0.5651893250298106,0.5967100787694999,0.055330884490394916,0.15972187295989593,0.37431013153145865,0.4950720878825172,0.5633049124591488,0.8082462242455096,0.6543010094563193,0.13193782663920584,0.4487601373414243,0.9699368131327766,0.26798621750199036,0.18768416079024275,0.8735471487226935,0.49868387955298943,0.9958528845330199,0.21905912164705843,0.4628508026841496,0.3311192642740318,0.7971357008985916,0.7645902144706941,0.2875354108217463,0.909417281755697,0.8443712067159211,0.8568925534619668,0.22550732397987594,0.8306150106374339,0.06977409806638157,0.9114586785252025,0.8379197702314123,0.5377866257845059,0.1399834685166257,0.8865725631715798,0.5872927996059086,0.3158315365200517,0.05189455726270131,0.0816414827456774,0.8093606743070036,0.0864612490285721,0.8027317008114625,0.15650671976781205,0.20013066591576922,0.5266070385512986,0.6565174253921201,0.18612104028232257,0.8976132934851399,0.13839773856258208,0.3170215110887076,0.5949309678721502,0.18903659626263736,0.5225097338414009,0.7583445946737076,0.4858292288788778,0.10082829641507485,0.24320501260167204,0.12938238900127308,0.1373772048520906,0.30274098633604907,0.14181377570246434,0.9731884607914257,0.36844308061910713,0.5814472758945074,0.011386576484796684,0.43506570370736974,0.6517514309748034,0.8978247120123758,0.771428263896079],"z":[0.1900140342079966,-0.017559974767502068,0.07401687624389894,0.10880608557845402,-0.21445949928063152,0.08556314031363713,-0.06388830401975742,0.08739710835377007,0.254643803042109,-0.030889528376100178,-0.16123366272049044,0.1491176120037063,0.004833648198968643,0.1420690638316705,-0.08476064869993152,0.18939360729485205,-0.2690667445177383,-0.2883924774877205,0.004736589758373877,0.21515242179496463,0.18668447757556983,0.3916398671356899,-0.050041313024624445,-0.020224394352822243,0.2223873593691364,-0.11594899660183115,-0.04048951438764767,0.24574255094352682,0.14833884763480296,-0.28323578680650946,0.04693209709868769,-0.314804337753456,-0.29652765463578823,0.014727126025508491,0.10232761867779194,-0.05392647402048228,-0.09193001259257964,-0.03217063715289673,0.0012265756208495876,0.254454377098365,0.2066184758030603,-0.13961227235321516,-0.00744860127056937,0.03448355260044804,-0.16056353076345084,-0.24104417748897805,0.35762867399150267,-0.08699696431422677,0.06252817745768727,0.25821097959363687,0.09641553277736489,-0.053775245882418665,-0.11149327982774904,-0.11020335980879088,0.1400846529213357,-0.3550506441726803,-0.18318505674111352,0.10273954006314706,0.14280864405795562,0.19561485896395275,0.027503331030453222,0.21735516584477788,0.11469943552706487,0.002640065091465485,0.19865710198492892,-0.26054244448506725,0.29199439813167377,0.4553038215202182,-0.0631980042303563,-0.06926598098830392,0.13915238237102878,0.20564163088563067,-0.2091465455062242,0.05343866642067048,0.0070404391409659844,-0.2855213829349277,0.11142074171374244,0.14470300375182737,0.03898777709122254,0.15327550681514884,0.1922633989908792,-0.04893616934321085,0.22597618266493202,0.183442871737314,-0.18657698156591745,0.46774459769880167,-0.01902027648444135,0.051004364057010344,-0.12315172902857217,0.11983590175585067,0.0013803030267964256,0.28797002790521786,0.1655324709175992,-0.2715458428286329,-0.056654631611527645,0.17786566672454215,0.16585166489387274,0.22854806478995096,-0.14078704725615124,-0.006635819646412147,0.03395595081506653,0.06901900086642788,0.02358923697888441,0.010338739109916076,-0.21031005687354026,0.1951287520378101,-0.14710555222582872,-0.009578553127551112,0.3068660794168091,0.0832603966121706,-0.31928611831060344,-0.25599115407834777,0.014384277467034232,0.23761359721072448,-0.26585973969683896,0.021264232559885105,0.30037073405258774,0.026789935987475866,0.012331308615984637,-0.02196403272508627,0.187700516038314,0.12300678163458457,0.14028821961721294,0.33174010023765743,0.21632240132167874,0.0154126230022721,0.11741199679874893,-0.12303283789690549,0.286912313788453,-0.07596405061888145,0.08374980275795413,-0.1436247000836583,-0.057838882476546574,0.029245439581868954,0.04607309262460968,0.24054810798660942,-0.1289122400064428,-0.2492906103736142,0.01969030137217247,0.02099079112699946,0.028133271297165294,-0.10210194636144165,-0.1533037223864866,0.04148556993066009,-0.04760393610565739,0.31427502239544386,0.29151111639634797,0.09047656065748205,-0.19894733997577824,-0.2870810238854856,0.25193567346479784,-0.18607350433976333,0.10688148745293183,-0.04162479847717995,0.10915133230328945,0.3641922294301553,-0.06491957755977934,-0.004900750332979216,0.21163261022929078,-0.11715998593363569,-0.4432865323565569,-0.08780664596730192,-0.30449688079827225,-0.13975767233765352,-0.05381726139272203,0.021740658956532793,0.16324645505321528,0.09254889918540038,0.1500296006870414,0.06324305400378706,-0.03660446351461588,-0.17070110676705372,-0.1938762818579025,0.3099908442874937,-0.06883717466406106,0.013044036877785256,0.0591654622555331,0.07326555469203197,-0.11579807151215964,0.20027397750580656,0.30927616218637294,-0.2017794838596949,0.006676050944096473,0.2884645545477023,0.101676973324694,-0.0009321192326673649,-0.21052800502504326,0.01819414377949461,0.2800252551356037,-0.27151471864349264,0.0802793689765913,0.143052585865785,0.2635510217932321,0.16657813662651666,0.1019426335689775,0.20953039542926533,0.17746414836235663,0.43015298204260033,0.14362173710820397,0.11823302535824161,0.10143945421673292,0.21480017517364444,0.04883261633580792,0.06606968601181329,0.11315841310302559,-0.10895911584936002,0.10690858139506862,-0.14779670320457322,-0.0269542389284316,0.18624645602013942,0.1749186299241806,0.23830861245468585,0.009416053825556131,-0.18312068029831313,0.17416389798496756,-0.5133890183664284,-0.28026190408839813,0.0764526895534604,0.16018589828783253,0.07954630556616846,0.26149042042429455,-0.22449536807298895,-0.2108823329090065,0.28881805212116607,-0.004575923887697619,0.19892029943063405,-0.2328019583303006,0.21647163390707008,0.2204296229120928,-0.004762990871068018,-0.0802850577472349,0.08858523080276763,-0.347797345164214,-0.11294924055846108,-0.45928853479809006,0.05333679094705743,0.1667463344213094,-0.01797978401437819,0.14920002473286392,0.03285581245239557,-0.1756059397795803,-0.39654822491484343,0.06634213412965263,0.4374912566926718,0.0054965064077769354,0.23671987917002696,0.17095772178071944,-0.07195707166351023,0.009990656515913507,0.15023105110343865,0.03516290736379912,0.17976976865971644,-0.1915196893958857,0.07475035482143982,0.1140058591061881,0.23489982759518702,-0.07638627204239791,0.0973835919107462,0.28481407768721645,0.4360990166806454,0.1639630137639727,-0.03338723737126964,-0.05621767127720574,0.13562492625995592,-0.14960453564755893,0.09521820172602609,0.18736190231225697,0.1890896599750096,0.03812033475264089,0.09851175576569218,-0.38396163268256805,0.1348949336256647,0.19826064313152253,0.03026200050152758,-0.01011376740277048,0.1408577341045619,0.14229428049661258,-0.09577103965366923,0.05589784111071325,-0.136268790891511,-0.25152164277708106,0.15961005155991922,0.3622275029294004,-0.13328000283365415,0.04936936934195056,-0.21351700378695979,0.08343762594156012,0.33512313314200265,0.046760443531040086,0.21463302873216583,0.2156324005186513,0.06642538935087235,-0.011437257471578508,0.3652866312209476,0.13861373835560004,-0.06219664994664609,-0.012831909195504254,0.15789233623820392,0.21152963264012353,-0.4075215140426156,-0.020829948607111237,-0.09274981733343696,0.12998911499874646,0.25234856998405986,0.07046869722642239,-0.06193107619865995,0.254438576713284,0.0784204430989001,0.14863255106698556,-0.14672882500712425,-0.1527685515756732,-0.21081942369714995,-0.1495658608398989,0.15972848923087854,-0.046812363860347636,0.10360925764129632,-0.2413330531950297,-0.08459104628429148,-0.1360722042053124,0.040070576853440995,0.042925627997824164,-0.014862832742528278,-0.057504123941473965,0.3998452946322666,0.08846297950711987,-0.07743968849308196,-0.22777013343607616,-0.2818245700474781,0.22825010405431612,0.09205346976233275,0.16572425901747453,0.051567077190934014,0.3054528264886879,0.10051642817631015,0.0825015830269755,0.018860948156993268,-0.15382198231535785,-0.24144988388982788,0.14830541417454085,0.030534918124562965,-0.28593824707349635,0.19616577732813645,0.008803703520281297,-0.15792923240413748,-0.22723006897802336,0.08407391249250928,-0.30674134192617153,0.21445992119202817,-0.07822364748007937,0.05347545482611359,-0.15644791860903906,0.3715019742143015,0.12739855907714884,0.05107072499630687,0.17234423385899544,0.015883834309802405,0.0009095477003625441,-0.09569820784350365,-0.2611181675439048,-0.2592624527574774,0.05725289454037244,0.2755643942963517,-0.3235847564418167,0.15107545459396335,0.29331900035024766,0.16381414695544255,0.33589732504198433,-0.06934306360664988,-0.06025328384164205,0.059790734722130985,0.011279942665827117,-0.05665902452307914,-0.23580977266569356,0.3217988717012242,-0.11921071380262824,-0.03738429937079281,0.06994319213540824,0.10815896803792076,0.05490268471397839,-0.03756270502623117,-0.11513970835884937,0.09435515683182388,-0.30186002389182803,-0.04221021328806569,-0.15054879977488594,-0.14172581186275288,0.05543521287185839,-0.10261886846785233,-0.19174930471649718,0.2843851334260964,0.3837244282865897,0.02524531103611083,-0.19213194925195154,-0.25423665245973426,-0.2444898456703477,0.09254134891320925,-0.03815136614153086,0.00569619334189829,0.21155397857268762,0.01937576247464895,0.17166612816069093,0.10850267210861667,0.20719987783456073,-0.02002965052516432,0.3051639810323856,0.18152882368829576,0.13099574665357772,-0.38218905880459714,-0.09611734246926518,0.14094330726125937,-0.1692415944949999,0.11707473913809105,-0.1292787710968823,-0.07707982623684274,0.17388247233835513,-0.24816572804021636,0.13310124522152758,0.17993539258573382,0.2233301633783136,-0.15290924324351066,0.35584174789523937,0.012057468740978405,-0.015867095481452875,-0.07038170048640463,-0.32246253542268855,0.12016816481873993,-0.276100954008953,0.1693804084369698,0.17171273018388142,0.1458969698068539,0.1869333584866491,0.18422985046317922,-0.38231569901518975,0.27362098138172114,0.20857261633817747,-0.20665881140778508,-0.13776690870755195,-0.14638928977789084,0.11573767517808844,0.03381840666759084,0.38295072912800876,0.11471076147423817,0.058161156245054094,-0.10170588530960972,0.16261259428947838,0.2211763255211194,0.20703796115582368,-0.21527468487272106,0.3813663520366856,0.1530499479131129,0.2860583551583379,-0.32295022039688415,0.2732258027318051,0.1322951279935548,0.13949121681028717,0.15498575943645476,-0.26838405800545717,0.15169302184027875,0.07304642386936953,-0.19928261374276351,0.3009546108658108,-0.27126878788094877,0.058669747118972417,0.029934619241496124,-0.1800335856079849,-0.04654239502785197,-0.10534943163632322,0.2826757075100826,0.029283471037211087,0.17871362332370494,-0.07670156590624906,0.39900181581423044,0.14935954060594994,0.33770780004376355,-0.05228510668358973,-0.01257059925435449,-0.06865621784612894,0.004907831796467536,-0.21103689798244668,0.27140677837362503,0.2598770321122288,-0.03938196299436071,-0.01118740410094654,0.3722707941163441,0.16678794597467056,-0.03752713969365458,-0.195810458544927,0.1665137579837182,-0.10828936969365421,0.0046991891422796805,-0.07115503750067528,0.17808196884270622,0.15475213883691338,0.06609093046502652,0.06457769125477834,-0.10165157875721427,0.18115884189354,-0.236472252729314,0.08517474773229212,0.04803137298807964,0.2035295963512847,0.3420230950849055,-0.19017344184206317,0.16015744356236836,0.045745462785331696,-0.08067284767844476,-0.015749262161305656,-0.37845199539996643,0.13634153086753875,0.12253722260453956,-0.2469508285646843,0.0016400497640235087,-0.1021379644823073,0.05668002751878945,0.05676062112834221,0.041199287694120484,-0.19375882881840623,-0.26593974096212386,0.041114395854341526,0.13787947080434587,-0.24306096402582583,-0.13244484009636742,-0.07764927254824344,0.19738501378447082,-0.30237031159790584,-0.18153402855847947,-0.23573710655444188,-0.32720421531112637,0.15689855920754725,-0.07761097861815788,-0.029066127335661905,0.13623278563967628,-0.1442170004503972,0.22751062444594294,0.005034494348363789,-0.3482479638281548,-0.1058984414479835,-0.15074440907875722,0.3622589268096822,-0.24991971214261754,0.03380587066202714,0.040628302991137205,-0.04779690858098637,0.06582476488791772,0.031015847042466925,-0.1366219957880218,-0.32485902699281616,-0.07017432558186644,0.0328144191222586,-0.06565077746907663,0.38688209209215196,0.16402204361992131,-0.0597385611451029,0.2287414923338378,-0.18468274656021283,0.29151793105209783,-0.17945041033500203,0.17333542462340185,-0.3508726105177101,0.1758133459860326,-0.07651540975161555,0.05881269277249792,-0.3097909298425968,0.009476999270205504,0.22200144133380212,0.03146060754195883,-0.10273903448184352,0.2517552953707542,0.19290673052093396,0.2180298230559044,0.1844932343631119,0.28317497831682464,-0.18880226533203018,0.2700852236124192,0.1353026073248304,-0.07018266126833025,-0.0450036251859548,-0.08332161544175667,0.25251864728784224,0.12882477303197845,-0.17509828322026663,0.19300067689018433,-0.09866113700914719,0.29827220591106496,0.2811424742289973,0.16185170777471203,-0.1274012737657303,0.2588160262677072,0.12623032830932432,0.1840335040209087,0.189364145184863,0.021005298865970884,0.3206641939019202,0.001630706846417057,-0.1206032313555682,0.46959816497705603,-0.11670364578640542,-0.05530613962192594,0.1860573829775401,0.3583510908904565,-0.27033789610898834,-0.04035032349659612,0.16111791544858708,-0.03544250408306587,0.4512769924284761,-0.009112630265082217,0.18908905869760334,0.10266770767846521,-0.07052294426555372,-0.010664179973943817,-0.04889054250108634,0.1110547588618036,-0.15850638051193372,0.26274171165296367,-0.07377313048619556,0.01006439691662481,-0.02455316014370191,-0.37562739556981073,0.0518907907345747,0.06283488984360178,-0.03619767493569209,0.1767330069714389,0.06417384326702326,-0.26558732793375595,-0.13426520384981872,0.2688398111483384,0.2224075082630383,0.10580512092929246,0.02094623317077665,0.22169750772750918,0.07320650039635293,0.15247768333763717,-0.04977333296272417,-0.030143635894415176,-0.03548200870114357,-0.15133213416960717,0.05960397562148875,-0.07034532911323584,-0.005915985592557363,0.02921996652622986,-0.02307854360091397,0.04338186798809008,0.24699638957238546,0.14683544976791824,0.09466061261502276,0.023129861893777342,0.24064695894768484,0.16970617296264195,-0.01071027625951633,-0.07053982237900736,-0.15208169541980737,0.18162691745428206,0.11811237227594848,0.2613074389494558,-0.383660864888013,-0.05579877102971073,0.2681987576832695,-0.029347992747643492,-0.04655818085410393,0.2354736672077009,-0.04816100918703137,-0.006932384067234912,-0.26445747177207213,0.21302675342975907,0.32331111600336176,-0.04354419505540193,0.18307690215741684,0.2539383815800309,-0.289754932841048,-0.016386568120281053,-0.20136188510622402,-0.08062692700026484,0.29252600199343537,0.19256677325627602,0.14806252130907083,-0.21479662192900065,-0.10003307672378416,0.019659704030762115,0.3823904895777894,0.2431524634048472,0.2551923611070046,-0.2994446646565775,0.10305891932516681,-0.16160895027873837,0.0389900170796057,-0.22700886325558403,-0.1058228682594621,0.16426055079653462,-0.06380779440752897,0.06972134874364189,0.2440899925974922,0.3250309809354813,-0.14956587449164147,-0.27115805935594134,-0.03418117958515663,-0.19002782255033146,0.23405245227482852,-0.23897841036456452,0.10994919580125397,0.1563156392488927,0.3322959176594697,-0.22937712215268138,0.11466749116260108,0.11471231575429039,-0.05344857964220225,0.34345725447021525,0.07204358737145028,0.1836031423554929,-0.24648913524996477,-0.297710926415344,-0.07202773087488691,-0.28566299269359974,0.35251276429436196,-0.22211704573221375,-0.004660550157818001,0.1161893066531029,-0.3084485653226971,0.304861533127178,-0.034658773171490714,0.35805770092774736,-0.05748205045356809,0.05188421276936833,0.07495890665581587,-0.01480422668002103,0.2624206920411025,0.29360814044720496,0.20937839627060922,-0.10669003697638768,0.06659935990914546,0.2679509747604231,0.27006124998803716,-0.19369452229156764,0.10504279656528405,0.06571922310026126,0.027487720289289336,-0.010087738945123803,0.15554510971335506,0.29133519653953255,0.25324122047038744,-0.06998695858060873,-0.10611857932722828,0.17355142644496502,-0.2534238845194823,-0.12689292790133253,0.007021712452790707,0.12023513840830455,-0.1730038034684182,-0.10338570868128051,0.06289853280932711,-0.21547065850230376,0.04098344581368267,0.19442014644646424,0.08243550047123724,-0.230888344028444,0.2102638280324044,0.09700569752891788,0.22654325400755077,0.009193858734785565,0.004000043308911853,0.20724349384451035,0.036680397000847284,0.34332359172424876,0.2500762519903109,0.18613775456718828,0.08149829216196186,-0.057690740758918835,0.21448281677733902,-0.14123798268678361,-0.26195952931759525,0.03776198340181576,0.13953758174456382,-0.08156417459003029,-0.019855272906383323,0.06718547149722214,0.10464661084495286,0.1572205496919441,-0.24801226470836835,0.2931783169197777,0.08047524454092234,-0.01873524971071952,-0.03206627557987454,0.12273619216425244,-0.18960098468239706,0.4185880228586881,-3.0301110787278596e-05,0.13628149435463416,0.18723263097992526,0.23330551806939845,0.036861622715196385,0.4740047330993725,-0.04289694973365471,-0.1585070107820478,0.05479642448877181,-0.033646454777879475,0.13469642505206664,0.02969942219128003,-0.04484588431132963,0.1484975125404588,-0.028328376522962474,0.10799533803769891,-0.22908971656582786,-0.3354840392973181,-0.3064723332189253,0.17341614860102422,-0.25551325673825587,0.22311448620847565,-0.1164574577101413,0.2627744287419966,0.030147674822415316,-0.00969558981355094,0.05500769737505737,-0.24378471266635643,-0.11647912967568903,-0.03790226901532077,0.046544051555949734,0.13971583075800217,-0.20137562287889818,-0.06004330672139947,-0.13787953645564804,0.13657428248285935,0.3046719888490164,0.20117514296215436,0.32040545970238676,-0.17420793161469317,0.21773510398899082,0.04431005015367549,0.12317334027425134,0.19916035221248887,-0.06974030806384068,-0.2778152939397447,-0.38260312929632745,-0.10022484188036675,-0.05100717164488342,0.07604735346256401,0.09340874656767699,0.3785935261280443,0.0630231334889858,-0.04472282322060467,-0.11791677595538669,0.02200984610563402,0.13907195531711042,0.24200927959599597,-0.32296944562032437,-0.06496702558523001,-0.1810469011795514,0.2757555178152866,-0.2740475354048786,-0.0329480403429058,0.2255958103058021,-0.3800033061555175,-0.23573714440308174,-0.1345893483120863,-0.354189552421025,-0.019229148079205047,-0.15088079518191477,-0.04460768711507085,-0.21254341284102846,0.13698399458530366,-0.32516591212197216,0.1628150550174206,0.275552238182429,-0.14600581357077186,0.27063216379765653,0.06505384049281077,0.1347266283859915,-0.1451463135445761,-0.06377319034379604,0.1315082614232588,0.018375629650856165,-0.016354738589118803,-0.15499836592088248,0.1823850718223964,0.011230219287285209,-0.12999521145774823,0.10333832878567069,-0.04054740490696264,0.07964308167673909,0.04740969809413066,0.23975917001973382,-0.18284935877511488,0.070975953230752,0.03600401536775928,-0.15134487041469508,-0.256832443503967,-0.07775483624050814,0.019299764421273374,-0.12616379402454003,0.009475849194466882,0.4256631450946521,-0.09410109530093638,-0.3392319689004539,0.18782324114847349,0.13122067697280856,0.18291800442734557,0.22700627084093386,-0.10683258857317415,-0.3132820340576138,0.004308675869347661,0.1551355680270834,0.15654427399188078,0.1940213524844934,0.16219239180312273,0.07421134130891859,-0.010919555259732368,0.2025550216318261,-0.044309834958284854,0.012725382620664083,-0.011945196135599426,0.30852339214528957,-0.1367215254099949,-0.030204234661117754,0.23169995891563,-0.07724434331511207,0.26064989805898703,0.20267799517854793,-0.047770648655267395,-0.1469888698504822,-0.0734754555631717,-0.11468942352870243,0.054187212874720544,-0.06153967326843812,0.2536258972489585,0.02852927563697029,0.4041856174969919,-0.08702293619438602,0.04564042108364242,-0.36649459134018814,0.06291069441703546,0.09821566134972629,-0.0744920290234342,0.08463059091302375,0.04307344528811295,-0.09792762175534851,0.3344979532313217,-0.047271758415324286,0.22547044881710993,0.14406609451146649,0.42103277046525023,-0.1221660519686451,0.01977318835454875,-0.36335918862533706,-0.13608692805051203,0.18203641480010432,-0.33225436693250493,0.02705818861561199,0.22617066372476857,0.0030918211725916786,-0.04278132995442498,0.3054488149515743,0.14228362543052403,0.1848047384744902,-0.008589043456912512,0.08970151341719404,-0.09263573708976439,0.06304751324049096,-0.2216547330072052,0.03941491378141879,-0.1986238010421516,0.10633319542756037,-0.27416539374415566,0.01278115580549724,0.1727876822957154,0.1048373950973441,0.18131355984261072,-0.08213063648456878,0.22205508028908338,-0.3581391606328834,0.17907120437646717,-0.07564283014819202,0.07486933689874906,-0.02528529808286703,0.004664517170848113,-0.3690428590262971,0.3276377517128082,0.030514118101915597,0.2721294095881031,-0.0977140607641595,0.14127075528218414,0.10472632558748511,0.23134134705137843,0.26202041723564906,-0.2096222807531752,-0.15437605437538898,0.019497288722414102,0.13719443338701665,0.09844195762520679,0.09237806041729668,0.05309116575479096,0.019817953780194977,-0.03497661821133442,-0.1701771842748302,-0.014632483417327067,0.2731037188862757,-0.089812601102591,-0.4047480210716348,0.05494082821167002,0.2594580935828873,-0.19331418819137852,0.13038509613838192,0.25475463874033755],"type":"scatter3d","uid":"cd17a7c2-d256-44f9-8fbe-e6feab17832c"},{"colorscale":[[0,"#165aa7"],[1,"#fec630"]],"opacity":1.0,"x":[0.0,0.02040816326530612,0.04081632653061224,0.061224489795918366,0.08163265306122448,0.1020408163265306,0.12244897959183673,0.14285714285714285,0.16326530612244897,0.18367346938775508,0.2040816326530612,0.22448979591836732,0.24489795918367346,0.26530612244897955,0.2857142857142857,0.3061224489795918,0.32653061224489793,0.3469387755102041,0.36734693877551017,0.3877551020408163,0.4081632653061224,0.42857142857142855,0.44897959183673464,0.4693877551020408,0.4897959183673469,0.5102040816326531,0.5306122448979591,0.5510204081632653,0.5714285714285714,0.5918367346938775,0.6122448979591836,0.6326530612244897,0.6530612244897959,0.673469387755102,0.6938775510204082,0.7142857142857142,0.7346938775510203,0.7551020408163265,0.7755102040816326,0.7959183673469387,0.8163265306122448,0.836734693877551,0.8571428571428571,0.8775510204081632,0.8979591836734693,0.9183673469387754,0.9387755102040816,0.9591836734693877,0.9795918367346939,1.0],"y":[0.0,0.02040816326530612,0.04081632653061224,0.061224489795918366,0.08163265306122448,0.1020408163265306,0.12244897959183673,0.14285714285714285,0.16326530612244897,0.18367346938775508,0.2040816326530612,0.22448979591836732,0.24489795918367346,0.26530612244897955,0.2857142857142857,0.3061224489795918,0.32653061224489793,0.3469387755102041,0.36734693877551017,0.3877551020408163,0.4081632653061224,0.42857142857142855,0.44897959183673464,0.4693877551020408,0.4897959183673469,0.5102040816326531,0.5306122448979591,0.5510204081632653,0.5714285714285714,0.5918367346938775,0.6122448979591836,0.6326530612244897,0.6530612244897959,0.673469387755102,0.6938775510204082,0.7142857142857142,0.7346938775510203,0.7551020408163265,0.7755102040816326,0.7959183673469387,0.8163265306122448,0.836734693877551,0.8571428571428571,0.8775510204081632,0.8979591836734693,0.9183673469387754,0.9387755102040816,0.9591836734693877,0.9795918367346939,1.0],"z":[[0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.01101634269372725,0.01101634269372725,0.01101634269372725,0.01101634269372725,-0.007218794058499694,-0.007218794058499694,-0.007218794058499694,-0.007218794058499694,-0.007218794058499694,-0.007218794058499694,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243],[0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.01101634269372725,0.01101634269372725,0.01101634269372725,0.01101634269372725,-0.007218794058499694,-0.007218794058499694,-0.007218794058499694,-0.007218794058499694,-0.007218794058499694,-0.007218794058499694,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243],[0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.01101634269372725,0.01101634269372725,0.01101634269372725,0.01101634269372725,-0.007218794058499694,-0.007218794058499694,-0.007218794058499694,-0.007218794058499694,-0.007218794058499694,-0.007218794058499694,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243],[0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.01101634269372725,0.01101634269372725,0.01101634269372725,0.01101634269372725,-0.007218794058499694,-0.007218794058499694,-0.007218794058499694,-0.007218794058499694,-0.007218794058499694,-0.007218794058499694,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243],[0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.01101634269372725,0.01101634269372725,0.01101634269372725,0.01101634269372725,-0.007218794058499694,-0.007218794058499694,-0.007218794058499694,-0.007218794058499694,-0.007218794058499694,-0.007218794058499694,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243],[0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.01101634269372725,0.01101634269372725,0.01101634269372725,0.01101634269372725,-0.007218794058499694,-0.007218794058499694,-0.007218794058499694,-0.007218794058499694,-0.007218794058499694,-0.007218794058499694,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243],[0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.01101634269372725,0.01101634269372725,0.01101634269372725,0.01101634269372725,-0.007218794058499694,-0.007218794058499694,-0.007218794058499694,-0.007218794058499694,-0.007218794058499694,-0.007218794058499694,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243],[0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.01101634269372725,0.01101634269372725,0.01101634269372725,0.01101634269372725,-0.007218794058499694,-0.007218794058499694,-0.007218794058499694,-0.007218794058499694,-0.007218794058499694,-0.007218794058499694,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243],[0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.01101634269372725,0.01101634269372725,0.01101634269372725,0.01101634269372725,-0.007218794058499694,-0.007218794058499694,-0.007218794058499694,-0.007218794058499694,-0.007218794058499694,-0.007218794058499694,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243],[0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.01101634269372725,0.01101634269372725,0.01101634269372725,0.01101634269372725,-0.007218794058499694,-0.007218794058499694,-0.007218794058499694,-0.007218794058499694,-0.007218794058499694,-0.007218794058499694,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243],[0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.01101634269372725,0.01101634269372725,0.01101634269372725,0.01101634269372725,-0.007218794058499694,-0.007218794058499694,-0.007218794058499694,-0.007218794058499694,-0.007218794058499694,-0.007218794058499694,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243],[0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.01101634269372725,0.01101634269372725,0.01101634269372725,0.01101634269372725,-0.007218794058499694,-0.007218794058499694,-0.007218794058499694,-0.007218794058499694,-0.007218794058499694,-0.007218794058499694,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243],[0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.01101634269372725,0.01101634269372725,0.01101634269372725,0.01101634269372725,-0.007218794058499694,-0.007218794058499694,-0.007218794058499694,-0.007218794058499694,-0.007218794058499694,-0.007218794058499694,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243],[0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07280781666807863,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.01101634269372725,0.01101634269372725,0.01101634269372725,0.01101634269372725,-0.007218794058499694,-0.007218794058499694,-0.007218794058499694,-0.007218794058499694,-0.007218794058499694,-0.007218794058499694,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243,-0.09991096685718243],[0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.01101634269372725,0.01101634269372725,0.01101634269372725,0.01101634269372725,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303],[0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.01101634269372725,0.01101634269372725,0.01101634269372725,0.01101634269372725,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303],[0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.01101634269372725,0.01101634269372725,0.01101634269372725,0.01101634269372725,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303],[0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.01101634269372725,0.01101634269372725,0.01101634269372725,0.01101634269372725,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303],[0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.01101634269372725,0.01101634269372725,0.01101634269372725,0.01101634269372725,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303],[0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.01101634269372725,0.01101634269372725,0.01101634269372725,0.01101634269372725,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303],[0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.01101634269372725,0.01101634269372725,0.01101634269372725,0.01101634269372725,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303],[0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.01101634269372725,0.01101634269372725,0.01101634269372725,0.01101634269372725,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303],[0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.01101634269372725,0.01101634269372725,0.01101634269372725,0.01101634269372725,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303],[0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.01101634269372725,0.01101634269372725,0.01101634269372725,0.01101634269372725,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303],[0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.01101634269372725,0.01101634269372725,0.01101634269372725,0.01101634269372725,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303],[0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.01101634269372725,0.01101634269372725,0.01101634269372725,0.01101634269372725,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303],[0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.01101634269372725,0.01101634269372725,0.01101634269372725,0.01101634269372725,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303],[0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.01101634269372725,0.01101634269372725,0.01101634269372725,0.01101634269372725,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303],[0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.01101634269372725,0.01101634269372725,0.01101634269372725,0.01101634269372725,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303],[0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.01101634269372725,0.01101634269372725,0.01101634269372725,0.01101634269372725,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303],[0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.01101634269372725,0.01101634269372725,0.01101634269372725,0.01101634269372725,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303],[0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.01101634269372725,0.01101634269372725,0.01101634269372725,0.01101634269372725,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303],[0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.01101634269372725,0.01101634269372725,0.01101634269372725,0.01101634269372725,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303],[0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.01101634269372725,0.01101634269372725,0.01101634269372725,0.01101634269372725,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303],[0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.01101634269372725,0.01101634269372725,0.01101634269372725,0.01101634269372725,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303],[0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.01101634269372725,0.01101634269372725,0.01101634269372725,0.01101634269372725,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303],[0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.01101634269372725,0.01101634269372725,0.01101634269372725,0.01101634269372725,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303],[0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.01101634269372725,0.01101634269372725,0.01101634269372725,0.01101634269372725,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303],[0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.01101634269372725,0.01101634269372725,0.01101634269372725,0.01101634269372725,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303],[0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.01101634269372725,0.01101634269372725,0.01101634269372725,0.01101634269372725,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303],[0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.01101634269372725,0.01101634269372725,0.01101634269372725,0.01101634269372725,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303],[0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.01101634269372725,0.01101634269372725,0.01101634269372725,0.01101634269372725,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303],[0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.01101634269372725,0.01101634269372725,0.01101634269372725,0.01101634269372725,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303],[0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.01101634269372725,0.01101634269372725,0.01101634269372725,0.01101634269372725,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303],[0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.01101634269372725,0.01101634269372725,0.01101634269372725,0.01101634269372725,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303],[0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.01101634269372725,0.01101634269372725,0.01101634269372725,0.01101634269372725,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303],[0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.01101634269372725,0.01101634269372725,0.01101634269372725,0.01101634269372725,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303],[0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.01101634269372725,0.01101634269372725,0.01101634269372725,0.01101634269372725,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303],[0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.01101634269372725,0.01101634269372725,0.01101634269372725,0.01101634269372725,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303],[0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.1775896003687168,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.07033956372616093,0.01101634269372725,0.01101634269372725,0.01101634269372725,0.01101634269372725,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.10634026412132509,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303,-0.22877256993857303]],"type":"surface","uid":"7d7ebb62-568d-49b7-9060-a2d787b28784"},{"colorscale":[[0,"#165aa7"],[1,"#fec630"]],"opacity":0.8,"x":[0.0,0.02040816326530612,0.04081632653061224,0.061224489795918366,0.08163265306122448,0.1020408163265306,0.12244897959183673,0.14285714285714285,0.16326530612244897,0.18367346938775508,0.2040816326530612,0.22448979591836732,0.24489795918367346,0.26530612244897955,0.2857142857142857,0.3061224489795918,0.32653061224489793,0.3469387755102041,0.36734693877551017,0.3877551020408163,0.4081632653061224,0.42857142857142855,0.44897959183673464,0.4693877551020408,0.4897959183673469,0.5102040816326531,0.5306122448979591,0.5510204081632653,0.5714285714285714,0.5918367346938775,0.6122448979591836,0.6326530612244897,0.6530612244897959,0.673469387755102,0.6938775510204082,0.7142857142857142,0.7346938775510203,0.7551020408163265,0.7755102040816326,0.7959183673469387,0.8163265306122448,0.836734693877551,0.8571428571428571,0.8775510204081632,0.8979591836734693,0.9183673469387754,0.9387755102040816,0.9591836734693877,0.9795918367346939,1.0],"y":[0.0,0.02040816326530612,0.04081632653061224,0.061224489795918366,0.08163265306122448,0.1020408163265306,0.12244897959183673,0.14285714285714285,0.16326530612244897,0.18367346938775508,0.2040816326530612,0.22448979591836732,0.24489795918367346,0.26530612244897955,0.2857142857142857,0.3061224489795918,0.32653061224489793,0.3469387755102041,0.36734693877551017,0.3877551020408163,0.4081632653061224,0.42857142857142855,0.44897959183673464,0.4693877551020408,0.4897959183673469,0.5102040816326531,0.5306122448979591,0.5510204081632653,0.5714285714285714,0.5918367346938775,0.6122448979591836,0.6326530612244897,0.6530612244897959,0.673469387755102,0.6938775510204082,0.7142857142857142,0.7346938775510203,0.7551020408163265,0.7755102040816326,0.7959183673469387,0.8163265306122448,0.836734693877551,0.8571428571428571,0.8775510204081632,0.8979591836734693,0.9183673469387754,0.9387755102040816,0.9591836734693877,0.9795918367346939,1.0],"z":[[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.0,-0.0,-0.0,-0.0,-0.0,-0.0,-0.0,-0.0,-0.0,-0.0,-0.0,-0.0,-0.0,-0.0,-0.0,-0.0,-0.0,-0.0,-0.0],[0.0,0.003853979053497032,0.007667864019745306,0.0114019779221783,0.015017473666267794,0.01847673817737098,0.021743783700702617,0.02478462219262264,0.027567618908335596,0.030063821507524142,0.032247261254133075,0.03409522317683338,0.03558848237960649,0.036711504044040076,0.03745260504265187,0.03780407548193024,0.037762258910643304,0.037327590358985675,0.03650459181283028,0.035301825170168,0.03373180316914337,0.03181085921432805,0.029558977455468392,0.02699958488644794,0.024159307627321228,0.02106769392488915,0.017756906753522882,0.01426138921420151,0.010617506212716076,0.006863166144768219,0.003037426523685465,-0.0008199123464751883,-0.004668721425295604,-0.008468960410307121,-0.012181094288133614,-0.015766504628962518,-0.019187891345524975,-0.022409660736967544,-0.025398295780679883,-0.02812270481982264,-0.03055454501905593,-0.032668517223462305,-0.034442629153159446,-0.03585842419551121,-0.0369011734147433,-0.03756002878143103,-0.037828136027767134,-0.037702705954542816,-0.037185043448014536,-0.03628053390478513],[0.0,0.005555777676905859,0.011053756950701448,0.016436740712208034,0.021648728184670826,0.026635497516450363,0.031345169867028205,0.03572874911797297,0.03974063159409351,0.0433390804919987,0.04648666008044295,0.049150625155342464,0.05130326169784605,0.052922175191493705,0.05399052359901397,0.05449719257503324,0.05443691109190742,0.05381030627578596,0.052623896882432554,0.05089002548067528,0.04862673004900217,0.04585755632112051,0.042611312832705125,0.038921771217656616,0.034827314871776614,0.030370539638916186,0.025597810673777522,0.02055878009145691,0.015305870421763745,0.009893729242093474,0.004378660662470689,-0.0011819604228084737,-0.006730285223219727,-0.01220859287013767,-0.017559890904152865,-0.022728508184532176,-0.0276606740526087,-0.0323050777238932,-0.03661340208938482,-0.04054082637278758,-0.04404649241434098,-0.04709392973037502,-0.04965143492656961,-0.051692401517769576,-0.05319559672314153,-0.05414538235709091,-0.05453187751794529,-0.054351061381905326,-0.05360481503286736,-0.05230090189294968],[0.0,0.006930255358669589,0.013788413215212243,0.02050312611920099,0.02700453892059846,0.03322501549360297,0.03909984237532723,0.04456790199914784,0.04957230851887945,0.054060999609105855,0.05798727808499391,0.06131029770695849,0.06399548811620859,0.066014914480444,0.06734756810820161,0.06797958500850435,0.06790439012207147,0.06712276572361059,0.06564284328358191,0.063480018874099,0.06065679299902394,0.05720253651455157,0.05315318507548063,0.04855086528593775,0.04344345644381925,0.03788409243825336,0.031930608981991175,0.025644941929331845,0.01909248293905911,0.012341399185618662,0.005461924195783009,-0.0014743728115603279,-0.008395331481221114,-0.015228951027285786,-0.021904139278649697,-0.028351452271960443,-0.03450381669776678,-0.04029722768405192,-0.045671414657902094,-0.05057046835815372,-0.05494342247602116,-0.05874478387272422,-0.061935005858105005,-0.06448089960657598,-0.06635597943031671,-0.06754073831774055,-0.06802285087072209,-0.06779730152936998,-0.06686643675038405,-0.06523994059617026],[0.0,0.008143584248041682,0.016202448373626464,0.024092753622860792,0.031732414809839175,0.03904195427318104,0.04594532870571651,0.0523707202555665,0.058251283668558404,0.06352584169923249,0.06813952155587275,0.07204432575843522,0.07519963147056985,0.07757261311103635,0.07913858384796256,0.07988125242327888,0.07979289263550829,0.07887442371773193,0.07713540077453483,0.07459391537741862,0.07127640735281698,0.06721738972073868,0.06245908964558336,0.057051009134424985,0.05104941005294904,0.044516728816575764,0.0375209268459106,0.03013478354392732,0.022435139150251207,0.0145020953493541,0.006418181948971511,-0.0017325017019673794,-0.009865161623811423,-0.017895191343109566,-0.025739052078849554,-0.03331514182117604,-0.040544644261303546,-0.04735234874095498,-0.05366743269114852,-0.05942419842038975,-0.06456275658724359,-0.0690296492469172,-0.07277840599011615,-0.07577002738849647,-0.07797339071728546,-0.07936557373421893,-0.07993209314642677,-0.07966705528442455,-0.07857321741570539,-0.07666195906006834],[0.0,0.00925775805990823,0.01841920491671196,0.027389031321320118,0.03607392150904366,0.044383523991174706,0.052231391508323344,0.05953588036689867,0.06622099980267031,0.07221720253522868,0.07746210828897201,0.08190115275361662,0.08548815523290124,0.08818579907606844,0.08996601989405104,0.09081029752163605,0.09070984868823673,0.08966571839286169,0.08768876903268043,0.08479956739828344,0.08102817071125931,0.07641381393000307,0.07100450157680624,0.06485650833257244,0.05803379359461995,0.050607336088100086,0.0426543954532658,0.034257708490517624,0.025504628424928125,0.016486216144730847,0.0072962928678900795,-0.0019695359078590255,-0.011214875017361044,-0.020343542455536302,-0.029260569987308558,-0.03787319113032398,-0.04609180623218301,-0.053830914602201047,-0.06101000400045468,-0.06755438823049471,-0.07339598412200135,-0.0784740198201993,-0.08273566701348906,-0.08613659052204238,-0.08864140952982849,-0.0902240656617378,-0.09086809407658754,-0.09056679475574833,-0.08932330220542531,-0.08715055284746039],[0.0,0.010303147452196976,0.020499108205361338,0.030481810655483887,0.040147401787937964,0.04939532759119522,0.058129379150033536,0.06625869353541915,0.07369869907850912,0.08037199519480745,0.08620915760540404,0.09114946057834061,0.09514150867641244,0.09814377143914643,0.10012501543650006,0.1010646291994985,0.10095283764745994,0.0997908037810595,0.09759061658328823,0.09437516525417716,0.09017790108766018,0.0850424894678409,0.07902235560505017,0.07218012873753579,0.0645869905809205,0.05632193480370054,0.04747094523268132,0.03812610133772303,0.02838462030170333,0.01834784564132756,0.00812019290046448,-0.0021919366157284663,-0.012481262765142553,-0.022640742635857736,-0.03256468414530568,-0.042149845586355136,-0.05129650968141422,-0.05990952097084708,-0.06789927574344527,-0.07518265421041329,-0.08168388522518635,-0.08733533455314302,-0.09207820849060912,-0.09586316551319687,-0.09865082959031814,-0.10041219982571042,-0.1011289521623628,-0.10079363001311621,-0.09940972183375152,-0.09699162483154913],[0.0,0.011296788919914045,0.022476053993875984,0.033421494011362705,0.044019240313270064,0.05415904140134892,0.06373540991724388,0.07264872005877153,0.0808062440166914,0.08812311664960994,0.09452321836121899,0.09993996699503456,0.10431701050831994,0.10760881321910148,0.10978112952737468,0.11081136018223887,0.11068878738860774,0.10941468630761059,0.1070023117907129,0.10347676048556495,0.0988747097481322,0.0932440360772821,0.08664331704136533,0.07914122187839719,0.07081579710960603,0.06175365459832316,0.05204907050107783,0.041803004484777063,0.03112204941334456,0.020117322430539902,0.008903308975353106,-0.0024033282439760843,-0.013684962926715425,-0.024824228880937237,-0.03570524101882539,-0.04621480094361764,-0.05624357458610356,-0.06568722963937684,-0.07444752095877086,-0.0824333126352396,-0.08956152610925229,-0.09575800446169039,-0.1009582838902733,-0.1051082643456149,-0.10816477235008405,-0.11009601014429839,-0.11088188648864697,-0.11051422567841536,-0.1089968525980668,-0.10634555292983461],[0.0,0.012249008336796674,0.024370586606603063,0.03623863043521369,0.04772967304241743,0.05872416970353437,0.0691077414086457,0.07877236478136046,0.08761749587804198,0.09555111617627632,0.10249068987085522,0.10836402251823937,0.11311001209676844,0.11667928466911634,0.11903470803400648,0.12015177802351562,0.12001887342720105,0.11863737689099997,0.11602166053315371,0.11219893642679851,0.1072089735046973,0.10110368383123589,0.09394658254581925,0.08581212709603664,0.07678494263474883,0.06695894163952194,0.05643634691326702,0.045326628130100445,0.03374536298985359,0.021813034828979082,0.009653779196664556,-0.002605907564106806,-0.01483848429553255,-0.02691669187348706,-0.03871487712193657,-0.05011030002020067,-0.060984410603793955,-0.07122408227487059,-0.08072278869177701,-0.08938171199418138,-0.09711077083458611,-0.10382955752130521,-0.1094681745235311,-0.11396796163608072,-0.11728210623890944,-0.11937613030268279,-0.12022824907394247,-0.11982959770835823,-0.11818432349433301,-0.11530954270753138],[0.0,0.013166349305062861,0.02619572517294376,0.03895257914175635,0.0513041978757649,0.06312208382158094,0.0742832920063258,0.08467170907065076,0.09417926123045595,0.1027070386005224,0.11016632418338668,0.11647951681858024,0.12158093849051968,0.1254175175963841,0.12794934106574096,0.12915006958805714,0.12900721162835851,0.12752225338037398,0.12471064330522097,0.12060163141648228,0.11523796498363875,0.10867544381954522,0.10098233977842633,0.09223868650352626,0.08253544681337896,0.07197356638862566,0.06066292360415709,0.048721186431787564,0.03627258830544203,0.02344663568389309,0.010376760756656882,-0.002801066690653297,-0.0159497538103369,-0.028932510910165472,-0.04161427451712453,-0.05386311248278583,-0.06555159651258055,-0.0765581278401451,-0.08676820225535593,-0.09607560132559112,-0.10438349741759147,-0.11160546102404974,-0.11766635991541154,-0.12250314076274521,-0.12606548510022508,-0.128316332803057,-0.12923226763494883,-0.12880376085416279,-0.1270352703438417,-0.12394519423532857],[0.0,0.014053048273286524,0.027959898517014768,0.04157587364309276,0.05475932262355588,0.06737309412822692,0.07928596335068533,0.09037399717885172,0.10052184350791384,0.10962393128248793,0.1175855687836222,0.12432392873483636,0.12976890997884666,0.13386386676069828,0.13656619803035636,0.1378477906340656,0.1376953117838276,0.1361103477623492,0.1331093874204729,0.12872365063877073,0.12299876353786632,0.11599428381636841,0.10778308115446673,0.09845057912903428,0.08809386652681922,0.07682068730099817,0.0647483196788726,0.052002356081683354,0.03871539654939994,0.02502566926317308,0.011075592516565709,-0.0029897069041992714,-0.01702390351724228,-0.0308809954124184,-0.044416823152494664,-0.05749056950653621,-0.06996622441332812,-0.08171399993438525,-0.0926116804763736,-0.1025458942361788,-0.11141329264140228,-0.11912162551621447,-0.12559070078729692,-0.1307532187457678,-0.13455547218601518,-0.13695790513768763,-0.13793552437818668,-0.1374781594445751,-0.1355905684399194,-0.13229238853333236],[0.0,0.01491185044441656,0.02966856848555336,0.04411663560853685,0.058105744285546074,0.07149036166960838,0.08413124361597404,0.09589688328025603,0.10666487922313957,0.1163232087888615,0.12477139351012606,0.13192154441540382,0.13769927636397175,0.14204448189659602,0.14491195655125952,0.1462718691385884,0.14611007208456364,0.14442824861193,0.1412438952291305,0.13659013970893938,0.1305153964504156,0.12308286280954894,0.11436986163842028,0.10446703687263603,0.09347741053557959,0.08151531096974403,0.06870518344507333,0.05518029551790664,0.04108135060906028,0.026555024224405914,0.011752438046076844,-0.0031724122312171456,-0.018064258963078714,-0.03276817785078338,-0.047131199664711715,-0.06100390162861885,-0.07424196190909674,-0.08670766103841544,-0.098271314650996,-0.1088126226283318,-0.11822192061680489,-0.12640132089848466,-0.1332657307460924,-0.13874373766788384,-0.1427783523329822,-0.1453276014482917,-0.14636496441911545,-0.14587964925076863,-0.14387670482090018,-0.14037696835452115],[0.0,0.015744494340896222,0.03132519403708137,0.04658000844811758,0.061350237214851396,0.07548222126632338,0.08882894138066118,0.1012515476696528,0.11262080407531924,0.12281843285095907,0.13173834503963044,0.1392877441490382,0.14538809154096793,0.14997592349203542,0.1530035114256253,0.1544393584464308,0.15426852701199806,0.15249279433241086,0.14913063388144737,0.14421702321155236,0.13780308007198466,0.12995553061570897,0.12075601522743451,0.11030023919447703,0.09869697705626697,0.08606694099055158,0.07254152500880433,0.05826143802534993,0.04337524002079206,0.028037796528469636,0.012408667522378045,-0.0033495525325691776,-0.019072926198986206,-0.03459787855680345,-0.04976289892167955,-0.06441022108855779,-0.07838746462029318,-0.09154922010653878,-0.10375856190097243,-0.11488847259940965,-0.1248231644395354,-0.13345928387533962,-0.1407069867947127,-0.14649087319439352,-0.15075077158856787,-0.15344236499068342,-0.1545376519562059,-0.15402523788995237,-0.15191045358744476,-0.14821529977706271],[0.0,0.01655201633623934,0.03293183713692541,0.04896905826773738,0.06449683976034098,0.07935364149806659,0.09338490376565863,0.106444655179838,0.11839703127290047,0.12911768793107337,0.13849509498321402,0.1464316964822619,0.15284492560869242,0.1576680636376313,0.1608509340335394,0.162360424451595,0.16218083121523874,0.16031402268617878,0.15677941982726995,0.1516137941604762,0.14487088522182107,0.13662084149305387,0.12694949062618718,0.11595744655297498,0.10375906276833652,0.09048124267703003,0.07626211937989086,0.06124961863421108,0.04559991993820901,0.029475831749383903,0.013045097739840951,-0.0035213482908858524,-0.020051160690851166,-0.03637237491863697,-0.0523151965414839,-0.0677137676571396,-0.08240789236281185,-0.09624470332066215,-0.1090802520820644,-0.12078100662601322,-0.13122524053234677,-0.14030429933780147,-0.14792373090062194,-0.1540042680142083,-0.1584826530473779,-0.1613122960322662,-0.16246375935358148,-0.16192506399684864,-0.15970181416964996,-0.1558171389993911],[0.0,0.017334951170152595,0.03448956170144303,0.0512853670921673,0.06754763559692636,0.08310718601280193,0.09780214772159246,0.11147964467786764,0.12399738582381505,0.13522514538532007,0.14504611764932135,0.1533581321282905,0.1600747164701189,0.16512599605564626,0.16845942092504965,0.17004031247066256,0.1698522242088168,0.1678971128774906,0.16419531807978271,0.15878535068498847,0.15172349218860365,0.1430832091992324,0.13295438914366706,0.12144240514132454,0.10866701977642992,0.0947611391723292,0.07986943032966567,0.0641468161126617,0.04775686354060947,0.030870082151652548,0.013662150141484787,-0.0036879132690292842,-0.020999610223908255,-0.03809284200478919,-0.054789782651314435,-0.07091672893734388,-0.08630590745713954,-0.10079722002186989,-0.11423990920855626,-0.1264941267342609,-0.13743238833925636,-0.1469409000435929,-0.15492074197962508,-0.16128889748473668,-0.16597911674830562,-0.16894260602813485,-0.1701485352662253,-0.1695843588230142,-0.1672559459933845,-0.16318751994664943],[0.0,0.018093469292569275,0.035998706857259616,0.053529439196689906,0.070503288902422,0.08674367198123684,0.10208163491079393,0.1163576123132383,0.1294230869611566,0.1411421348463462,0.15139283923759014,0.16006855901657438,0.16707903809707325,0.17235134438578587,0.17583062851653578,0.17748069446449638,0.17728437610417372,0.17524371579370146,0.17137994312758376,0.16573325407892642,0.15836239282880593,0.1493440406331291,0.13877201808478298,0.12675630907017837,0.11342191657425624,0.09890756223735755,0.08336424319285152,0.06695366119916885,0.04984654040844538,0.03222085127252767,0.014259959063573757,-0.0038492837292631817,-0.021918481281702,-0.0397596543719759,-0.05718719598443357,-0.07401980223437776,-0.09008235852615522,-0.10520776132223726,-0.11923865657088664,-0.13202907670703112,-0.14344595919614012,-0.15337053082323945,-0.16169954332589676,-0.16834634751652133,-0.17324179471956383,-0.17633495614569875,-0.17759365271912608,-0.17700478984604184,-0.1745744936415814,-0.17032804719802658],[0.0,0.018827474215285498,0.03745908063176279,0.05570098912143238,0.07336342369941831,0.0902626368198296,0.1062228209559829,0.1210779375782497,0.13467344450213037,0.14686790363643898,0.1575344524057189,0.16656212353924496,0.1738569994964555,0.17934318951898426,0.18296361914473047,0.1846806239704168,0.18447634148554914,0.18235289690141115,0.17833238104186117,0.17245662052593885,0.1647867426331412,0.1554025393782021,0.1444016374110958,0.13189848237803925,0.11802314830948808,0.10291998442141581,0.08674611340760326,0.06966979684565025,0.05186868472187442,0.03352796728580962,0.01483844846110978,-0.004005439144265377,-0.022807656978141813,-0.04137259999133874,-0.05950713157501434,-0.07702259281809558,-0.0937367651818893,-0.10947576617358935,-0.12407585829742582,-0.13738515246440056,-0.14926518813953343,-0.159592373787877,-0.16825927263391377,-0.17517572035820017,-0.1802697631035252,-0.1834884060322303,-0.18479816464722426,-0.18418541314113882,-0.18165652614964423,-0.17723781243422357],[0.0,0.01953667363348547,0.03887010145625469,0.05779915208404643,0.07612690098850387,0.09366267916145433,0.11022405670115694,0.12563874068468195,0.13974636758229345,0.15240017156681535,0.16346851136206483,0.17283623974609558,0.18040590146190216,0.1860987470733656,0.18985555221899578,0.19163723374053093,0.19142525627662577,0.18922182509171326,0.18504986313398503,0.17895277256116154,0.17099398321497197,0.16125629274169587,0.14984100522369007,0.13686687728297134,0.12246888262085222,0.10679680784649101,0.09001369420236727,0.07229414139787549,0.053822491196759206,0.03479090965506376,0.015397387960655911,-0.0041563173278319305,-0.02366678321432439,-0.042931036535071,-0.06174866554867324,-0.07992390487887635,-0.09726767211984552,-0.11359953491651888,-0.12874958805591655,-0.1425602210414678,-0.15488775776175728,-0.1656039511957284,-0.1745973176044836,-0.1817742963296884,-0.18706022313285148,-0.19040010694954448,-0.19175920197775645,-0.1911233691487783,-0.1884992232201287,-0.18391406396026935],[0.0,0.020220632805723292,0.04023090437058771,0.05982264190411974,0.07879202674951802,0.0969417147695962,0.11408288937637773,0.13003722584606645,0.14463874648404285,0.1577355473400526,0.16919137851005844,0.17888706158440598,0.1867217294961696,0.1926138758711684,0.1965022029629532,0.19834625935144481,0.19812686077105557,0.1958462896902944,0.1915282715665705,0.18521772802322192,0.17698030951654387,0.16690171235461865,0.1550867871732038,0.1416584481434677,0.1267563942594087,0.11053565600778348,0.09316498253996826,0.07482508612448366,0.05570676214366274,0.03600890419269545,0.01593643493056304,-0.004301825791167738,-0.024495333343198776,-0.04443400867673135,-0.06391042383812295,-0.08272195990341627,-0.10067291488310505,-0.11757653966797933,-0.13325698083550178,-0.14755111010512298,-0.1603102214101713,-0.1714015779311726,-0.1807097929963552,-0.1881380304834224,-0.19360901223445565,-0.19706582200351352,-0.19847249757322022,-0.19781440488038052,-0.19509839025847936,-0.19035270921324415],[0.0,0.020878815600716513,0.04154042268972971,0.06176987243991125,0.08135671188511376,0.1000971733248017,0.11779629417988091,0.13426994524623792,0.14934674624546665,0.16286984874473842,0.17469856789757293,0.18470984603002552,0.19279953284617019,0.19888346893452097,0.20289836130335265,0.20480244183646745,0.20457590181929697,0.20222109801483534,0.19776252814553502,0.19124657603623335,0.18274102906946482,0.1723343729732134,0.16013487127763826,0.14626943901745657,0.13088232239719216,0.11413359815512417,0.09619750823749712,0.07726064710787096,0.05752002054956984,0.03718099615608263,0.016455166831072933,-0.004441850475360771,-0.025292657893771393,-0.045880338289884026,-0.06599071191788429,-0.08541456459584397,-0.10394982422291177,-0.12140366299531646,-0.13759450345120566,-0.15235390747484004,-0.16552832860827363,-0.17698070944069969,-0.1865919074573488,-0.19426193451441048,-0.19991099704535056,-0.2034803261770524,-0.2049327891198356,-0.2042532754708814,-0.20144885441223515,-0.1965487011680085],[0.0,0.021510616754203335,0.042797452181900744,0.06363905302010328,0.08381859791332055,0.10312615307043485,0.1213608562682001,0.1383330064805291,0.15386603739453694,0.16779835428226603,0.17998501511857304,0.19029923845600863,0.19863372236982063,0.2049017607517052,0.20903814533916953,0.21099984409642883,0.21076644888943835,0.20834038779776534,0.2037468998545572,0.1970337724773931,0.1882708443216046,0.17754927872802864,0.16498061532373265,0.15069560964218912,0.13484287283467683,0.11758732562439374,0.09910848258725491,0.07959858460851232,0.05926059894380387,0.0383061076905892,0.01695310663683151,-0.004576262613846855,-0.026058023647156557,-0.047268695331215416,-0.06798761675705078,-0.08799924283948804,-0.10709538669660564,-0.12507738547961933,-0.14175816712142528,-0.1569641965028022,-0.17053728078973088,-0.1823362151603686,-0.19223825180069432,-0.20014037688642766,-0.20596038226639815,-0.2096377206983261,-0.2111341357397435,-0.21043405974111254,-0.20754477580070133,-0.2024963419963523],[0.0,0.022115387713885427,0.04400070248958608,0.06542826490593667,0.08617515767534323,0.1060255447184471,0.12477291657087913,0.14222223876247153,0.15819198081870173,0.17251600477606382,0.18504529356458205,0.1956495012765911,0.20421830919387576,0.2106625734660196,0.21491525250032198,0.2169321044153727,0.2166921473024671,0.2141978775066261,0.2094752436563784,0.20257337671248093,0.19356407884396135,0.18254107644885556,0.1696190450906885,0.15493241649457173,0.13863398001409244,0.12089329311834854,0.1018949174353035,0.08183649870242689,0.060926710599462315,0.03938308385424087,0.017429743205996644,-0.004704924230775744,-0.026790644945215342,-0.04859765463373256,-0.06989908850621043,-0.09047334142770892,-0.11010637334241508,-0.12859393599583152,-0.14574369778995183,-0.1613772446659228,-0.17533193619842002,-0.18746259759223396,-0.19764302997872565,-0.20576732330009054,-0.2117509581231039,-0.21553168491985492,-0.21707017166804957,-0.21635041303370528,-0.21337989687938766,-0.20818952636575527],[0.0,0.022692457702307836,0.04514883903617265,0.06713552360563657,0.08842377740950175,0.10879213242899122,0.12802869062523947,0.145933328377443,0.16231977842824805,0.17701756767727841,0.18987379066336407,0.2007547002854081,0.20954709921315098,0.21615951751257337,0.22052316423475166,0.22259264306850374,0.2223464246116797,0.21978707034791845,0.2149412059987736,0.2078592445284344,0.1986148616827061,0.18730422951837272,0.1740450158967643,0.15897516035009288,0.14225143905558396,0.12404783384640233,0.10455372222703566,0.08397190722294263,0.0625165075605245,0.04041073012650298,0.0178845478804711,-0.004827692622020242,-0.0274897092061895,-0.0498657421915752,-0.07172300706992116,-0.09283411623146444,-0.11297944454568623,-0.1319494141866893,-0.14954667493348528,-0.1655881572628805,-0.1799069768760281,-0.19235417084528905,-0.20280024731975302,-0.21113653266737376,-0.217276302038997,-0.22115568159269064,-0.22273431299226804,-0.22199577326703074,-0.2189477456648037,-0.21362193972082935],[0.0,0.023241151135450208,0.046240517681920675,0.06875883040722822,0.09056182462477987,0.1114226773185944,0.13112436685041715,0.14946193070019487,0.16624459775196748,0.18129777294237315,0.19446485362488014,0.20560885875357018,0.21461385393758942,0.2213861575410049,0.22585531528065084,0.2279748331829341,0.22772266127443241,0.22510142297430838,0.22013838780210174,0.2128851876848289,0.2034172798147314,0.1918331616457231,0.17825334619516123,0.1628191083111804,0.14569101494854297,0.12704725474298217,0.10708178426197035,0.08600231021701685,0.06402812862913915,0.04138784342730889,0.018316988213971718,-0.004944424060883428,-0.028154398025633482,-0.05107147343665337,-0.07345723716085076,-0.09507880345820759,-0.11571123676121103,-0.13513989174628196,-0.15316264635274388,-0.16959200451857231,-0.18425704675754356,-0.19700520828534657,-0.20770386619628747,-0.21624171917871643,-0.22252994541558596,-0.22650312662415995,-0.22811992862181285,-0.22736353133779244,-0.22424180379740455,-0.2187872222582089],[0.0,0.02376080220422567,0.04727441373526587,0.07029621551782916,0.09258670491931661,0.11391398736674921,0.13405618881479447,0.1528037639672714,0.16996167623902173,0.185351426779356,0.1988129114485434,0.21020608642866692,0.21941242514100026,0.22633615131316365,0.23090523536809393,0.2330721437691004,0.2328143335253457,0.23013448671326647,0.22506048257413658,0.21764510747805066,0.20796550577165865,0.1961223772226454,0.18223892941017808,0.16645959595991472,0.14894853395815494,0.12988791617697965,0.10947603587686734,0.0879252439030882,0.06545973953687402,0.0423132380837942,0.018726539464108353,-0.0050549769002304476,-0.028783904840484587,-0.05221338528089134,-0.07509967438684961,-0.09720467930433069,-0.11829843511046051,-0.1381614972068239,-0.15658722426671307,-0.17338392798614213,-0.1883768672741324,-0.201410066136112,-0.21234793633845286,-0.22107668797308325,-0.2275055132476531,-0.23156753118593298,-0.23322048341044782,-0.2324471737689125,-0.22925564723090466,-0.22367910619365974],[0.0,0.0242507671998128,0.04824924647525914,0.07174577452808259,0.09449591000748286,0.11626297650599365,0.1368205247743517,0.15595468854020547,0.17346640942238295,0.18917350779427491,0.20291257805252808,0.21454068857400577,0.22393686867585919,0.23100336710942318,0.2356666689954507,0.23787826062120734,0.23761513414301505,0.23488002694367408,0.22970139315465823,0.22213310763934474,0.2122539055168307,0.2001665630571346,0.18599682846917448,0.16989211370485136,0.1520199608887885,0.1325662993279137,0.11173351123372906,0.08973832628013487,0.0668095669003464,0.0431857677792485,0.01911269430631575,-0.005159214194633494,-0.029377449860005865,-0.05329006320063728,-0.0766482842070603,-0.09920911037770264,-0.12073783474598933,-0.14101048760653412,-0.15981616653841785,-0.17695923048563017,-0.19226133506900644,-0.20556328795570333,-0.2167267049842565,-0.22563544981606026,-0.23219684213614528,-0.23634262183398605,-0.2380296591336776,-0.23724040328578758,-0.23398306515300818,-0.2282915317901162],[0.0,0.024710435006307513,0.04916380002771237,0.07310569945461615,0.09628705860570935,0.11846671492576077,0.13941392687121268,0.15891077438289977,0.17675442597306412,0.19275924884148501,0.2067587400685616,0.21860725879457346,0.22818154136474306,0.23538198367759924,0.24013367739598862,0.24238718924072633,0.24211907525966184,0.23933212472206727,0.23405533110104187,0.22634359044581115,0.21627713028184395,0.20396067498010784,0.18952235627837546,0.17311238028874987,0.15490146485892684,0.13507906354378782,0.11385139466387297,0.09143929595502477,0.06807592712791813,0.04400434423843081,0.019474971103400174,-0.005257005932624442,-0.029934292776615176,-0.054300164293834476,-0.0781011350955598,-0.10108959662277128,-0.12302639309143573,-0.14368330950106556,-0.16284544582306146,-0.18031345266945245,-0.19590560518371014,-0.20945969357833316,-0.22083471065138066,-0.2299123187262198,-0.23659808075347633,-0.24082244276767084,-0.2425414574777525,-0.24173724146380365,-0.23841816122356146,-0.23261874613326033],[0.0,0.025139236073062635,0.05001694121653552,0.07437430528466393,0.09795793139424187,0.1205224720875499,0.141833181754732,0.16166835876200425,0.17982165187951185,0.19610420701438025,0.2103466319158607,0.2224007584131596,0.23214118385286822,0.23946657569996257,0.2443007257302226,0.24659334284699302,0.24632057627437912,0.24348526368394297,0.23811690167356442,0.2302713389055843,0.2200301950966041,0.20750001190336478,0.19281114453828943,0.1761164056455429,0.1575894755457676,0.13742309538819178,0.11582706200728338,0.09302604534405186,0.0692572511380112,0.044767953205512225,0.019812920976886073,-0.005348230945509563,-0.030453743635003395,-0.05524243699636809,-0.07945642689982035,-0.10284380802611667,-0.1251612745123082,-0.14617665113480194,-0.16567130869707877,-0.18344243849407366,-0.19930516219129174,-0.21309445516895933,-0.2246668633220924,-0.23390199545610263,-0.24070377575210372,-0.24500144327215373,-0.2467502881062807,-0.2459321165029384,-0.24255544014399982,-0.23665538759520624],[0.0,0.02553665009968939,0.050807634853790334,0.07555005271210535,0.0995065009626357,0.12242775356834426,0.14407535393954357,0.1642240956685283,0.18266436541863512,0.1992043235939584,0.21367190010356954,0.22591658445771173,0.23581099157306878,0.24325218699757148,0.2481627577680346,0.2504916177601748,0.25021453915271485,0.24733440447659416,0.24188117662714503,0.23391158715120466,0.22350854605200604,0.21078027925182163,0.1958592026855926,0.1789005447386804,0.16008073135993095,0.13959555065019771,0.11765811602065435,0.09449664911076525,0.07035210553087598,0.04547566812952714,0.020126133863776423,-0.005432778542310338,-0.030935172141767532,-0.0561157379695399,-0.08071251513034014,-0.10446961605554535,-0.12713988857747355,-0.14848748712669368,-0.16829032630344035,-0.18634239129790126,-0.20245588112339213,-0.21646316236572555,-0.22821851312334107,-0.23759963899547204,-0.24450894534707046,-0.2488745526147437,-0.2506510440911734,-0.2498199384085929,-0.24638988180593807,-0.24039655817945896],[0.0,0.025902212609389366,0.05153495681796986,0.07663156758463512,0.10093095742350118,0.12418033257446527,0.14613782289167493,0.16657499809039789,0.1852792441750309,0.2020559753255604,0.21673065823156584,0.22915062781401763,0.23918667544537667,0.24673439314527293,0.251715259767694,0.25407745787975766,0.25379641283383647,0.2508750484249056,0.24534375647343726,0.23726008065027715,0.22670811783276823,0.21379764321999123,0.19866296830880764,0.18146154361151848,0.16237232065148235,0.14159389027733843,0.11934241666350194,0.0958493884901402,0.07135921069776932,0.04612666187016329,0.020414243697175802,-0.005510549908202816,-0.03137801562836418,-0.05691904654469482,-0.08186793173667109,-0.1059651205513806,-0.1289599227859511,-0.15061311669208982,-0.1706994376708887,-0.18900992176726317,-0.2053540795839603,-0.21956187799926,-0.23148550907197488,-0.24100092773130544,-0.2480091422545049,-0.2524372441071337,-0.25423916643216915,-0.25339616329718345,-0.24991700471383874,-0.24383788540085471],[0.0,0.02623552054458521,0.052198105187046956,0.07761765745840896,0.102229730216524,0.1257782767681333,0.14801831459856427,0.16871847399749612,0.18766340506668538,0.20465601806883296,0.21951953381058165,0.2320993227289005,0.2422645135482945,0.24990935477350235,0.2549543148536693,0.25734690957266115,0.2570622480579321,0.2541032917276554,0.24850082348221136,0.24031312746054379,0.229625382692975,0.21654877695848745,0.20121935006527908,0.1837965785882865,0.16446171678707294,0.1434159109640028,0.12087810687959473,0.0970827719948317,0.0722774562366179,0.04672021666246115,0.020676932816360318,-0.0055814592949565535,-0.03178178582967285,-0.05765147701938914,-0.0829214027932716,-0.10732867261800819,-0.13061937042600405,-0.15255119617954854,-0.17289598658997798,-0.1914420887680443,-0.20799656211170242,-0.2223871855244221,-0.23446424908233876,-0.24410211151099875,-0.251200507268804,-0.25568558963918214,-0.257510698979883,-0.25665684813707695,-0.2531329199743484,-0.2469755749616307],[0.0,0.026536236988294458,0.052796409632103976,0.07850732556620463,0.10340150650584236,0.12721997090266796,0.14971492820660276,0.1706523567044299,0.18981443811709192,0.20700182362785619,0.22203570776718554,0.2347596885790748,0.2450413947169905,0.2527738625422522,0.2578766488984935,0.26029666798567763,0.2600087436274222,0.2570158711862294,0.25134918640184145,0.24306764147690463,0.23225739177971905,0.21903089954438695,0.20352576389283342,0.18590328935032646,0.166346807746655,0.14505977096072464,0.12226363435031384,0.09819555288629939,0.07310591395915059,0.047255732524636815,0.020913935687821704,-0.0056454350253803106,-0.03214607460348187,-0.05831228903241531,-0.08387186342213539,-0.10855889393886486,-0.13211655408154835,-0.1542997665241219,-0.17487775272763034,-0.1936364337978204,-0.21038065761137761,-0.22493622904135038,-0.23715172216072422,-0.24690005557125705,-0.2540798144689589,-0.25861630569237926,-0.26046233477772107,-0.2595986969368461,-0.25603437685038266,-0.24980645519742753],[0.0,0.026804095092269723,0.05332933923274761,0.07929978243880523,0.10444524648649817,0.1285041356547693,0.15122615818332866,0.1723749301320761,0.1917304345533463,0.20909131039347545,0.22427694731156994,0.23712936462245474,0.24751485481660065,0.2553253745590064,0.2604796686960598,0.2629241155770025,0.2626332848958678,0.2596102022504968,0.2538863176663467,0.24552117841254706,0.23460180951399678,0.22124180840421642,0.20558016313755295,0.18777980645522374,0.16802592074768113,0.14652401154727673,0.12349776959325148,0.09918674371062494,0.07384384871271472,0.047732734253314285,0.021125042001142305,-0.0057024203290117524,-0.03247055868904888,-0.058900896195728254,-0.0847184702082835,-0.10965469284633401,-0.13345014518927167,-0.15585727608824013,-0.1766429775141643,-0.19559100964972032,-0.2125042504962961,-0.227206746592612,-0.23954554351076945,-0.24939227708653355,-0.2566445088298082,-0.26122679162263396,-0.2630914546179345,-0.2622190991734792,-0.2586188006617121,-0.2523280140559605],[0.0,0.027038901276446738,0.053796508842720164,0.07999445537055039,0.1053601958514553,0.12962984296330532,0.15255091236840904,0.17388494938296908,0.19341000969188754,0.21092296830130852,0.22624163270817887,0.2392066383028998,0.24968310628662976,0.25756204685543693,0.2627614930535571,0.26522735349994797,0.2649339751169698,0.26188441000696167,0.2561103837003155,0.24767196510537298,0.23665694159375467,0.22317990572229374,0.2073810630926773,0.18942477375098465,0.1694978423013758,0.14780757452238524,0.12457962070341735,0.10005562813785385,0.07449072719458438,0.048150877121102734,0.021310099190567734,-0.005752374022782032,-0.032755003582921466,-0.059416873125101036,-0.08546061131211276,-0.11061527741058502,-0.13461917996779693,-0.1572225992654832,-0.1781903852281345,-0.19730440375898184,-0.21436580605370872,-0.22919709728354323,-0.24164398312660987,-0.251576974937527,-0.2588927368561926,-0.26351516084138826,-0.26539615844530723,-0.26451616109194015,-0.2608843236547762,-0.25453842921593844],[0.0,0.027240537752155347,0.05419768411007181,0.08059099588424344,0.10614589562358184,0.13059652812608408,0.15368852620935655,0.17518165696786248,0.19485232098709585,0.2124958785146137,0.22792877838808034,0.2409904675724536,0.25154506144083083,0.2594827574225288,0.2647209773117952,0.26720522635337773,0.2669096601676265,0.2638373536172418,0.2580202688185653,0.24951892263020317,0.2384217570781956,0.22484421926738674,0.2089275603509493,0.19083736605274065,0.1707618340299019,0.14890981599680836,0.1255086449787356,0.10080177029896681,0.07504622490327165,0.048509951369930845,0.02146901442361864,-0.005795271047814699,-0.03299926659556231,-0.059859960984777014,-0.08609791444439235,-0.11144016576194822,-0.13562307198004875,-0.15839505115224883,-0.17951919962466684,-0.1987757566149651,-0.21596439044895402,-0.23090628267041968,-0.24344598834256453,-0.25345305318779315,-0.26082337074225626,-0.26548026540627423,-0.26737529012334366,-0.26648873038923665,-0.2628298093478356,-0.2564365918400321],[0.0,0.027408964412741627,0.0545327852387361,0.0810892853245476,0.10680218952267731,0.13140399886387077,0.15463877342823978,0.1762647949641648,0.19605708155496482,0.21380972817250943,0.22933804876830172,0.2424804976172218,0.25310034992562214,0.2610871242199111,0.26635773171809307,0.2688573407270134,0.26855994707272607,0.2654686446296286,0.259615593133969,0.25106168361657044,0.2398959049353764,0.22623441799796573,0.2102193473051758,0.19201730238776407,0.17181764451803988,0.14983051672838166,0.12628465763093827,0.10142502178199954,0.07551023134708955,0.04880988557786673,0.021601756091141802,-0.0058311028716449215,-0.033203299141654284,-0.06023007164203426,-0.08663025284186633,-0.1121291938253895,-0.13646162154613595,-0.159374398541118,-0.18062915639494065,-0.20000477555713048,-0.21729968571440778,-0.23233396278644428,-0.244951200729439,-0.25502013867458534,-0.262436026473823,-0.2671217144466993,-0.2690284559913779,-0.26813641470999805,-0.2644548707726044,-0.2580221243727856],[0.0,0.02754422012890548,0.054801889565729435,0.08148943869023581,0.1073292290129049,0.1320524415304921,0.15540187332985494,0.177134613346149,0.19702456943868743,0.21486482049454286,0.23046976909026168,0.2436770723169116,0.25434933068151666,0.262375517514738,0.26767213401424705,0.27018407790753374,0.26988521669719107,0.26677865952503765,0.26089672482679266,0.25230060411380295,0.24107972537960037,0.2273508227539587,0.21125672208311677,0.1929648550701352,0.1726655174332264,0.15056988920295342,0.12690783775373193,0.10192552642535199,0.07588285361273785,0.04905074896585527,0.021708354828201205,-0.005859877763795273,-0.03336714830927623,-0.0605272905136366,-0.08705774936136765,-0.11268252061969475,-0.13713502219247786,-0.16016086745283079,-0.18152051170265573,-0.2009917442271907,-0.21837200001898693,-0.2334804671217653,-0.24615996767082893,-0.2562785930610123,-0.2637310762310263,-0.26843988678792097,-0.27035603757876014,-0.2694595943185042,-0.2657598829722897,-0.25929539273464813],[0.0,0.027646423481572,0.05500523301926944,0.08179180680238703,0.10772747615850915,0.1325424246263668,0.1559784949365165,0.17779187469798757,0.19775563285090506,0.2156620804976165,0.2313249315518986,0.24458124072836776,0.255293098710616,0.2633490668627363,0.268665336558506,0.2711866010673847,0.2708866309268369,0.2677685468152109,0.26186478708639643,0.25323677030400704,0.2419742562858453,0.22819441230589382,0.21204059416841048,0.19368085483497446,0.17330619611968,0.15112858164061002,0.12737873169944502,0.1023037230297292,0.07616441838432438,0.049232752702815435,0.02178890409167402,-0.005881620951689976,-0.033490957747705385,-0.06075187817628827,-0.08738077879617079,-0.11310063125562309,-0.1376438643005672,-0.16075514739769947,-0.18219404701376388,-0.2017375279169149,-0.21918227347839095,-0.23434680083571016,-0.2470733489127193,-0.2572295196548645,-0.26470965540542357,-0.2694359380934489,-0.27135919879824183,-0.27045942926821875,-0.26674599007268734,-0.2602575132210675],[0.0,0.027715772962033015,0.055143210516976,0.08199697689650405,0.10799770440369061,0.13287489975802383,0.1563697581172367,0.1782378555008467,0.19825169160532555,0.21620305655723665,0.2319051969823076,0.2451947588561765,0.25593348692476114,0.2640096630146761,0.2693392682705278,0.27186685722798887,0.27156613462940926,0.26844022898118125,0.2625216600069599,0.25387200033533147,0.2425812349414949,0.22876682500687373,0.21257248593560507,0.1941666922405616,0.17374092485302176,0.1515076790897456,0.12769825400116436,0.10256034609875178,0.07635547249474443,0.049356250262052405,0.021843560317987697,-0.005896374663233901,-0.03357496790986952,-0.060904270806436775,-0.08759996850857002,-0.1133843377546796,-0.13798913610341915,-0.16115839253936834,-0.1826510704154316,-0.20224357502857304,-0.21973207974183484,-0.23493464645329934,-0.2476931183521291,-0.2578747652707826,-0.26537366451631694,-0.27011180281558056,-0.27203988791043454,-0.2711378613597345,-0.2674151072132437,-0.2609103543865797],[0.0,0.0277525466670746,0.055216375359297916,0.08210577172054971,0.10814099738463506,0.1330512001764832,0.1565772318676641,0.1784743441722762,0.19851473493596172,0.21648991802928785,0.23221289229079203,0.24552008695553246,0.2562730633302732,0.2643599550122736,0.26969663166865726,0.2722275742692195,0.27192645266736637,0.2687963995204396,0.2628699776997534,0.254208841529385,0.242903095377954,0.2290703562761487,0.21285453031186913,0.1944243155325093,0.1739714469291316,0.15170870176048307,0.12786768596805906,0.10269642471079574,0.0764567820857733,0.049421736878340596,0.02187254268284153,-0.005904198061952654,-0.03361951568302344,-0.060985079510328125,-0.08771619746628298,-0.1135347778018916,-0.13817222216769603,-0.16137221992207698,-0.1828934146068849,-0.20251191485026387,-0.22002362357500407,-0.23524636128097376,-0.24802176131249642,-0.25821691739364516,-0.2657257662916524,-0.27047019122418087,-0.272400834531381,-0.2714976111582664,-0.2677699176054992,-0.26125653417480543],[0.0,0.027757101515582503,0.05522543767087366,0.08211924721727792,0.10815874587692623,0.13307303702150752,0.1566029298902456,0.1785036360282635,0.1985473158934885,0.21652544913898897,0.23225100391199408,0.24556038260173554,0.2563151237939175,0.26440334272587995,0.26974089525697154,0.2722722532450031,0.27197108222198163,0.26884051535939213,0.262913120872885,0.25425056320546857,0.24294296151401787,0.22910795213295443,0.21288946476856685,0.1944562251555838,0.17399999975330915,0.15173360074226444,0.12788867207595137,0.10271327962009372,0.07646933044985608,0.04942984815285457,0.02187613248379199,-0.005905167080329796,-0.033625033439742574,-0.06099508860252945,-0.08773059376641143,-0.11355341154096682,-0.13819489949340327,-0.16139870491547048,-0.18292343173672243,-0.20254515183944022,-0.22005973464926212,-0.2352849707660969,-0.24806246754255867,-0.258259296889661,-0.26576937816715435,-0.270514581771892,-0.27244554194326653,-0.2715421703298596,-0.2678138649745208,-0.26129941254374006],[0.0,0.02772987201248748,0.05517126194128104,0.08203868886737209,0.10805264297911622,0.13294249339571118,0.1564493036205235,0.1783285253342677,0.19835254250370948,0.21631303933877358,0.23202316746349838,0.24531948975511336,0.2560636806289051,0.2641439650803053,0.26947628151341724,0.2720051562587323,0.271704280682168,0.26857678488159215,0.26265520512348617,0.25400114535851404,0.242704636337351,0.2288831989940794,0.21268062184105296,0.19426546509435233,0.17382930709185967,0.1515847512469844,0.12776321427229143,0.10261251868279825,0.07639431462484317,0.04938135785200276,0.0218546721660841,-0.005899374156843162,-0.033592047540562726,-0.06093525288974973,-0.08764453072870018,-0.11344201651763305,-0.13805933135991644,-0.161240374027335,-0.1827439852571312,-0.20234645660334727,-0.21984385774214493,-0.23505415801945284,-0.24781912016985413,-0.25800594650579517,-0.2655086604513185,-0.27024920904781735,-0.27217827496211555,-0.2712757895493132,-0.267551141632852,-0.2610430798298004],[0.0,0.027671368585716106,0.055054863716240104,0.08186560676932236,0.1078246776324628,0.13266201639157366,0.15611923284437013,0.17795229461028741,0.19793406587172016,0.2158566703353051,0.23153365383066052,0.24480192404842652,0.25552344723793474,0.2635866842132599,0.26890775072845574,0.2714312901502464,0.2711310493495027,0.26801015182033117,0.26210106518543874,0.253465263425823,0.24219258734871377,0.2284003099470071,0.21223191637353533,0.19385561122246156,0.1734625686469946,0.15126494351796782,0.1274936643137886,0.1023960307029806,0.07623314081237871,0.04927717494586945,0.02180856401195684,-0.005886927882160525,-0.0335211763193575,-0.0608066940163577,-0.087459621639214,-0.11320268087615777,-0.13776805904612133,-0.16090019523349675,-0.18235843896414486,-0.20191955376365836,-0.2193800395526213,-0.23455824971829642,-0.2472962808382166,-0.2574616153963265,-0.2649485004020084,-0.2696790475698116,-0.2716040436143894,-0.27070346223090375,-0.2669866724346146,-0.2604923410922354],[0.0,0.027582175521924965,0.05487740549055661,0.0816017295323054,0.10747712657727951,0.13223440719494062,0.15561601405159603,0.17737870135573663,0.1972960654161572,0.21516089998671095,0.23078735189434885,0.24401285452480648,0.2546998190513119,0.26273706581196177,0.26804098094472206,0.270556386247236,0.27025711321202983,0.26714627526525814,0.2612562353768084,0.2526482693787252,0.24141192849455478,0.22766410771140336,0.21154782968671373,0.19323075684113683,0.17290344711663802,0.15077737154570456,0.1270827142554682,0.10206597779395445,0.0759874186909587,0.049118339932170026,0.021738268513738668,-0.0058679525599786495,-0.03341312758268063,-0.060610695928242565,-0.08717771322590759,-0.11283779491450253,-0.13732399155300767,-0.16038156597477346,-0.18177064339382465,-0.2012687068934093,-0.2186729123354868,-0.2338021986084622,-0.24649917125963575,-0.2566317399163812,-0.26409449246147154,-0.2688097916666018,-0.2707285828755037,-0.2698309043340852,-0.2661260948584792,-0.25965269668056956],[0.0,0.027462948527115644,0.054640191854729694,0.08124899705927599,0.1070125448475909,0.13166280939012953,0.15494334667317153,0.17661196236196092,0.1964432314202103,0.21423084327361758,0.2297897481199659,0.24295808205710495,0.25359885100113655,0.26160135587650435,0.26688234425131185,0.2693868764370657,0.26908889704255107,0.26599150603579463,0.26012692649417823,0.2515561693782065,0.2403683988163882,0.22668000450435935,0.21063339086835686,0.19239549558973237,0.172156052902737,0.15012561973300387,0.12653338521139595,0.10162478635149642,0.0756589546955556,0.048906020492194,0.021644302451301313,-0.005842587688057875,-0.03326869565468797,-0.060348699512365885,-0.08680087794857379,-0.11235004110487745,-0.13673039345864885,-0.15968829897272768,-0.18098491974634892,-0.2003987007166672,-0.21772767456179348,-0.23279156282675087,-0.2454336514137152,-0.25552242092525973,-0.26295291489965644,-0.26764783170407985,-0.26955832872642194,-0.26866453049947364,-0.26497573547130554,-0.25853031927075454],[0.0,0.027314411938676567,0.05434466394803954,0.08080955229880661,0.10643375490753221,0.13095069559390682,0.1541053173518318,0.1756567357831078,0.19538074508927988,0.21307215055106193,0.22854690322986554,0.2416440146835004,0.25222723177669004,0.26018645416285313,0.2654388796906667,0.2679298658194071,0.2676334980815083,0.26455285967845543,0.25871999940532475,0.25019559823766246,0.23906833804929697,0.22545397902850095,0.2094941553904165,0.1913549019143036,0.17122492663448285,0.14931364765468075,0.12584901450937935,0.10107513673719529,0.07524974433694284,0.048641506526008994,0.02152723669479358,-0.005810987365099713,-0.03308875799979683,-0.060022296470338146,-0.08633140518707234,-0.11174238268829369,-0.13599087103773225,-0.1588246060185779,-0.18000604151295385,-0.1993148207646407,-0.21655006881577354,-0.23153248226861103,-0.2441061946319664,-0.2541403978465485,-0.2615307031200021,-0.2662002269144781,-0.26810039078888304,-0.26721142677481075,-0.2635425830285814,-0.25713202761992804],[0.0,0.02713735561590918,0.053992393271948415,0.08028573204572453,0.10574383453492595,0.1301018525480546,0.15310638239872085,0.1745181011393562,0.1941142563088249,0.2116909832923486,0.22706542618556372,0.2400776400987331,0.2505922551108757,0.2584998845632078,0.2637182630410007,0.2661931022050907,0.2658986555695747,0.2628379863501398,0.2570429355965219,0.24857379094062085,0.2375186594063831,0.22399255080629793,0.20813618126020417,0.19011450928375653,0.17011501967605808,0.14834577305968394,0.12503324136431015,0.10041995177205393,0.07476196363526064,0.0483262046151095,0.021387693753979753,-0.0057733196292237875,-0.03287427145586036,-0.05963322248547802,-0.08577179141335897,-0.11101805095380592,-0.13510935678035296,-0.1577950798925986,-0.17883921398403796,-0.19802283068567222,-0.21514635714277008,-0.23003165223047142,-0.2425238598087489,-0.2524930197367663,-0.25983541988677133,-0.26447467509212375,-0.26636252180484077,-0.26547932019560294,-0.26183425847272357,-0.2554652572944926],[0.0,0.02693263153661982,0.05358507491869115,0.0796800568721458,0.10494610355950995,0.1291203648027591,0.1519513485916859,0.173201537429879,0.19264985929969014,0.2100939875400071,0.22535244571052307,0.23826649554497403,0.248701788352407,0.25654976268646223,0.261728773742301,0.2641849427317274,0.26389271740020315,0.26085513785453107,0.25510380493572227,0.24669855146607336,0.2357268197905692,0.2223027520882308,0.20656600291717064,0.18868028634680062,0.16883167279180766,0.1472266532664382,0.12408999119722804,0.09966238414240752,0.07419795974380848,0.047961631961627456,0.021226345095920404,-0.005729765733911926,-0.032626268111269036,-0.059183349743963,-0.08512472943449007,-0.1101805313152788,-0.13409009244738973,-0.15660467457438604,-0.17749005182019575,-0.19652894741037508,-0.2135232940668419,-0.22829629456051898,-0.2406942609853491,-0.2505882136191689,-0.2578752227380152,-0.26247947942457506,-0.2643530842308378,-0.263476545490261,-0.2598589820953592,-0.2535380286311598],[0.0,0.026701150127852284,0.05312452027091278,0.07899522027196573,0.10404410956521393,0.1280105971254164,0.15064535247361982,0.17171289953608626,0.1909940663717349,0.20828826528285507,0.2234155795884231,0.23621863535070145,0.24656423858299273,0.2543447609061909,0.2594792592387335,0.2619143178714882,0.2616246041672191,0.25861313210352554,0.25291123091753964,0.24457821917857928,0.2337007876794768,0.2203920975665471,0.20479060309061708,0.18705861122638645,0.1673805931448267,0.14596126510487267,0.12302345872945825,0.09880580282204722,0.07356024084040981,0.04754940985410068,0.021043908253122152,-0.005680519367392172,-0.032345850859989485,-0.0586746788717652,-0.08439309679533787,-0.1092335483005319,-0.13293761080224162,-0.1552586839072368,-0.17596455487119925,-0.19483981437677983,-0.21168809750062179,-0.22633412655590268,-0.2386255345581007,-0.2484344503439303,-0.2556588288529813,-0.26022351273275157,-0.26208101422287755,-0.26121200918439125,-0.2576255381344695,-0.25135891219525247],[0.0,0.026443876359266837,0.05261264922906823,0.07823407710313995,0.10304161266756014,0.12677717577151842,0.14919383831167582,0.1700583930984665,0.18915377997966448,0.20628134398153075,0.22126290197542922,0.23394259636976658,0.24418851654274704,0.25189407114769835,0.25697909701446753,0.2593906931106583,0.2591037708867618,0.2561213152797791,0.2504743536600365,0.2422216330442481,0.2314490089328763,0.2182685521298937,0.20281738283696774,0.18525624415133027,0.165767829807658,0.14455488356096088,0.1218380899831611,0.09785377861603285,0.07285146536487985,0.04709125671056183,0.020841143744622582,-0.005625785821527151,-0.0320341886690724,-0.058109330350012436,-0.08357994343111314,-0.10818104956947822,-0.13165671616084706,-0.15376271888239068,-0.17426908243078357,-0.192962473023498,-0.20964841777342308,-0.2241533278479926,-0.23632630436529936,-0.2460407082399238,-0.25319547764690203,-0.2577161793978679,-0.2595557832915896,-0.2586951513831033,-0.2551432370814309,-0.2489369920046104]],"type":"surface","uid":"80a58fd6-3a39-4bc5-b92d-944aa2e34a5f"}],                        {"autosize":true,"height":700,"scene":{"xaxis":{"title":{"text":"X1"}},"yaxis":{"title":{"text":"X2"}},"zaxis":{"title":{"text":"Y"}}},"width":860,"template":{"data":{"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"contour"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"heatmapgl"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"heatmap"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"histogram2dcontour"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"histogram2d"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}}},                        {"responsive": true}                    ).then(function(){
                            
var gd = document.getElementById('ac00dae5-4c3e-462f-9907-94668cb5782a');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })                };                });            </script>        </div></div></div>
</div>
<p>As you can see, predictions from regression trees are piecewise-constant on rectangular regions. The boundaries of these regions are
determined by a decision tree. The following code displays the
decision graph.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">graphviz</span>
<span class="n">tree_graph</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">export_graphviz</span><span class="p">(</span><span class="n">fitted_tree</span><span class="p">,</span> <span class="n">out_file</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                  <span class="n">feature_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;X1&quot;</span><span class="p">,</span> <span class="s2">&quot;X2&quot;</span><span class="p">],</span>
                                  <span class="n">filled</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">rounded</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                  <span class="n">special_characters</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">graphviz</span><span class="o">.</span><span class="n">Source</span><span class="p">(</span><span class="n">tree_graph</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">ModuleNotFoundError</span><span class="g g-Whitespace">                       </span>Traceback (most recent call last)
<span class="n">Cell</span> <span class="n">In</span><span class="p">[</span><span class="mi">26</span><span class="p">],</span> <span class="n">line</span> <span class="mi">1</span>
<span class="ne">----&gt; </span><span class="mi">1</span> <span class="kn">import</span> <span class="nn">graphviz</span>
<span class="g g-Whitespace">      </span><span class="mi">2</span> <span class="n">tree_graph</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">export_graphviz</span><span class="p">(</span><span class="n">fitted_tree</span><span class="p">,</span> <span class="n">out_file</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="g g-Whitespace">      </span><span class="mi">3</span>                                   <span class="n">feature_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;X1&quot;</span><span class="p">,</span> <span class="s2">&quot;X2&quot;</span><span class="p">],</span>
<span class="g g-Whitespace">      </span><span class="mi">4</span>                                   <span class="n">filled</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">rounded</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="g g-Whitespace">      </span><span class="mi">5</span>                                   <span class="n">special_characters</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="g g-Whitespace">      </span><span class="mi">6</span> <span class="n">display</span><span class="p">(</span><span class="n">graphviz</span><span class="o">.</span><span class="n">Source</span><span class="p">(</span><span class="n">tree_graph</span><span class="p">))</span>

<span class="ne">ModuleNotFoundError</span>: No module named &#39;graphviz&#39;
</pre></div>
</div>
</div>
</div>
<p>Regression trees are formed iteratively.</p>
<p>We begin with a rectangular region <span class="math notranslate nohighlight">\( R \)</span> containing all values of
X.</p>
<p>We then choose a feature and location to split on, aiming to minimize MSE.</p>
<p>We then repeat to generate all the branches.</p>
<ul class="simple">
<li><p>For each region, solve</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\min_{j,s} \left[ \min_{c_1} \sum_{i: x_{i,j} \leq s, x_i \in R}
    (y_i - c_1)^2 + \min_{c_2} \sum_{i: x_{i,j} &gt; s, x_i \in R}
    (y_i - c_2)^2 \right]
\]</div>
<ul class="simple">
<li><p>Repeat with each of the two smaller rectangles.</p></li>
<li><p>Stop when <span class="math notranslate nohighlight">\( |R| = \)</span> some chosen minimum size or when depth of tree <span class="math notranslate nohighlight">\( = \)</span>
some chosen maximum.</p></li>
<li><p>Prune tree.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\min_{tree \subset T} \sum (\hat{f}(x)-y)^2 + \alpha|\text{terminal
   nodes in tree}|
\]</div>
<p>This tree-building algorithm has many variations, but every variation 1) shares some rule to decide on a splitting variable and location and 2) has a stopping rule (though not necessarily the same one).</p>
<p>For example, some algorithms stop splitting into new branches when the improvement
in MSE becomes small.</p>
<p>As with lasso, regression trees also involve some regularization.</p>
<p>In the above description, the minimum leaf size, maximum tree depth, and
<span class="math notranslate nohighlight">\( \alpha \)</span> in the pruning step serve as regularization
parameters.</p>
</section>
<section id="id5">
<h3>Exercise<a class="headerlink" href="#id5" title="Permalink to this heading">#</a></h3>
<p>See exercise 6 in the <span class="xref myst">exercise list</span>.</p>
</section>
<section id="id6">
<h3>Exercise<a class="headerlink" href="#id6" title="Permalink to this heading">#</a></h3>
<p>See exercise 7 in the <span class="xref myst">exercise list</span>.</p>
<p>An advantage of regression trees (and random forests) is that they adapt
automatically to feature scales and units.</p>
<p>For example, including zip code in linear regression or lasso was a little
strange. While zip codes are numerical in value, they actually represent categorical
variables. (i.e. we can compute <code class="docutils literal notranslate"><span class="pre">10025</span></code> - <code class="docutils literal notranslate"><span class="pre">85001</span></code> (NYC and Phoenix), but the numerical difference is meaningless.)</p>
<p>Including an indicator or dummy variable for each zip code would make more sense.</p>
<p>Regression trees do not impose linearity or even monotonicity, so having the numeric zip code as a feature is
less harmful than it could have been.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ax</span> <span class="o">=</span> <span class="n">var_scatter</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">var</span><span class="o">=</span><span class="s2">&quot;zipcode&quot;</span><span class="p">)</span>
<span class="n">zip_tree</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">DecisionTreeRegressor</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">[[</span><span class="s2">&quot;zipcode&quot;</span><span class="p">]],</span><span class="n">y</span><span class="p">)</span>
<span class="n">scatter_model</span><span class="p">(</span><span class="n">zip_tree</span><span class="p">,</span> <span class="n">X</span><span class="p">[[</span><span class="s2">&quot;zipcode&quot;</span><span class="p">]],</span> <span class="n">ax</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s2">&quot;zipcode&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="id7">
<h3>Random Forests<a class="headerlink" href="#id7" title="Permalink to this heading">#</a></h3>
<p>A random forest is the average of many randomized regression trees.</p>
<p>Trees are randomized by:</p>
<ul class="simple">
<li><p>Fitting on randomly resampled subsets of data</p></li>
<li><p>Randomize features chosen for branching</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\min_{j \in S,s} \left[ \min_{c_1} \sum_{i: x_{i,j} \leq s, x_i \in R}
    (y_i - c_1)^2 + \min_{c_2} \sum_{i: x_{i,j} &gt; s, x_i \in R}
    (y_i - c_2)^2 \right]
\]</div>
<p>where <span class="math notranslate nohighlight">\( S \)</span> is a random subset of features.</p>
<p>Randomizing and averaging smooths out the predictions from individual
trees.</p>
<p>This improves predictions and reduces the variance of the predictions.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># example of forest for simulated data</span>

<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestRegressor</span>
<span class="n">forest</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">(</span><span class="n">n_estimators</span> <span class="o">=</span> <span class="mi">10</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">Xsim</span><span class="p">,</span><span class="n">ysim</span><span class="p">)</span>
<span class="n">fig</span><span class="o">=</span><span class="n">surface_scatter_plot</span><span class="p">(</span><span class="n">Xsim</span><span class="p">,</span><span class="n">ysim</span><span class="p">,</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">forest</span><span class="o">.</span><span class="n">predict</span><span class="p">([</span><span class="n">x</span><span class="p">]),</span>
                         <span class="n">show_f0</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">fig</span>
</pre></div>
</div>
</div>
</div>
<p>Random forests generally produce more accurate predictions than any single
tree.</p>
<p>However, random forests have at least two downsides compared to trees:
longer computation time and more difficulty in interpretation.</p>
<p>We can no longer draw a single decision graph.</p>
<p>Instead, people often report “feature importance” for random forests.</p>
<p>Feature importance is the average MSE decrease caused by splits on each feature.</p>
<p>If a given feature has greater importance, the trees split on that feature more often and/or splitting on that
feature resulted in larger MSE decreases.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">forest</span><span class="o">.</span><span class="n">feature_importances_</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="id8">
<h3>Exercise<a class="headerlink" href="#id8" title="Permalink to this heading">#</a></h3>
<p>See exercise 8 in the <span class="xref myst">exercise list</span>.</p>
</section>
</section>
<section id="neural-networks">
<h2>Neural Networks<a class="headerlink" href="#neural-networks" title="Permalink to this heading">#</a></h2>
<p>The final regression algorithm we will discuss in this lecture is a type of
neural network.</p>
<p>If you’re interested in this course, you have
probably heard about neural networks in the news or social media.</p>
<p>The purpose of this section is not to give an exhaustive overview of the topic,
but instead, to introduce you to a particular neural network model and present
it from a different perspective that hopefully complements materials you may later encounter.</p>
<section id="mathematical-background">
<h3>Mathematical Background<a class="headerlink" href="#mathematical-background" title="Permalink to this heading">#</a></h3>
<p>If linear regression is the <a class="reference external" href="https://en.wikipedia.org/wiki/%22Hello,_World!%22_program">“Hello World”</a> of regression
algorithms, then the multi-layer perceptron (MLP) is the “Hello World” of neural
networks.</p>
<p>We’ll start with a single (hidden) layer MLP and then build up to the general form.</p>
<p>The prediction function for a single layer MLP is</p>
<div class="math notranslate nohighlight">
\[
y = f_1(X w_1 + b_1) w_2 + b_2
\]</div>
<p>What we have here is <em>nested linear regression</em> (the <span class="math notranslate nohighlight">\( (\cdot) w_i + b_i \)</span>
parts), separated by an <em>activation function</em> (the <span class="math notranslate nohighlight">\( f_1 \)</span>).</p>
<p>Let’s unpack what happens, starting from our <span class="math notranslate nohighlight">\( N_\text{samples} \times
N_\text{features} \)</span> feature matrix <span class="math notranslate nohighlight">\( X \)</span>.</p>
<ol class="arabic simple">
<li><p>First, <span class="math notranslate nohighlight">\( X \)</span> is multiplied by a coefficient matrix <span class="math notranslate nohighlight">\( w_1 \)</span>. <span class="math notranslate nohighlight">\( w_1 \)</span> is often called the <em>weight matrix</em> or <em>weights</em> for short and has dimension <span class="math notranslate nohighlight">\( N_{\text{features}} \times N_1 \)</span>.</p></li>
<li><p>The vector <span class="math notranslate nohighlight">\( b_1 \)</span> is added to each row. <span class="math notranslate nohighlight">\( b_1 \)</span> is often called the <em>bias vector</em> or <em>bias</em> for short and has dimension <span class="math notranslate nohighlight">\( N_1 \times 1 \)</span>.</p></li>
<li><p>The function <span class="math notranslate nohighlight">\( f_1 \)</span> is then applied. Typically <span class="math notranslate nohighlight">\( f_1 \)</span> a non-linear function that is applied separately to each element. <span class="math notranslate nohighlight">\( f_1 \)</span> is called the <em>activation function</em>.</p></li>
<li><p>The output is then multiplied by a weight matrix <span class="math notranslate nohighlight">\( w_2 \)</span> with dimension <span class="math notranslate nohighlight">\( N_1 \times 1 \)</span>.</p></li>
<li><p>Finally, a scalar <span class="math notranslate nohighlight">\( b_2 \)</span> is added to each row to generate the final prediction with dimension <span class="math notranslate nohighlight">\( N_{\text{samples}} \times 1 \)</span>.</p></li>
</ol>
<p>The way we might write this in Python is:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">X</span><span class="nd">@w1</span> <span class="o">+</span> <span class="n">b1</span><span class="p">)</span><span class="nd">@w2</span> <span class="o">+</span> <span class="n">b2</span>
</pre></div>
</div>
</div>
</div>
<p>In order to build an <span class="math notranslate nohighlight">\( N \)</span>-hidden layer MLP, we will <em>nest</em> additional linear regressions
separated by activation functions.</p>
<p>The equation for this case is difficult to express, but has the following form</p>
<div class="math notranslate nohighlight">
\[
y = f_{\cdots} \left(f_2(f_1(X w_1 + b_1) w_2 + b_2) w_{\cdots} + b_{\cdots} \right) w_{N+1} + b_{N+1}
\]</div>
<p>where the <span class="math notranslate nohighlight">\( \cdots \)</span> represents layers 3 to <span class="math notranslate nohighlight">\( N \)</span>.</p>
<p>Notice the pattern of a linear regression (<span class="math notranslate nohighlight">\( (\cdot) w + b \)</span>),
followed by applying an activation function (<span class="math notranslate nohighlight">\( f \)</span>) at each step.</p>
</section>
<section id="id9">
<h3>Exercise<a class="headerlink" href="#id9" title="Permalink to this heading">#</a></h3>
<p>See exercise 9 in the <span class="xref myst">exercise list</span>.</p>
<p>The loss or error function typically used when using an MLP for regression is
our now familiar mean squared error loss function:</p>
<div class="math notranslate nohighlight">
\[
{||y - \hat{y}||_2}^2
\]</div>
<p>where <span class="math notranslate nohighlight">\( \hat{y} \)</span> is the output of the neural network.</p>
<p>Here, we fit a neural network to the same simulated data that we used
in the random forests section.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">neural_network</span>
<span class="n">nn</span> <span class="o">=</span> <span class="n">neural_network</span><span class="o">.</span><span class="n">MLPRegressor</span><span class="p">((</span><span class="mi">6</span><span class="p">,),</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;logistic&quot;</span><span class="p">,</span>
                                 <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">solver</span><span class="o">=</span><span class="s2">&quot;lbfgs&quot;</span><span class="p">,</span>
                                 <span class="n">alpha</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">Xsim</span><span class="p">,</span><span class="n">ysim</span><span class="p">)</span>
<span class="n">fig</span><span class="o">=</span><span class="n">surface_scatter_plot</span><span class="p">(</span><span class="n">Xsim</span><span class="p">,</span><span class="n">ysim</span><span class="p">,</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">predict</span><span class="p">([</span><span class="n">x</span><span class="p">]),</span> <span class="n">show_f0</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">fig</span>
</pre></div>
</div>
</div>
</div>
<p>We are nearly ready to test out a MLP on our housing data, but there are a few
more talking points to cover:</p>
<ul class="simple">
<li><p>[<span class="xref myst">regHSW89</span>] show that MLPs are universal approximators,
meaning they are theoretically capable of approximating any
function. This fact is sometimes stated as though it helps explain
the exceptionally good predictive ability of neural networks. Do not
be fooled by this fallacy. Many other methods are universal approximators,
including regression trees and more classic statistical methods like
kernel regression. The explanation for neural networks’ predictive success
lies elsewhere.  <sup><a href=#rate id=rate-link>[1]</a></sup></p></li>
<li><p>The activation functions must be non-linear. If they were not,
the MLP would be combining linear combinations of linear combinations and
would therefore always be linear.</p></li>
<li><p>The hidden layer structure of an MLP allows it to automatically perform feature engineering.
Contrastly,the example we had above required us to manually engineer
the square feet above ground feature.</p></li>
</ul>
</section>
<section id="application">
<h3>Application<a class="headerlink" href="#application" title="Permalink to this heading">#</a></h3>
<p>Ok, now let’s try out our first neural network!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">neural_network</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s2">&quot;price&quot;</span><span class="p">,</span> <span class="s2">&quot;date&quot;</span><span class="p">,</span> <span class="s2">&quot;id&quot;</span><span class="p">,</span> <span class="s2">&quot;log_price&quot;</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="n">X</span><span class="p">[</span><span class="n">col</span><span class="p">]</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">col</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s2">&quot;price&quot;</span><span class="p">])</span>

<span class="c1"># two hidden layers, with N1=30 and N2=20</span>
<span class="n">nn_model</span> <span class="o">=</span> <span class="n">neural_network</span><span class="o">.</span><span class="n">MLPRegressor</span><span class="p">((</span><span class="mi">30</span><span class="p">,</span> <span class="mi">20</span><span class="p">))</span>
<span class="n">nn_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">var_scatter</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
<span class="n">scatter_model</span><span class="p">(</span><span class="n">nn_model</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Wow! That plot looks horrible. Let’s check the MSE.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mse_nn</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">nn_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
<span class="n">mse_nn</span> <span class="o">/</span> <span class="n">metrics</span><span class="o">.</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">lr_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>So… after all that talk about neural networks being all-powerful, our neural network above
produced a mean squared error which was tens of thousands of times larger than the
MSE from a linear regression!</p>
</section>
<section id="input-scaling">
<h3>Input Scaling<a class="headerlink" href="#input-scaling" title="Permalink to this heading">#</a></h3>
<p>The issue here is that neural networks are extremely sensitive to the scale
(both relative and absolute) of the input features.</p>
<p>The reasons for why are a bit beyond the scope of this lecture, but the main
idea is that the training procedure pays too much attention to relatively larger
features (relative scale) and becomes unstable if features are very large
(absolute scale).</p>
<p>A common technique to overcome this issue is to scale each variable so that the
observations have a mean of 0 and a standard deviation of 1.</p>
<p>This is known as scaling or normalizing the inputs.</p>
</section>
<section id="id10">
<h3>Exercise<a class="headerlink" href="#id10" title="Permalink to this heading">#</a></h3>
<p>See exercise 10 in the <span class="xref myst">exercise list</span>.</p>
<p>If we decide to scale our variables, we must remember to apply the same
transformation at prediction time as we did when we fit the model.</p>
<p>In practice, we must do three things:</p>
<ol class="arabic simple">
<li><p>Store the mean and standard deviation of each feature in the training set.</p></li>
<li><p>Subtract each feature’s mean from the training data and then divide by the feature’s standard deviation before fitting.</p></li>
<li><p>Subtract the <em>training data’s</em> mean and divide by <em>training data’s</em> standard deviation for all prediction inputs.</p></li>
</ol>
<p>Applying the transformation to the prediction data is easily forgotten, so this is a tedious and
somewhat error-prone process.</p>
<p>Thankfully, scikit-learn has a way to automate the process and ensure that it is
always applied.</p>
<p>Let’s see an example:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">preprocessing</span><span class="p">,</span> <span class="n">pipeline</span>

<span class="c1"># the pipeline defines any number of steps that will be applied</span>
<span class="c1"># to transform the `X` data and then a final step that is a model</span>
<span class="c1"># we can use for prediction</span>
<span class="n">nn_scaled_model</span> <span class="o">=</span> <span class="n">pipeline</span><span class="o">.</span><span class="n">make_pipeline</span><span class="p">(</span>
    <span class="n">preprocessing</span><span class="o">.</span><span class="n">StandardScaler</span><span class="p">(),</span>  <span class="c1"># this will do the input scaling</span>
    <span class="n">neural_network</span><span class="o">.</span><span class="n">MLPRegressor</span><span class="p">((</span><span class="mi">30</span><span class="p">,</span> <span class="mi">20</span><span class="p">))</span>  <span class="c1"># put your favorite model here</span>
<span class="p">)</span>

<span class="c1"># We can now use `model` like we have used our other models all along</span>
<span class="c1"># Call fit</span>
<span class="n">nn_scaled_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># Call predict</span>
<span class="n">mse_nn_scaled</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">nn_scaled_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unscaled mse </span><span class="si">{</span><span class="n">mse_nn</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Scaled mse </span><span class="si">{</span><span class="n">mse_nn_scaled</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>There we have it, much better. This is the smallest MSE we have seen so far.</p>
<p>A scatter plot of the predictions looks very similar to the observed prices.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ax</span> <span class="o">=</span> <span class="n">var_scatter</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
<span class="n">scatter_model</span><span class="p">(</span><span class="n">nn_scaled_model</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="tradeoffs">
<h3>Tradeoffs<a class="headerlink" href="#tradeoffs" title="Permalink to this heading">#</a></h3>
<p>So we’ve seen that neural networks are very flexible and can approximate highly
nonlinear functions – but they have a few cons.</p>
<p>We’ll discuss a few of them here.</p>
<ul class="simple">
<li><p><strong>Interpretability</strong>: Unlike linear regression or lasso, neural
networks are not easily interpretable. We could look at the <span class="math notranslate nohighlight">\( w \)</span>
matrices or <span class="math notranslate nohighlight">\( b \)</span> vectors, but the nested composition and
nonlinear activation functions make it very difficult to interpret
just how each coefficient impacts the output. In settings like
making economic policy recommendations or decisions with ethical consequences
(e.g. approving loans, screening),
the lack of interpretability can be a non-starter.</p></li>
<li><p><strong>Efficiency/time</strong>: Neural networks require more computational
power to evaluate (generate predictions) and are orders of magnitude
more expensive to train than classical machine learning
methods.</p></li>
<li><p><strong>Automated feature engineering</strong>: The nested linear regressions
allow neural networks to learn data features that are
composed of arbitrary linear combinations of the original feature
set. The non-linear activation functions allow the network to learn
arbitrary non-linear features. Manual feature engineering is based
largely on the researchers intuition, as well as a fair amount of trial and
error. Determining the proper features that allow for more
explanatory power without overfitting is very difficult. Neural
networks automate that process by using the data itself to guide the
training process and select features that satisfy accuracy and
regularization conditions.</p></li>
<li><p><strong>Overfitting</strong>: Neural networks’ flexibility and explanatory
power make them easy to accidentally overfit. When training neural
networks, the various approaches to regularization should be studied and evaluated,
especially when building networks used for decision-making.</p></li>
</ul>
</section>
<section id="id11">
<h3>Exercise<a class="headerlink" href="#id11" title="Permalink to this heading">#</a></h3>
<p>See exercise 11 in the <span class="xref myst">exercise list</span>.</p>
</section>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this heading">#</a></h2>
<p>Two good text books covering the above regression methods are
[<span class="xref myst">regFHT09</span>] and [<span class="xref myst">regEH16</span>]</p>
<p><a id='id24'></a>
[regAI18] Susan Athey and Guido Imbens. Machine learning and econometrics. 2018. URL: <a class="reference external" href="https://www.aeaweb.org/conference/cont-ed/2018-webcasts">https://www.aeaweb.org/conference/cont-ed/2018-webcasts</a>.</p>
<p><a id='id25'></a>
[regAI17] Susan Athey and Guido W. Imbens. The state of applied econometrics: causality and policy evaluation. <em>Journal of Economic Perspectives</em>, 31(2):3–32, May 2017. URL: <a class="reference external" href="http://www.aeaweb.org/articles?id=10.1257/jep.31.2.3">http://www.aeaweb.org/articles?id=10.1257/jep.31.2.3</a>, <a class="reference external" href="https://doi.org/10.1257/jep.31.2.3">doi:10.1257/jep.31.2.3</a>.</p>
<p><a id='id23'></a>
[regBC11] Alexandre Belloni and Victor Chernozhukov. <em>High Dimensional Sparse Econometric Models: An Introduction</em>, pages 121–156. Springer Berlin Heidelberg, Berlin, Heidelberg, 2011. URL: <a class="reference external" href="https://doi.org/10.1007/978-3-642-19989-9_3">https://doi.org/10.1007/978-3-642-19989-9_3</a>, <a class="reference external" href="https://doi.org/10.1007/978-3-642-19989-9_3">doi:10.1007/978-3-642-19989-9_3</a>.</p>
<p><a id='id14'></a>
[regCCD+18] Victor Chernozhukov, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen, Whitney Newey, and James Robins. Double/debiased machine learning for treatment and structural parameters. <em>The Econometrics Journal</em>, 21(1):C1–C68, 2018. URL: <a class="reference external" href="https://onlinelibrary.wiley.com/doi/abs/10.1111/ectj.12097">https://onlinelibrary.wiley.com/doi/abs/10.1111/ectj.12097</a>, <a class="reference external" href="https://arxiv.org/abs/https://onlinelibrary.wiley.com/doi/pdf/10.1111/ectj.12097">arXiv:https://onlinelibrary.wiley.com/doi/pdf/10.1111/ectj.12097</a>, <a class="reference external" href="https://doi.org/10.1111/ectj.12097">doi:10.1111/ectj.12097</a>.</p>
<p><a id='id21'></a>
[regCHS16] Victor Chernozhukov, Chris Hansen, and Martin Spindler. hdm: high-dimensional metrics. <em>R Journal</em>, 8(2):185–199, 2016. URL: <a class="reference external" href="https://journal.r-project.org/archive/2016/RJ-2016-040/index.html">https://journal.r-project.org/archive/2016/RJ-2016-040/index.html</a>.</p>
<p><a id='id27'></a>
[regEH16] Bradley Efron and Trevor Hastie. <em>Computer age statistical inference</em>. Volume 5. Cambridge University Press, 2016. URL: <a class="reference external" href="https://web.stanford.edu/~hastie/CASI/">https://web.stanford.edu/~hastie/CASI/</a>.</p>
<p><a id='id26'></a>
[regFHT09] Jerome Friedman, Trevor Hastie, and Robert Tibshirani. <em>The elements of statistical learning</em>. Springer series in statistics, 2009. URL: <a class="reference external" href="https://web.stanford.edu/~hastie/ElemStatLearn/">https://web.stanford.edu/~hastie/ElemStatLearn/</a>.</p>
<p><a id='id28'></a>
[regHSW89] Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are universal approximators. <em>Neural Networks</em>, 2(5):359 – 366, 1989. URL: <a class="reference external" href="http://www.sciencedirect.com/science/article/pii/0893608089900208">http://www.sciencedirect.com/science/article/pii/0893608089900208</a>, <a class="reference external" href="https://doi.org/https://doi.org/10.1016/0893-6080%2889%2990020-8">doi:https://doi.org/10.1016/0893-6080(89)90020-8</a>.</p>
<p><a id='app-reg-ex'></a></p>
</section>
<section id="exercises">
<h2>Exercises<a class="headerlink" href="#exercises" title="Permalink to this heading">#</a></h2>
<section id="exercise-1">
<h3>Exercise 1<a class="headerlink" href="#exercise-1" title="Permalink to this heading">#</a></h3>
<p>Use the <code class="docutils literal notranslate"><span class="pre">sqft_lr_model</span></code> that we fit to generate predictions for all data points
in our sample.</p>
<p>Note that you need to pass a DataFrame (not Series)
containing the <code class="docutils literal notranslate"><span class="pre">sqft_living</span></code> column to the <code class="docutils literal notranslate"><span class="pre">predict</span></code>. (See how we passed that to <code class="docutils literal notranslate"><span class="pre">.fit</span></code>
above for help)</p>
<p>Make a scatter chart with the actual data and the predictions on the same
figure. Does it look familiar?</p>
<p>When making the scatter for model predictions, we recommend passing
<code class="docutils literal notranslate"><span class="pre">c=&quot;red&quot;</span></code> and <code class="docutils literal notranslate"><span class="pre">alpha=0.25</span></code> so you can distinguish the data from
predictions.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Generate predictions</span>

<span class="c1"># Plot</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>

<span class="c1"># Make scatter of data</span>

<span class="c1"># Make scatter of predictions</span>
</pre></div>
</div>
</div>
</div>
<p>(<span class="xref myst">back to text</span>)</p>
</section>
<section id="exercise-2">
<h3>Exercise 2<a class="headerlink" href="#exercise-2" title="Permalink to this heading">#</a></h3>
<p>Use the <code class="docutils literal notranslate"><span class="pre">metrics.mean_squared_error</span></code> function to evaluate the loss
function used by <code class="docutils literal notranslate"><span class="pre">sklearn</span></code> when it fits the model for us.</p>
<p>Read the docstring to learn which the arguments that function takes.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>

<span class="c1"># your code here</span>
</pre></div>
</div>
</div>
</div>
<p>(<span class="xref myst">back to text</span>)</p>
</section>
<section id="exercise-3">
<h3>Exercise 3<a class="headerlink" href="#exercise-3" title="Permalink to this heading">#</a></h3>
<p>Compare the mean squared error for the <code class="docutils literal notranslate"><span class="pre">lr_model</span></code> and the <code class="docutils literal notranslate"><span class="pre">sqft_lr_model</span></code>.</p>
<p>Which model has a better fit? Defend your choice.</p>
<p>(<span class="xref myst">back to text</span>)</p>
</section>
<section id="exercise-4">
<h3>Exercise 4<a class="headerlink" href="#exercise-4" title="Permalink to this heading">#</a></h3>
<p>Explore how you can improve the fit of the full model by adding additional
features created from the existing ones.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># your code here</span>
</pre></div>
</div>
</div>
</div>
<p>(<span class="xref myst">back to text</span>)</p>
</section>
<section id="exercise-5">
<h3>Exercise 5<a class="headerlink" href="#exercise-5" title="Permalink to this heading">#</a></h3>
<p>Experiment with how the size of the holdout dataset can impact a diagnosis
of overfitting.</p>
<p>Evaluate only the <code class="docutils literal notranslate"><span class="pre">LinearRegression</span></code> model on the full feature set and use
the <code class="docutils literal notranslate"><span class="pre">model_selection.train_test_split</span></code> function with various values for
<code class="docutils literal notranslate"><span class="pre">test_size</span></code>.</p>
<p>(<span class="xref myst">back to text</span>)</p>
</section>
<section id="exercise-6">
<h3>Exercise 6<a class="headerlink" href="#exercise-6" title="Permalink to this heading">#</a></h3>
<p>Read the documentation for <code class="docutils literal notranslate"><span class="pre">sklearn.tree.DecisionTreeRegressor</span></code> and
then experiment with adjusting some regularization parameters to see how they
affect the fitted tree.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># plot trees when varying some regularization parameter(s)</span>
</pre></div>
</div>
</div>
</div>
<p>(<span class="xref myst">back to text</span>)</p>
</section>
<section id="exercise-7">
<h3>Exercise 7<a class="headerlink" href="#exercise-7" title="Permalink to this heading">#</a></h3>
<p>Fit a regression tree to the housing price data and use graphviz
to visualize the decision graph.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># your code here</span>
</pre></div>
</div>
</div>
</div>
<p>(<span class="xref myst">back to text</span>)</p>
</section>
<section id="exercise-8">
<h3>Exercise 8<a class="headerlink" href="#exercise-8" title="Permalink to this heading">#</a></h3>
<p>Fit a random forest to the housing price data.</p>
<p>Compare the MSE on a testing set to that of lasso.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Fit random forest and compute MSE</span>
</pre></div>
</div>
</div>
</div>
<p>Produce a bar chart of feature importances for predicting house
prices.</p>
<p>(<span class="xref myst">back to text</span>)</p>
</section>
<section id="exercise-9">
<h3>Exercise 9<a class="headerlink" href="#exercise-9" title="Permalink to this heading">#</a></h3>
<p>In the pseudocode below, fill in the blanks for the generic MLP.</p>
<p>Note that this is inside a markdown cell because the code is not valid
Python.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ws</span> <span class="o">=</span> <span class="p">[</span><span class="n">w1</span><span class="p">,</span> <span class="n">w2</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="n">wend</span><span class="p">]</span>
<span class="n">bs</span> <span class="o">=</span> <span class="p">[</span><span class="n">b1</span><span class="p">,</span> <span class="n">b2</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="n">bend</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">eval_mlp</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">ws</span><span class="p">,</span> <span class="n">bs</span><span class="p">,</span> <span class="n">f</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Evaluate MLP-given weights (ws), bias (bs) and an activation (f)</span>

<span class="sd">    Assumes that the same activation is applied to all hidden layers</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">N</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">ws</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>

    <span class="n">out</span> <span class="o">=</span> <span class="n">X</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">__</span><span class="p">)</span>  <span class="c1"># replace the __</span>

    <span class="c1"># For this step remember python starts counting at 0!</span>
    <span class="k">return</span> <span class="n">out</span><span class="nd">@__</span> <span class="o">+</span> <span class="n">__</span>  <span class="c1"># replace the __</span>
</pre></div>
</div>
</div>
</div>
<p>(<span class="xref myst">back to text</span>)</p>
</section>
<section id="exercise-10">
<h3>Exercise 10<a class="headerlink" href="#exercise-10" title="Permalink to this heading">#</a></h3>
<p>Scale all variables in <code class="docutils literal notranslate"><span class="pre">X</span></code> by subtracting their mean and dividing by the
standard deviation.</p>
<p>Verify that the transformed data has mean 0 and standard deviation 1.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># your code here</span>
</pre></div>
</div>
</div>
</div>
<p>(<span class="xref myst">back to text</span>)</p>
</section>
<section id="exercise-11">
<h3>Exercise 11<a class="headerlink" href="#exercise-11" title="Permalink to this heading">#</a></h3>
<p>Read the documentation for sklearn.neural_network.MLPRegressor and
use the full housing data to experiment with how adjusting layer depth, width, and other
regularization parameters affects prediction.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># your code here</span>
</pre></div>
</div>
</div>
</div>
<p>(<span class="xref myst">back to text</span>)</p>
<p><a id=rate href=#rate-link><strong>[1]</strong></a> Two facts about neural networks are relevant to their
predictive success: automatic feature engineering, as
mentioned above, and the ability of neural networks to
approximate a broad (but not quite universal) class of
functions with relatively few parameters. This gives
neural networks its fast statistical convergence
rate. Under appropriate assumptions, lasso, series
regression, and kernel regression share this fast
convergence rate property, but they lack automatic feature
engineering. On the other hand, random forests have automatic feature
engineering, but do not have a fast convergence rate.
Neural networks are somewhat unique in combining both
properties.
See
[these notes and references therein](http://faculty.arts.ubc.ca/pschrimpf/628/machineLearningAndCausalInference.html#2_introduction_to_machine_learning)
for more information about convergence rates.</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./Lab2"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction-to-regression">Introduction to Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dataset">Dataset</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-regression">Linear Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise">Exercise</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Exercise</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multivariate-linear-regression">Multivariate Linear Regression</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Exercise</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#nonlinear-relationships-in-linear-regression">Nonlinear Relationships in Linear Regression</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Exercise</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretability">Interpretability</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lasso-regression">Lasso Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#overfitting-and-regularization">Overfitting and Regularization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cross-validation-of-regularization-parameter">Cross-validation of Regularization Parameter</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#holdout">Holdout</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">Exercise</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lasso-in-econometrics">Lasso in Econometrics</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#random-forests">Random Forests</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#regression-trees">Regression Trees</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">Exercise</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">Exercise</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">Random Forests</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">Exercise</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#neural-networks">Neural Networks</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mathematical-background">Mathematical Background</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">Exercise</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#application">Application</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#input-scaling">Input Scaling</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id10">Exercise</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tradeoffs">Tradeoffs</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id11">Exercise</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercises">Exercises</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-1">Exercise 1</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-2">Exercise 2</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-3">Exercise 3</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-4">Exercise 4</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-5">Exercise 5</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-6">Exercise 6</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-7">Exercise 7</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-8">Exercise 8</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-9">Exercise 9</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-10">Exercise 10</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-11">Exercise 11</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Miguel C. Herculano
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>