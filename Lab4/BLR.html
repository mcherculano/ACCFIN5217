

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Bayesian Linear Regression &#8212; ECON 5129</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'Lab4/BLR';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="prev" title="Penalized Regression" href="../Lab3/regression_2.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
    
    
      
    
    
    <img src="../_static/ASBS_small_.PNG" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="../_static/ASBS_small_.PNG" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Course Overview
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">PRELIMINARY</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../getting_started/getting_started.html">Getting Started</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../getting_started/about_py.html">About Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../getting_started/setting_up_your_python_environment.html">Setting up Your Python Environment</a></li>
<li class="toctree-l2"><a class="reference internal" href="../getting_started/python_by_example.html">An Introductory Example</a></li>
<li class="toctree-l2"><a class="reference internal" href="../getting_started/learn_more.html">Learn More</a></li>
<li class="toctree-l2"><a class="reference internal" href="../getting_started/exercises.html">Exercises</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LAB MATERIALS</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Lab1/regression_sol.html">Introduction to Regression</a></li>



<li class="toctree-l1"><a class="reference internal" href="../Lab2/mle_sol.html">Maximum Likelihood Estimation</a></li>


<li class="toctree-l1"><a class="reference internal" href="../Lab3/regression_2.html">Penalized Regression</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Bayesian Linear Regression</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/Lab4/BLR.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Bayesian Linear Regression</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise">Exercise</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#predicting-house-values">Predicting House Values</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Exercise</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Exercise</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#automatic-relevance-determination-regression">Automatic Relevance Determination Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-regressions-with-polynomial-feature-expansion">Bayesian regressions with polynomial feature expansion</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Exercise</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="bayesian-linear-regression">
<h1>Bayesian Linear Regression<a class="headerlink" href="#bayesian-linear-regression" title="Permalink to this heading">#</a></h1>
<p>A probabilistic view of a linear regression offers advantages comparing with a frequencist treatment of the linear regression model as seen in the previous labs. Let’s discuss the differences between the two approaches looking at an example with data on <a class="reference external" href="https://www.kaggle.com/">Kaggle</a>. Before that, let us formally introduce some concepts and notation; I will follow your lecture’s notation and the flow of section 3.3 of <a class="reference external" href="https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf">Chris Bishop’s Book</a>.</p>
<p>Start by considering a standard linear regression model written as</p>
<div class="math notranslate nohighlight">
\[
y = X w + \varepsilon,
\]</div>
<p>Where <span class="math notranslate nohighlight">\(X \in \mathbb{R}^{n \times d}\)</span> is a matrix of inputs, <span class="math notranslate nohighlight">\(y \in \mathbb{R}^n\)</span> is a vector of outputs, <span class="math notranslate nohighlight">\(w \in \mathbb{R}^{d}\)</span> a vector of parameters.</p>
<p>Recall from your lecture that, we have assumed for simplicity that in a Bayesian setting we have</p>
<ul class="simple">
<li><p><strong>Likelihood:</strong> <span class="math notranslate nohighlight">\(y \sim N(Xw, \sigma^2 I) \)</span></p></li>
<li><p><strong>Prior:</strong> <span class="math notranslate nohighlight">\(w \sim N(0,\lambda^{-1}I)\)</span></p></li>
</ul>
<p>For Bayesians any unknown quantity is treated as random and therefore, contrary to standard linear regression, <span class="math notranslate nohighlight">\(w\)</span> is vector of random variables here. This different way to think about <span class="math notranslate nohighlight">\(w\)</span> leads to the need of specifying its ‘Prior’ distribution that reflects our prior belief on <span class="math notranslate nohighlight">\(w\)</span>. Here we assume a Gaussian prior for simplicity, but you can specify other types of distribution that best reflect your prior knowledge about the DGP.</p>
<p>Bayesian inference then proceeds by estimating the posterior which according to the Bayes Rule is proportional to the Likelihood times the Prior, i.e.</p>
<div class="math notranslate nohighlight">
\[
p(w|y,X) \propto p(y|w,X)p(w).
\]</div>
<p>The <span class="math notranslate nohighlight">\(\propto\)</span> sign indicates that the quantities in both sides are proportional to each other. One important thing to notice is that, while in the standard linear regression estimation yields a point estimate of <span class="math notranslate nohighlight">\(w\)</span> (and respective standard deviation), Bayesians report a distribution instead (i.e. <span class="math notranslate nohighlight">\(p(w|y,X)\)</span>) from which one can calculate moments (eg. mean, standard deviation).</p>
<p>Given our choice of Prior, it turns out that in this case</p>
<div class="math notranslate nohighlight">
\[
p(w|y,X) = N(w|\mu, \Sigma). 
\]</div>
<p>with</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\Sigma = (\lambda I + \sigma^{-2} X'X)^{-1},\\
\mu = (\lambda \sigma^{2} I +  X'X)^{-1} X'y.
\end{split}\]</div>
<p>Bayesian inference often involves Markov-Chain Monte Carlo (MCMC) algorithms such as the Gibbs Sampler or Metropolis Hastings which can be computationally expensive as model complexity grows. Variational Bayes (VB) methods are also popular as they don’t require simulation to compute posterior distributions and can therefore be faster. These alternative methods of inference are often necessary since the posterior distribution is of an unknown form.</p>
<p>However, in specific cases (such as the one above) where you have very simple models and conjugate priors, it may be possible to obtain a closed-form solution for the posterior distribution. Conjugate priors are chosen so that the posterior distribution belongs to the same family of distributions as the prior, simplifying the maths. These cases are the exception rather than the rule.</p>
<p>Maximizing the log posterior wrt. <span class="math notranslate nohighlight">\(w\)</span> gives the maximum-a-posterior (MAP) estimate.</p>
<p>In summary, while there are cases where you can obtain a closed-form solution for the posterior distribution in Bayesian linear regression, it is more common to use numerical methods like MCMC or VB to approximate the posterior distribution, especially in complex and real-world scenarios. These numerical methods provide a flexible and powerful way to estimate the Bayesian posterior when closed-form solutions are not available.</p>
<section id="exercise">
<h2>Exercise<a class="headerlink" href="#exercise" title="Permalink to this heading">#</a></h2>
<p>Think about how the previous solution compares with OLS estimators.</p>
<p>Now let’s see an example where we try and predict house prices in California with a Bayesian Linear Model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># load required libraries</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_california_housing</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">data_cali_hp</span> <span class="o">=</span> <span class="n">fetch_california_housing</span><span class="p">(</span><span class="n">as_frame</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">data_cali_hp</span><span class="o">.</span><span class="n">DESCR</span><span class="p">)</span> <span class="c1">#&lt;- uncomment to read a brief data description</span>
<span class="n">data_cali_hp</span> <span class="o">=</span> <span class="n">data_cali_hp</span><span class="o">.</span><span class="n">frame</span>
<span class="c1">#data_cali_hp.info()</span>
<span class="c1">#data_cali_hp.head()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>.. _california_housing_dataset:

California Housing dataset
--------------------------

**Data Set Characteristics:**

    :Number of Instances: 20640

    :Number of Attributes: 8 numeric, predictive attributes and the target

    :Attribute Information:
        - MedInc        median income in block group
        - HouseAge      median house age in block group
        - AveRooms      average number of rooms per household
        - AveBedrms     average number of bedrooms per household
        - Population    block group population
        - AveOccup      average number of household members
        - Latitude      block group latitude
        - Longitude     block group longitude

    :Missing Attribute Values: None

This dataset was obtained from the StatLib repository.
https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html

The target variable is the median house value for California districts,
expressed in hundreds of thousands of dollars ($100,000).

This dataset was derived from the 1990 U.S. census, using one row per census
block group. A block group is the smallest geographical unit for which the U.S.
Census Bureau publishes sample data (a block group typically has a population
of 600 to 3,000 people).

A household is a group of people residing within a home. Since the average
number of rooms and bedrooms in this dataset are provided per household, these
columns may take surprisingly large values for block groups with few households
and many empty houses, such as vacation resorts.

It can be downloaded/loaded using the
:func:`sklearn.datasets.fetch_california_housing` function.

.. topic:: References

    - Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,
      Statistics and Probability Letters, 33 (1997) 291-297
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Describe the data</span>
<span class="n">data_cali_hp</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">bins</span><span class="o">=</span><span class="mi">60</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;darkviolet&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">hspace</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.4</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/89b6ad0497dd51209417deab6fcc79476b9420da3648d89befc0e5147ebe958c.png" src="../_images/89b6ad0497dd51209417deab6fcc79476b9420da3648d89befc0e5147ebe958c.png" />
</div>
</div>
<p>We can first focus on features for which their distributions would be more or less expected.</p>
<p>The median income is a distribution with a long tail. It means that the salary of people is more or less normally distributed but there is some people getting a high salary.</p>
<p>Regarding the average house age, the distribution is more or less uniform.</p>
<p>The target distribution has a long tail as well. In addition, we have a threshold-effect for high-valued houses: all houses with a price above 5 are given the value 5.</p>
<p>Focusing on the average rooms, average bedrooms, average occupation, and population, the range of the data is large with unnoticeable bin for the largest values. It means that there are very high and few values (maybe they could be considered as outliers?). We can see this specificity looking at the statistics for these features:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">features_of_interest</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;AveRooms&quot;</span><span class="p">,</span> <span class="s2">&quot;AveBedrms&quot;</span><span class="p">,</span> <span class="s2">&quot;AveOccup&quot;</span><span class="p">,</span> <span class="s2">&quot;Population&quot;</span><span class="p">]</span>
<span class="n">data_cali_hp</span><span class="p">[</span><span class="n">features_of_interest</span><span class="p">]</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>AveRooms</th>
      <th>AveBedrms</th>
      <th>AveOccup</th>
      <th>Population</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>20640.000000</td>
      <td>20640.000000</td>
      <td>20640.000000</td>
      <td>20640.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>5.429000</td>
      <td>1.096675</td>
      <td>3.070655</td>
      <td>1425.476744</td>
    </tr>
    <tr>
      <th>std</th>
      <td>2.474173</td>
      <td>0.473911</td>
      <td>10.386050</td>
      <td>1132.462122</td>
    </tr>
    <tr>
      <th>min</th>
      <td>0.846154</td>
      <td>0.333333</td>
      <td>0.692308</td>
      <td>3.000000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>4.440716</td>
      <td>1.006079</td>
      <td>2.429741</td>
      <td>787.000000</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>5.229129</td>
      <td>1.048780</td>
      <td>2.818116</td>
      <td>1166.000000</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>6.052381</td>
      <td>1.099526</td>
      <td>3.282261</td>
      <td>1725.000000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>141.909091</td>
      <td>34.066667</td>
      <td>1243.333333</td>
      <td>35682.000000</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>For each of these features, comparing the interquartile range, we can see a huge difference. It confirms the intuitions that there are a couple of extreme values.</p>
<p>Up to know, we discarded the longitude and latitude that carry geographical information. In short, the combination of this feature could help us to decide if there are locations associated with high-valued houses. Indeed, we could make a scatter plot where the x- and y-axis would be the latitude and longitude and the circle size and color would be linked with the house value in the district.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="ch">#! pip install geopandas geoplot</span>
<span class="kn">import</span> <span class="nn">geopandas</span> <span class="k">as</span> <span class="nn">gpd</span>
<span class="kn">import</span> <span class="nn">geoplot</span> <span class="k">as</span> <span class="nn">gplt</span>
<span class="kn">import</span> <span class="nn">geoplot.crs</span> <span class="k">as</span> <span class="nn">gcrs</span>
<span class="kn">import</span> <span class="nn">mapclassify</span> <span class="k">as</span> <span class="nn">mc</span>
<span class="s1">&#39; Plot Two Geopandas Plots Side by Side &#39;</span>
<span class="c1"># defining a simple plot function, input list containing features of names found in dataframe</span>
<span class="k">def</span> <span class="nf">plotTwo</span><span class="p">(</span><span class="n">df</span><span class="p">,</span><span class="n">lst</span><span class="p">):</span>
    
    <span class="c1"># load california from module, common for all plots</span>
    <span class="n">cali</span> <span class="o">=</span> <span class="n">gpd</span><span class="o">.</span><span class="n">read_file</span><span class="p">(</span><span class="n">gplt</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">get_path</span><span class="p">(</span><span class="s1">&#39;california_congressional_districts&#39;</span><span class="p">))</span>
    <span class="n">cali</span> <span class="o">=</span> <span class="n">cali</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">area</span><span class="o">=</span><span class="n">cali</span><span class="o">.</span><span class="n">geometry</span><span class="o">.</span><span class="n">area</span><span class="p">)</span>
    
    <span class="c1"># Create a geopandas geometry feature; input dataframe should contain .longtitude, .latitude</span>
    <span class="n">gdf</span> <span class="o">=</span> <span class="n">gpd</span><span class="o">.</span><span class="n">GeoDataFrame</span><span class="p">(</span><span class="n">df</span><span class="p">,</span><span class="n">geometry</span><span class="o">=</span><span class="n">gpd</span><span class="o">.</span><span class="n">points_from_xy</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">Longitude</span><span class="p">,</span><span class="n">df</span><span class="o">.</span><span class="n">Latitude</span><span class="p">))</span>
    <span class="n">proj</span><span class="o">=</span><span class="n">gcrs</span><span class="o">.</span><span class="n">AlbersEqualArea</span><span class="p">(</span><span class="n">central_latitude</span><span class="o">=</span><span class="mf">37.16611</span><span class="p">,</span> <span class="n">central_longitude</span><span class="o">=-</span><span class="mf">119.44944</span><span class="p">)</span> <span class="c1"># related to view</span>

    <span class="n">ii</span><span class="o">=-</span><span class="mi">1</span>
    <span class="n">fig</span><span class="p">,</span><span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">21</span><span class="p">,</span><span class="mi">6</span><span class="p">),</span><span class="n">subplot_kw</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;projection&#39;</span><span class="p">:</span> <span class="n">proj</span><span class="p">})</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">lst</span><span class="p">:</span>

        <span class="n">ii</span><span class="o">+=</span><span class="mi">1</span>
        <span class="n">tgdf</span> <span class="o">=</span> <span class="n">gdf</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="n">i</span><span class="p">,</span><span class="n">ascending</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> 
        <span class="n">gplt</span><span class="o">.</span><span class="n">polyplot</span><span class="p">(</span><span class="n">cali</span><span class="p">,</span><span class="n">projection</span><span class="o">=</span><span class="n">proj</span><span class="p">,</span><span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="n">ii</span><span class="p">])</span> <span class="c1"># the module already has california </span>
        <span class="n">gplt</span><span class="o">.</span><span class="n">pointplot</span><span class="p">(</span><span class="n">tgdf</span><span class="p">,</span><span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="n">ii</span><span class="p">],</span><span class="n">hue</span><span class="o">=</span><span class="n">i</span><span class="p">,</span><span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;plasma&#39;</span><span class="p">,</span><span class="n">legend</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span><span class="n">s</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span> <span class="c1"># </span>
        <span class="n">ax</span><span class="p">[</span><span class="n">ii</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">wspace</span><span class="o">=-</span><span class="mf">0.5</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Call function that plots two geopandas plots </span>
<span class="n">plotTwo</span><span class="p">(</span><span class="n">data_cali_hp</span><span class="p">,[</span><span class="s1">&#39;Population&#39;</span><span class="p">,</span><span class="s1">&#39;MedInc&#39;</span><span class="p">])</span>
<span class="n">plotTwo</span><span class="p">(</span><span class="n">data_cali_hp</span><span class="p">,[</span><span class="s1">&#39;HouseAge&#39;</span><span class="p">,</span><span class="s1">&#39;MedHouseVal&#39;</span><span class="p">])</span>
<span class="c1">#del data_cali_hp[&#39;geometry&#39;] # not useful for anything other than gpd visualisation</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>C:\Users\Miguel\AppData\Local\Temp\ipykernel_36132\3254044107.py:12: UserWarning: Geometry is in a geographic CRS. Results from &#39;area&#39; are likely incorrect. Use &#39;GeoSeries.to_crs()&#39; to re-project geometries to a projected CRS before this operation.

  cali = cali.assign(area=cali.geometry.area)
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>C:\Users\Miguel\AppData\Local\Temp\ipykernel_36132\3254044107.py:12: UserWarning: Geometry is in a geographic CRS. Results from &#39;area&#39; are likely incorrect. Use &#39;GeoSeries.to_crs()&#39; to re-project geometries to a projected CRS before this operation.

  cali = cali.assign(area=cali.geometry.area)
</pre></div>
</div>
<img alt="../_images/416ca70be9593670d39627cd2a8efc614e79186dfbdfd0b1e1e45f49bd9bc7db.png" src="../_images/416ca70be9593670d39627cd2a8efc614e79186dfbdfd0b1e1e45f49bd9bc7db.png" />
<img alt="../_images/9aedb2b29b22eac8196d0c97241af5114120a9d5ad9b92be21ea25b251747806.png" src="../_images/9aedb2b29b22eac8196d0c97241af5114120a9d5ad9b92be21ea25b251747806.png" />
</div>
</div>
<p>If you are not familiar with the state of California, it is interesting to notice that all datapoints show a graphical representation of this state. The high-valued houses will be located on the coast, where the big cities from California are located: San Diego, Los Angeles, San Jose, or San Francisco.</p>
<p>Let’s make a final analysis by making a pair plot of all features and the target but dropping the longitude and latitude. We will remove the target such that we can create proper histogram.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>

<span class="c1"># Take a random sample of the data to acelerate plotting </span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">indices</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span>
    <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">data_cali_hp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">size</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span>
<span class="p">)</span>

<span class="c1"># Drop the unwanted columns</span>
<span class="n">columns_drop</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Longitude&quot;</span><span class="p">,</span> <span class="s2">&quot;Latitude&quot;</span><span class="p">]</span>
<span class="n">subset</span> <span class="o">=</span> <span class="n">data_cali_hp</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="n">columns_drop</span><span class="p">)</span>
<span class="c1"># Quantize the target and keep the midpoint for each interval</span>
<span class="n">subset</span><span class="p">[</span><span class="s2">&quot;MedHouseVal&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">qcut</span><span class="p">(</span><span class="n">subset</span><span class="p">[</span><span class="s2">&quot;MedHouseVal&quot;</span><span class="p">],</span> <span class="mi">6</span><span class="p">,</span> <span class="n">retbins</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">subset</span><span class="p">[</span><span class="s2">&quot;MedHouseVal&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">subset</span><span class="p">[</span><span class="s2">&quot;MedHouseVal&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">mid</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">_</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">pairplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">subset</span><span class="p">,</span> <span class="n">hue</span><span class="o">=</span><span class="s2">&quot;MedHouseVal&quot;</span><span class="p">,</span> <span class="n">palette</span><span class="o">=</span><span class="s2">&quot;viridis&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/4ddd486751b60f56b6d8369e4bdaca59f4aef0b17645c9173c2c79ff2fe17c3e.png" src="../_images/4ddd486751b60f56b6d8369e4bdaca59f4aef0b17645c9173c2c79ff2fe17c3e.png" />
</div>
</div>
<p>Looking at descriptive statistics helps build some intuition for the data and think about the predictive model we want to setup. We can for instance notice the presence of outliers, and see that the median income is helpful to distinguish high-valued from low-valued houses. It is reasonable to expect the longitude, latitude, and the median income to be useful features in predicting the median house values.</p>
</section>
<section id="predicting-house-values">
<h2>Predicting House Values<a class="headerlink" href="#predicting-house-values" title="Permalink to this heading">#</a></h2>
<p>Let’s setup a Bayesian Linear Regression model to try and predict house prices in Cali.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">BayesianRidge</span><span class="p">,</span> <span class="n">LinearRegression</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">data_cali_hp</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">data_cali_hp</span><span class="p">[</span><span class="s1">&#39;MedHouseVal&#39;</span><span class="p">])</span> <span class="c1"># &lt;- typically we keep the log to run the regression. Here I take the exp{} so that coeffs are meaningful.</span>
<span class="n">LRM</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">BLR</span> <span class="o">=</span> <span class="n">BayesianRidge</span><span class="p">()</span>
<span class="n">BLR</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">LRM</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># Create a DataFrame to store the coefficients</span>
<span class="n">coefficients_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span>
    <span class="s1">&#39;Feature&#39;</span><span class="p">:</span> <span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">,</span>  <span class="c1"># Assuming X contains column names</span>
    <span class="s1">&#39;Linear Regression Coefficient&#39;</span><span class="p">:</span>  <span class="n">LRM</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span>
    <span class="s1">&#39;Bayesian Ridge Coefficient&#39;</span><span class="p">:</span> <span class="n">BLR</span><span class="o">.</span><span class="n">coef_</span>
<span class="p">})</span>

<span class="c1"># Display the coefficients</span>
<span class="nb">print</span><span class="p">(</span><span class="n">coefficients_df</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>       Feature  Linear Regression Coefficient  Bayesian Ridge Coefficient
0       MedInc                       1.634156                    1.632060
1     HouseAge                       0.175638                    0.175636
2     AveRooms                      -2.077023                   -2.069044
3    AveBedrms                       8.571035                    8.532918
4   Population                      -0.000657                   -0.000657
5     AveOccup                      -0.001491                   -0.001493
6     Latitude                       8.749902                    8.739650
7    Longitude                       8.726341                    8.716120
8  MedHouseVal                      26.761079                   26.753811
</pre></div>
</div>
</div>
</div>
<p>Note that the Bayesian Linear Regression that we described is sometimes called Bayesian Ridge due to the Gaussian Priors put on the weights <span class="math notranslate nohighlight">\(w\)</span>. These priors penalize large values of the coefficients. For this specific data, both seem to give very similar results although this needs not be the case. Notice also that, in the above example we totally disregarded the hyperparameters <span class="math notranslate nohighlight">\(\lambda\)</span> and <span class="math notranslate nohighlight">\(\sigma\)</span>. It turns out that, when using <code class="docutils literal notranslate"><span class="pre">BayesianRidge()</span></code> if you don’t specify their values, Python will automatically define uninformative Gamma hyperpriors. However, you can also specify hyperparameter if you have a strong opinion about what the value of a specific coefficient should be.</p>
</section>
<section id="id1">
<h2>Exercise<a class="headerlink" href="#id1" title="Permalink to this heading">#</a></h2>
<p>Calculate the predicted value of a house for a household with average features, using the model above.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">data_cali_hp</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">values</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([ 3.87067100e+00,  2.86394864e+01,  5.42899974e+00,  1.09667515e+00,
        1.42547674e+03,  3.07065516e+00,  3.56318614e+01, -1.19569704e+02,
        2.06855817e+00])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">BLR</span><span class="o">.</span><span class="n">predict</span><span class="p">([</span><span class="n">data_cali_hp</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">values</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>C:\Users\Miguel\anaconda3\Lib\site-packages\sklearn\base.py:464: UserWarning: X does not have valid feature names, but BayesianRidge was fitted with feature names
  warnings.warn(
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([1.30271218e+08])
</pre></div>
</div>
</div>
</div>
<p>Let’s write a DIY machine learning model pipeline to help us compare predictive performance.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">from</span> <span class="nn">sklearn.dummy</span> <span class="kn">import</span> <span class="n">DummyRegressor</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span>

<span class="k">def</span> <span class="nf">suppress_warnings</span><span class="p">():</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s1">&#39;ignore&#39;</span><span class="p">)</span>

<span class="c1"># Model Evaluation Pipeline w/ Cross Validation</span>
<span class="k">def</span> <span class="nf">modelEval</span><span class="p">(</span><span class="n">datah</span><span class="p">,</span><span class="n">feature</span><span class="o">=</span><span class="s1">&#39;MedHouseVal&#39;</span><span class="p">,</span><span class="n">model_id</span> <span class="o">=</span> <span class="s1">&#39;dummy&#39;</span><span class="p">):</span>
    
    
    <span class="c1"># Input: Feature &amp; Target DataFrame</span>
    <span class="c1"># Split feature/target variable</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">datah</span><span class="p">[</span><span class="n">feature</span><span class="p">]</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">datah</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="k">del</span> <span class="n">X</span><span class="p">[</span><span class="n">feature</span><span class="p">]</span>     <span class="c1"># remove target variable</span>

    <span class="n">suppress_warnings</span><span class="p">()</span>
    <span class="c1"># Pick Model </span>
    <span class="k">if</span><span class="p">(</span><span class="n">model_id</span> <span class="o">==</span> <span class="s1">&#39;dummy&#39;</span><span class="p">):</span>    <span class="n">model</span> <span class="o">=</span> <span class="n">DummyRegressor</span><span class="p">()</span>
    <span class="k">if</span><span class="p">(</span><span class="n">model_id</span> <span class="o">==</span> <span class="s1">&#39;BLR&#39;</span><span class="p">):</span>      <span class="n">model</span> <span class="o">=</span> <span class="n">BayesianRidge</span><span class="p">(</span><span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="c1"># &lt;- set to false for minimalist output</span>
    <span class="k">if</span><span class="p">(</span><span class="n">model_id</span> <span class="o">==</span> <span class="s1">&#39;LR&#39;</span><span class="p">):</span>       <span class="n">model</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span> 
<span class="w">    </span><span class="sd">&#39;&#39;&#39; Standard Cross Validation &#39;&#39;&#39;</span>
    <span class="n">cv_score</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="o">-</span><span class="n">cross_val_score</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span><span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;neg_mean_squared_error&#39;</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;RMSE:&quot;</span><span class="p">,</span><span class="n">cv_score</span><span class="p">);</span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Mean across k-fold:&quot;</span><span class="p">,</span> <span class="n">cv_score</span><span class="o">.</span><span class="n">mean</span><span class="p">());</span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;std:&quot;</span><span class="p">,</span> <span class="n">cv_score</span><span class="o">.</span><span class="n">std</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># A simple comparison of models</span>
<span class="n">modelEval</span><span class="p">(</span><span class="n">data_cali_hp</span><span class="p">,</span><span class="n">model_id</span><span class="o">=</span><span class="s1">&#39;dummy&#39;</span><span class="p">)</span>
<span class="n">modelEval</span><span class="p">(</span><span class="n">data_cali_hp</span><span class="p">,</span><span class="n">model_id</span><span class="o">=</span><span class="s1">&#39;BLR&#39;</span><span class="p">)</span>
<span class="n">modelEval</span><span class="p">(</span><span class="n">data_cali_hp</span><span class="p">,</span><span class="n">model_id</span><span class="o">=</span><span class="s1">&#39;LR&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>RMSE: [1.14300564 1.09495832 1.25408734 1.12189797 1.23064605]
Mean across k-fold: 1.1689190658050883
std: 0.06231618587595042
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>RMSE: [0.69598835 0.78910253 0.80387139 0.73723379 0.70327065]
Mean across k-fold: 0.7458933435919393
std: 0.04384216713047099
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>RMSE: [0.69631786 0.78898504 0.80387217 0.73702076 0.70333835]
Mean across k-fold: 0.7459068363518117
std: 0.04373972167592658
</pre></div>
</div>
</div>
</div>
</section>
<section id="id2">
<h2>Exercise<a class="headerlink" href="#id2" title="Permalink to this heading">#</a></h2>
<p>Why are the results identical for the Linear Regression and Bayesian Linear Regression ? Tweek the pipeline to allow for a much wider range of models within the linear regression model seen in previous labs. Make sure you include the Ridge, Lasso and Elastic Net. Compare the RMSE and comment.</p>
</section>
<section id="automatic-relevance-determination-regression">
<h2>Automatic Relevance Determination Regression<a class="headerlink" href="#automatic-relevance-determination-regression" title="Permalink to this heading">#</a></h2>
<p>Automatic Relevance Determination (ARD) Regression is a kind of Bayesian Linear Regression that leads to a sparser solution for <span class="math notranslate nohighlight">\(w\)</span>.</p>
<p>It is best thought as a prior rather than a regression. It drops the spherical Gaussian distribution for a centered elliptic Gaussian distribution. What does that mean ? Each coefficient <span class="math notranslate nohighlight">\(w_i\)</span> can be independently drawn from a Gaussian distribution, centered in zero and with precision <span class="math notranslate nohighlight">\(\lambda_i\)</span>. Thus,</p>
<div class="math notranslate nohighlight">
\[
p(w|\lambda) = N(w|0,A^{-1}),
\]</div>
<p>with A being a positive definite diagonal matrix and <span class="math notranslate nohighlight">\(\lambda = diag(A) = (\lambda_1,...,\lambda_d)\)</span>. Contrary to the standard Bayesian Linear Regression we introduced, here each element <span class="math notranslate nohighlight">\(w_i\)</span> has its own standard deviation <span class="math notranslate nohighlight">\(1/ \lambda_i\)</span>.</p>
<p>Again, here <span class="math notranslate nohighlight">\(\lambda\)</span> is a hyperparameter which can be set by the econometrician with expert knwoledge about the DGP. However, when using <code class="docutils literal notranslate"><span class="pre">ARDRegression()</span></code> from <code class="docutils literal notranslate"><span class="pre">sklearn</span></code>, by default Gamma priors are specified. ARD is also known in the literature as Sparse Bayesian Learning and Relevance Vector Machine (see Section 7.2.1 of <a class="reference external" href="https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf">Chris Bishop’s Book</a> )</p>
<p>Let’s now see how the ARD Prior leads to different results with our dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">ARDRegression</span>

<span class="n">ard</span> <span class="o">=</span> <span class="n">ARDRegression</span><span class="p">(</span><span class="n">compute_score</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">coefficients_df</span><span class="p">[</span><span class="s1">&#39;ARDRegression&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">ard</span><span class="o">.</span><span class="n">coef_</span>

<span class="n">coefficients_df</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Feature</th>
      <th>Linear Regression Coefficient</th>
      <th>Bayesian Ridge Coefficient</th>
      <th>ARDRegression</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>MedInc</td>
      <td>1.634156</td>
      <td>1.632060</td>
      <td>1.615543</td>
    </tr>
    <tr>
      <th>1</th>
      <td>HouseAge</td>
      <td>0.175638</td>
      <td>0.175636</td>
      <td>0.194449</td>
    </tr>
    <tr>
      <th>2</th>
      <td>AveRooms</td>
      <td>-2.077023</td>
      <td>-2.069044</td>
      <td>-2.013981</td>
    </tr>
    <tr>
      <th>3</th>
      <td>AveBedrms</td>
      <td>8.571035</td>
      <td>8.532918</td>
      <td>8.394052</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Population</td>
      <td>-0.000657</td>
      <td>-0.000657</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>5</th>
      <td>AveOccup</td>
      <td>-0.001491</td>
      <td>-0.001493</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>6</th>
      <td>Latitude</td>
      <td>8.749902</td>
      <td>8.739650</td>
      <td>8.836697</td>
    </tr>
    <tr>
      <th>7</th>
      <td>Longitude</td>
      <td>8.726341</td>
      <td>8.716120</td>
      <td>8.790180</td>
    </tr>
    <tr>
      <th>8</th>
      <td>MedHouseVal</td>
      <td>26.761079</td>
      <td>26.753811</td>
      <td>26.780613</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</section>
<section id="bayesian-regressions-with-polynomial-feature-expansion">
<h2>Bayesian regressions with polynomial feature expansion<a class="headerlink" href="#bayesian-regressions-with-polynomial-feature-expansion" title="Permalink to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">make_pipeline</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">PolynomialFeatures</span><span class="p">,</span> <span class="n">StandardScaler</span>

<span class="c1"># Take a random sample of the data to acelerate plotting </span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">indices</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span>
    <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">data_cali_hp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span>
<span class="p">)</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">data_cali_hp</span><span class="p">[[</span><span class="s1">&#39;MedInc&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">data_cali_hp</span><span class="p">[</span><span class="s1">&#39;MedHouseVal&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">indices</span><span class="p">])</span> <span class="c1"># &lt;- typically we keep the log to run the regression. Here I take the exp{} so that coeffs are meaningful.</span>

<span class="n">blr_poly</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span>
    <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">include_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
    <span class="n">StandardScaler</span><span class="p">(),</span>
    <span class="n">BayesianRidge</span><span class="p">(),</span>
<span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">ard_poly</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span>
    <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">include_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
    <span class="n">StandardScaler</span><span class="p">(),</span>
    <span class="n">ARDRegression</span><span class="p">(),</span>
<span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">brr_poly</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span>
    <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">include_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
    <span class="n">StandardScaler</span><span class="p">(),</span>
    <span class="n">BayesianRidge</span><span class="p">(),</span>
<span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="n">y_ard</span><span class="p">,</span> <span class="n">y_ard_std</span> <span class="o">=</span> <span class="n">ard_poly</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">return_std</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">y_brr</span><span class="p">,</span> <span class="n">y_brr_std</span> <span class="o">=</span> <span class="n">brr_poly</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">return_std</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">y_blr</span><span class="p">,</span> <span class="n">y_blr_std</span> <span class="o">=</span> <span class="n">blr_poly</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">return_std</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Original data points</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Data&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">)</span>

<span class="c1"># Sort X for a smoother plot</span>
<span class="n">X_plot</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Predictions and standard deviations for ARDRegression</span>
<span class="n">y_ard</span><span class="p">,</span> <span class="n">y_ard_std</span> <span class="o">=</span> <span class="n">ard_poly</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_plot</span><span class="p">,</span> <span class="n">return_std</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_plot</span><span class="p">,</span> <span class="n">y_ard</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;ARDRegression&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">X_plot</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">y_ard</span> <span class="o">-</span> <span class="n">y_ard_std</span><span class="p">,</span> <span class="n">y_ard</span> <span class="o">+</span> <span class="n">y_ard_std</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>

<span class="c1"># Predictions and standard deviations for ARDRegression</span>
<span class="n">y_blr</span><span class="p">,</span> <span class="n">y_blr_std</span> <span class="o">=</span> <span class="n">blr_poly</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_plot</span><span class="p">,</span> <span class="n">return_std</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_plot</span><span class="p">,</span> <span class="n">y_blr</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;BayesianLR&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">X_plot</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">y_blr</span> <span class="o">-</span> <span class="n">y_blr_std</span><span class="p">,</span> <span class="n">y_blr</span> <span class="o">+</span> <span class="n">y_blr_std</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>

<span class="c1"># Predictions and standard deviations for BayesianRidge</span>
<span class="n">y_brr</span><span class="p">,</span> <span class="n">y_brr_std</span> <span class="o">=</span> <span class="n">brr_poly</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_plot</span><span class="p">,</span> <span class="n">return_std</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_plot</span><span class="p">,</span> <span class="n">y_brr</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;BayesianPoly&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;g&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">X_plot</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">y_brr</span> <span class="o">-</span> <span class="n">y_brr_std</span><span class="p">,</span> <span class="n">y_brr</span> <span class="o">+</span> <span class="n">y_brr_std</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;g&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;MedInc&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;MedHouseVal&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper left&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Polynomial Regression with ARDRegression and BayesianLR&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/fbac31af139350f813da9911d65d94753a715f15a0d06197a6877f4274385f20.png" src="../_images/fbac31af139350f813da9911d65d94753a715f15a0d06197a6877f4274385f20.png" />
</div>
</div>
</section>
<section id="id3">
<h2>Exercise<a class="headerlink" href="#id3" title="Permalink to this heading">#</a></h2>
<p>Use the <code class="docutils literal notranslate"><span class="pre">make_pipeline</span></code> function to construct MSE for the three models in the previous example. Which one returns the best in-sample fit ?</p>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this heading">#</a></h2>
<p>Some good resources for further study:</p>
<p><a id='id27'></a>
<a class="reference external" href="https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf">Christopher M. Bishop. <em>Pattern Recognition and Machine Learning</em>. Springer, 2006.</a></p>
<p><a id='id26'></a>
<a class="reference external" href="https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html">Pace, R. Kelley and Ronald Barry. <em>Sparse Spatial Autoregressions</em>, Statistics and Probability Letters, 33 ,1997 291-297.</a></p>
<p><a id='app-reg-ex'></a></p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./Lab4"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="../Lab3/regression_2.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Penalized Regression</p>
      </div>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise">Exercise</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#predicting-house-values">Predicting House Values</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Exercise</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Exercise</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#automatic-relevance-determination-regression">Automatic Relevance Determination Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-regressions-with-polynomial-feature-expansion">Bayesian regressions with polynomial feature expansion</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Exercise</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Miguel C. Herculano
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>