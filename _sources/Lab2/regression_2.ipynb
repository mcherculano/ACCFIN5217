{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f60ab0d9",
   "metadata": {},
   "source": [
    "### 2.1 Overfitting and Regularization \n",
    "\n",
    "You might be wondering, “Why would we ever want to throw variables out,\n",
    "can’t that only hurt our model?”\n",
    "\n",
    "The primary answer: it helps us avoid a common issue called **overfitting**.\n",
    "\n",
    "Overfitting occurs when a model that specializes its coefficients too much on the\n",
    "data it was trained on, and only to perform poorly when predicting on data\n",
    "outside the training set.\n",
    "\n",
    "The extreme example of overfitting is a model that can perfectly memorize the\n",
    "training data, but can do no better than just randomly guess when predicting\n",
    "on a new observation.\n",
    "\n",
    "The techniques applied to reduce overfitting are known as **regularization**.\n",
    "\n",
    "Regularization is an attempt to limit a model’s ability to specialize too narrowly\n",
    "on training data (e.g. limit overfitting) by penalizing extreme values of the\n",
    "model’s parameters.\n",
    "\n",
    "The additional term in the lasso regression loss function ($ \\alpha ||\\beta||_1 $)\n",
    "is a form of regularization.\n",
    "\n",
    "Let’s demonstrate the overfitting and regularization phenomenon on our housing\n",
    "price data as follows:\n",
    "\n",
    "1. Split the data set into training and testing subsets. We will use the first 50 observations for training and the rest for testing.  \n",
    "1. Fit the linear regression model and report MSE on training and testing datasets.  \n",
    "1. Fit the lasso model and report the same statistics.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed200c29",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "Let’s load the dataset and take a quick look at our task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e63a94",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "colors = ['#165aa7', '#cb495c', '#fec630', '#bb60d5', '#f47915', '#06ab54', '#002070', '#b27d12', '#007030']\n",
    "\n",
    "# We will import all these here to ensure that they are loaded, but\n",
    "# will usually re-import close to where they are used to make clear\n",
    "# where the functions come from\n",
    "from sklearn import (\n",
    "    linear_model, metrics, neural_network, pipeline, model_selection\n",
    ")\n",
    "\n",
    "url = \"https://datascience.quantecon.org/assets/data/kc_house_data.csv\"\n",
    "df = pd.read_csv(url)\n",
    "df.info()\n",
    "\n",
    "X = df.drop([\"price\", \"date\", \"id\"], axis=1).copy()\n",
    "# convert everything to be a float for later on\n",
    "for col in list(X):\n",
    "    X[col] = X[col].astype(float)\n",
    "X.head()\n",
    "y = np.log(df[\"price\"])\n",
    "df[\"log_price\"] = y\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14f0768",
   "metadata": {},
   "source": [
    "## Lasso Regression\n",
    "\n",
    "Lasso regression is very closely related to linear regression.\n",
    "\n",
    "The lasso model also generates predictions using $ y = X \\beta $ but\n",
    "optimizes over a slightly different loss function.\n",
    "\n",
    "The optimization problem solved by lasso regression can be written as\n",
    "\n",
    "$$\n",
    "\\min_{\\beta} {|| X \\beta - y||_2}^2 + \\underbrace{\\alpha {|| \\beta ||_1}}_{\\text{new part}}\n",
    "$$\n",
    "\n",
    "where $ || a ||_1 = \\sum_{i=1}^N | a_i| $ is the [l1-norm](http://mathworld.wolfram.com/L1-Norm.html) and $ \\alpha $ is called the regularization parameter.\n",
    "\n",
    "The additional term penalizes large coefficients and in practice, effectively sets coefficients to zero\n",
    "for features that are not informative about the\n",
    "target.\n",
    "\n",
    "Let’s see an example of what this looks like using the full feature set in\n",
    "`X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7fb88b",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "lasso_model = linear_model.Lasso()\n",
    "lasso_model.fit(X, y)\n",
    "\n",
    "lasso_coefs = pd.Series(dict(zip(list(X), lasso_model.coef_)))\n",
    "lr_coefs = pd.Series(dict(zip(list(X), lr_model.coef_)))\n",
    "coefs = pd.DataFrame(dict(lasso=lasso_coefs, linreg=lr_coefs))\n",
    "coefs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df270ca",
   "metadata": {},
   "source": [
    "Notice that many coefficients from the lasso regression have been set to\n",
    "zero.\n",
    "\n",
    "The intuition here is that the corresponding features hadn’t provided\n",
    "enough predictive power to be worth considering alongside the other features.\n",
    "\n",
    "The default value for the $ \\alpha $ parameter is 1.0.\n",
    "\n",
    "Larger $ \\alpha $ values cause coefficients to shrink (and maybe\n",
    "additional ones to be thrown out)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83b231c",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# Compute lasso for many alphas (the lasso path)\n",
    "from itertools import cycle\n",
    "alphas = np.exp(np.linspace(10, -2, 50))\n",
    "alphas, coefs_lasso, _ = linear_model.lasso_path(X, y, alphas=alphas, max_iter=10000)\n",
    "\n",
    "# plotting\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "color_cycle = cycle(colors)\n",
    "log_alphas = -np.log10(alphas)\n",
    "for coef_l, c, name in zip(coefs_lasso, color_cycle, list(X)):\n",
    "   ax.plot(log_alphas, coef_l, c=c)\n",
    "   ax.set_xlabel('-Log(alpha)')\n",
    "   ax.set_ylabel('lasso coefficients')\n",
    "   ax.set_title('Lasso Path')\n",
    "   ax.axis('tight')\n",
    "   maxabs = np.max(np.abs(coef_l))\n",
    "   i = [idx for idx in range(len(coef_l)) if abs(coef_l[idx]) >= (0.9*maxabs)][0]\n",
    "   xnote = log_alphas[i]\n",
    "   ynote = coef_l[i]\n",
    "   ax.annotate(name, (xnote, ynote), color=c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0f166d",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def fit_and_report_mses(mod, X_train, X_test, y_train, y_test):\n",
    "    mod.fit(X_train, y_train)\n",
    "    return dict(\n",
    "        mse_train=metrics.mean_squared_error(y_train, mod.predict(X_train)),\n",
    "        mse_test=metrics.mean_squared_error(y_test, mod.predict(X_test))\n",
    "    )\n",
    "\n",
    "n_test = 50\n",
    "X_train = X.iloc[:n_test, :]\n",
    "X_test = X.iloc[n_test:, :]\n",
    "y_train = y.iloc[:n_test]\n",
    "y_test = y.iloc[n_test:]\n",
    "\n",
    "fit_and_report_mses(linear_model.LinearRegression(), X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0472be92",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "fit_and_report_mses(linear_model.Lasso(), X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2b096b",
   "metadata": {},
   "source": [
    "Notice how the MSE on the training dataset was smaller for the linear model\n",
    "without the regularization, but the MSE on the test dataset was much\n",
    "higher.\n",
    "\n",
    "This strongly suggests that the linear regression model was\n",
    "overfitting.\n",
    "\n",
    "The regularization parameter has a large impact on overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f084e1",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "alphas = np.exp(np.linspace(10, -5, 100))\n",
    "mse = pd.DataFrame([fit_and_report_mses(linear_model.Lasso(alpha=alpha, max_iter=50000),\n",
    "                           X_train, X_test, y_train, y_test)\n",
    "                    for alpha in alphas])\n",
    "mse[\"log_alpha\"] = -np.log10(alphas)\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "mse.plot(x=\"log_alpha\", y=\"mse_test\", c=colors[0], ax=ax)\n",
    "mse.plot(x=\"log_alpha\", y=\"mse_train\", c=colors[1], ax=ax)\n",
    "ax.set_xlabel(r\"$-\\log(\\alpha)$\")\n",
    "ax.set_ylabel(\"MSE\")\n",
    "ax.get_legend().remove()\n",
    "ax.annotate(\"test\",(mse.log_alpha[15], mse.mse_test[15]),color=colors[0])\n",
    "ax.annotate(\"train\",(mse.log_alpha[30], mse.mse_train[30]),color=colors[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0192b225",
   "metadata": {},
   "source": [
    "### Cross-validation of Regularization Parameter\n",
    "\n",
    "As you can see in the figure above, the regularization parameter has a\n",
    "large impact on MSE in the test data.\n",
    "\n",
    "Moreover, the relationship between the test data MSE and $ \\alpha $ is\n",
    "complicated and non-monotonic.\n",
    "\n",
    "One popular method for choosing the regularization parameter is cross-validation.\n",
    "\n",
    "Roughly speaking, cross-validation splits the dataset into many training/testing\n",
    "subsets, then chooses the regularization parameter value that minimizes the\n",
    "average MSE.\n",
    "\n",
    "More precisely, k-fold cross-validation does the following:\n",
    "\n",
    "1. Partition the dataset randomly into k subsets/”folds”.  \n",
    "1. Compute $ MSE_j(\\alpha)= $ mean squared error in j-th subset\n",
    "  when using the j-th subset as test data, and other k-1 as training\n",
    "  data.  \n",
    "1. Minimize average (across folds) MSE $ \\min_\\alpha \\frac{1}{k}\n",
    "  \\sum_{j=1}^k MSE_j(\\alpha) $.  \n",
    "\n",
    "\n",
    "The following code plots 5-fold, cross-validated MSE as a function of\n",
    "$ \\alpha $, using the same training data as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84825215",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "mse[\"cv\"] = [-np.mean(cross_val_score(linear_model.Lasso(alpha=alpha, max_iter=50000),\n",
    "                                  X_train, y_train, cv=5, scoring='neg_mean_squared_error'))\n",
    "          for alpha in alphas]\n",
    "mse.plot(x=\"log_alpha\", y=\"cv\", c=colors[2], ax=ax)\n",
    "ax.annotate(\"5 fold cross-validation\", (mse.log_alpha[40], mse.cv[40]), color=colors[2])\n",
    "ax.get_legend().remove()\n",
    "ax.set_xlabel(r\"$-\\log(\\alpha)$\")\n",
    "ax.set_ylabel(\"MSE\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22c23ec",
   "metadata": {},
   "source": [
    "scikit learn also includes methods to automate the above and select\n",
    "$ \\alpha $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34e533f",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# LassoCV exploits special structure of lasso problem to minimize CV more efficiently\n",
    "lasso = linear_model.LassoCV(cv=5).fit(X_train,y_train)\n",
    "-np.log10(lasso.alpha_) # should roughly = minimizer on graph, not exactly equal due to random splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d61d96",
   "metadata": {},
   "source": [
    "### Holdout\n",
    "\n",
    "Practitioners often use another technique to avoid overfitting, called\n",
    "*holdout*.\n",
    "\n",
    "We demonstrated an extreme example of applying holdout above when we used only\n",
    "the first 50 observations to train our models.\n",
    "\n",
    "In general, good practice is to split the entire dataset into a training subset\n",
    "and testing or validation subset.\n",
    "\n",
    "The splitting should be done randomly. It should leave enough data in the\n",
    "training dataset to produce a good model, but also enough in the validation\n",
    "subset to determine the degree of overfitting.\n",
    "\n",
    "There aren’t hard and fast rules for how much data to put in each subset, but a\n",
    "reasonable default uses 75% of the data for training and the\n",
    "rest for testing.\n",
    "\n",
    "As in the example above, the training data is often further split\n",
    "while selecting regularization parameters with cross-validation.\n",
    "\n",
    "The `sklearn` function `model_selection.train_test_split` will do this for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661ee691",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# note test_size=0.25 is the default value, but is shown here so you\n",
    "# can see how to change it\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6dbe0c",
   "metadata": {},
   "source": [
    "### Exercise 5\n",
    "\n",
    "Experiment with how the size of the holdout dataset can impact a diagnosis\n",
    "of overfitting.\n",
    "\n",
    "Evaluate only the `LinearRegression` model on the full feature set and use\n",
    "the `model_selection.train_test_split` function with various values for\n",
    "`test_size`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529801e7",
   "metadata": {},
   "source": [
    "### Lasso in Econometrics\n",
    "\n",
    "Lasso is becoming increasingly popular in economics, partially due to\n",
    "the work of Victor Chernozhukov and his coauthors.\n",
    "\n",
    "In econometrics, the goal is typically to estimate some coefficient of interest or causal\n",
    "effect, rather than obtaining the most precise prediction.\n",
    "\n",
    "Among other things, this different goal affects how the regularization parameter\n",
    "must be chosen.\n",
    "\n",
    "[[regBC11](#id23)] and [[regCHS16](#id21)] are somewhat approachable introductions\n",
    "to this area.\n",
    "\n",
    "The latter of these two references includes an R package.\n",
    "\n",
    "[[regCCD+18](#id14)] reflects close to the state-of-art in this rapidly\n",
    "advancing area."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6486648e",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "Two good text books covering the above regression methods are\n",
    "[[regFHT09](#id26)] and [[regEH16](#id27)]\n",
    "\n",
    "<a id='id24'></a>\n",
    "\\[regAI18\\] Susan Athey and Guido Imbens. Machine learning and econometrics. 2018. URL: [https://www.aeaweb.org/conference/cont-ed/2018-webcasts](https://www.aeaweb.org/conference/cont-ed/2018-webcasts).\n",
    "\n",
    "<a id='id25'></a>\n",
    "\\[regAI17\\] Susan Athey and Guido W. Imbens. The state of applied econometrics: causality and policy evaluation. *Journal of Economic Perspectives*, 31(2):3–32, May 2017. URL: [http://www.aeaweb.org/articles?id=10.1257/jep.31.2.3](http://www.aeaweb.org/articles?id=10.1257/jep.31.2.3), [doi:10.1257/jep.31.2.3](https://doi.org/10.1257/jep.31.2.3).\n",
    "\n",
    "<a id='id23'></a>\n",
    "\\[regBC11\\] Alexandre Belloni and Victor Chernozhukov. *High Dimensional Sparse Econometric Models: An Introduction*, pages 121–156. Springer Berlin Heidelberg, Berlin, Heidelberg, 2011. URL: [https://doi.org/10.1007/978-3-642-19989-9_3](https://doi.org/10.1007/978-3-642-19989-9_3), [doi:10.1007/978-3-642-19989-9_3](https://doi.org/10.1007/978-3-642-19989-9_3).\n",
    "\n",
    "<a id='id14'></a>\n",
    "\\[regCCD+18\\] Victor Chernozhukov, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen, Whitney Newey, and James Robins. Double/debiased machine learning for treatment and structural parameters. *The Econometrics Journal*, 21(1):C1–C68, 2018. URL: [https://onlinelibrary.wiley.com/doi/abs/10.1111/ectj.12097](https://onlinelibrary.wiley.com/doi/abs/10.1111/ectj.12097), [arXiv:https://onlinelibrary.wiley.com/doi/pdf/10.1111/ectj.12097](https://arxiv.org/abs/https://onlinelibrary.wiley.com/doi/pdf/10.1111/ectj.12097), [doi:10.1111/ectj.12097](https://doi.org/10.1111/ectj.12097).\n",
    "\n",
    "<a id='id21'></a>\n",
    "\\[regCHS16\\] Victor Chernozhukov, Chris Hansen, and Martin Spindler. hdm: high-dimensional metrics. *R Journal*, 8(2):185–199, 2016. URL: [https://journal.r-project.org/archive/2016/RJ-2016-040/index.html](https://journal.r-project.org/archive/2016/RJ-2016-040/index.html).\n",
    "\n",
    "<a id='id27'></a>\n",
    "\\[regEH16\\] Bradley Efron and Trevor Hastie. *Computer age statistical inference*. Volume 5. Cambridge University Press, 2016. URL: [https://web.stanford.edu/~hastie/CASI/](https://web.stanford.edu/~hastie/CASI/).\n",
    "\n",
    "<a id='id26'></a>\n",
    "\\[regFHT09\\] Jerome Friedman, Trevor Hastie, and Robert Tibshirani. *The elements of statistical learning*. Springer series in statistics, 2009. URL: [https://web.stanford.edu/~hastie/ElemStatLearn/](https://web.stanford.edu/~hastie/ElemStatLearn/).\n",
    "\n",
    "<a id='id28'></a>\n",
    "\\[regHSW89\\] Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are universal approximators. *Neural Networks*, 2(5):359 – 366, 1989. URL: [http://www.sciencedirect.com/science/article/pii/0893608089900208](http://www.sciencedirect.com/science/article/pii/0893608089900208), [doi:https://doi.org/10.1016/0893-6080(89)90020-8](https://doi.org/https://doi.org/10.1016/0893-6080%2889%2990020-8).\n",
    "\n",
    "\n",
    "<a id='app-reg-ex'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c006fc0",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef5751a",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "Use the `sqft_lr_model` that we fit to generate predictions for all data points\n",
    "in our sample.\n",
    "\n",
    "Note that you need to pass a DataFrame (not Series)\n",
    "containing the `sqft_living` column to the `predict`. (See how we passed that to `.fit`\n",
    "above for help)\n",
    "\n",
    "Make a scatter chart with the actual data and the predictions on the same\n",
    "figure. Does it look familiar?\n",
    "\n",
    "When making the scatter for model predictions, we recommend passing\n",
    "`c=\"red\"` and `alpha=0.25` so you can distinguish the data from\n",
    "predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9a37ff",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Make scatter of data\n",
    "\n",
    "# Make scatter of predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fe9608",
   "metadata": {},
   "source": [
    "([back to text](#app-reg-dir1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3b566e",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "Use the `metrics.mean_squared_error` function to evaluate the loss\n",
    "function used by `sklearn` when it fits the model for us.\n",
    "\n",
    "Read the docstring to learn which the arguments that function takes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb43d293",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212e082c",
   "metadata": {},
   "source": [
    "([back to text](#app-reg-dir2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b350f2",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "Compare the mean squared error for the `lr_model` and the `sqft_lr_model`.\n",
    "\n",
    "Which model has a better fit? Defend your choice.\n",
    "\n",
    "([back to text](#app-reg-dir3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0626bf6",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "\n",
    "Explore how you can improve the fit of the full model by adding additional\n",
    "features created from the existing ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a8391e",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6848ed",
   "metadata": {},
   "source": [
    "([back to text](#app-reg-dir4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850f2e2d",
   "metadata": {},
   "source": [
    "### Exercise 5\n",
    "\n",
    "Experiment with how the size of the holdout dataset can impact a diagnosis\n",
    "of overfitting.\n",
    "\n",
    "Evaluate only the `LinearRegression` model on the full feature set and use\n",
    "the `model_selection.train_test_split` function with various values for\n",
    "`test_size`.\n",
    "\n",
    "([back to text](#app-reg-dir5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9fe617",
   "metadata": {},
   "source": [
    "### Exercise 6\n",
    "\n",
    "Read the documentation for `sklearn.tree.DecisionTreeRegressor` and\n",
    "then experiment with adjusting some regularization parameters to see how they\n",
    "affect the fitted tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c92009",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# plot trees when varying some regularization parameter(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca54b70",
   "metadata": {},
   "source": [
    "([back to text](#app-reg-dir6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1a70b1",
   "metadata": {},
   "source": [
    "### Exercise 7\n",
    "\n",
    "Fit a regression tree to the housing price data and use graphviz\n",
    "to visualize the decision graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a6dc4b",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f61ece",
   "metadata": {},
   "source": [
    "([back to text](#app-reg-dir7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eacc95e",
   "metadata": {},
   "source": [
    "### Exercise 8\n",
    "\n",
    "Fit a random forest to the housing price data.\n",
    "\n",
    "Compare the MSE on a testing set to that of lasso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fd1c5e",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# Fit random forest and compute MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a82550e",
   "metadata": {},
   "source": [
    "Produce a bar chart of feature importances for predicting house\n",
    "prices.\n",
    "\n",
    "([back to text](#app-reg-dir8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8232fcc",
   "metadata": {},
   "source": [
    "### Exercise 9\n",
    "\n",
    "In the pseudocode below, fill in the blanks for the generic MLP.\n",
    "\n",
    "Note that this is inside a markdown cell because the code is not valid\n",
    "Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebf7bc4",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "ws = [w1, w2, ..., wend]\n",
    "bs = [b1, b2, ..., bend]\n",
    "\n",
    "def eval_mlp(X, ws, bs, f):\n",
    "    \"\"\"\n",
    "    Evaluate MLP-given weights (ws), bias (bs) and an activation (f)\n",
    "\n",
    "    Assumes that the same activation is applied to all hidden layers\n",
    "    \"\"\"\n",
    "    N = len(ws) - 1\n",
    "\n",
    "    out = X\n",
    "    for i in range(N):\n",
    "        out = f(__)  # replace the __\n",
    "\n",
    "    # For this step remember python starts counting at 0!\n",
    "    return out@__ + __  # replace the __"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f1522d",
   "metadata": {},
   "source": [
    "([back to text](#app-reg-dir9))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0d2d14",
   "metadata": {},
   "source": [
    "### Exercise 10\n",
    "\n",
    "Scale all variables in `X` by subtracting their mean and dividing by the\n",
    "standard deviation.\n",
    "\n",
    "Verify that the transformed data has mean 0 and standard deviation 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59162681",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36aa21f",
   "metadata": {},
   "source": [
    "([back to text](#app-reg-dir10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f0bc1b",
   "metadata": {},
   "source": [
    "### Exercise 11\n",
    "\n",
    "Read the documentation for sklearn.neural_network.MLPRegressor and\n",
    "use the full housing data to experiment with how adjusting layer depth, width, and other\n",
    "regularization parameters affects prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63c2de1",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  }
 ],
 "metadata": {
  "date": 1689807042.4906044,
  "filename": "regression.md",
  "kernelspec": {
   "display_name": "Python",
   "language": "python3",
   "name": "python3"
  },
  "title": "Regression"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
